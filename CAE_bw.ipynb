{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 38M\n",
      "encoder size: 13292288\n",
      "decoder size: 21661963\n",
      "mlp_key size: 2100736\n",
      "mlp_map size: 1052160\n"
     ]
    }
   ],
   "source": [
    "channels    = [256, 512, 512]\n",
    "latent_dim  = 512\n",
    "hidden_size = 512\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = Magician(channels, latent_dim, hidden_size)\n",
    "model_size = count_parameters(model)\n",
    "print(f\"model {int(model_size/1e6)}M\")\n",
    "print(\"encoder size:\", count_parameters(model.encoder))\n",
    "print(\"decoder size:\", count_parameters(model.decoder))\n",
    "print(\"mlp_key size:\", count_parameters(model.mlp_key))\n",
    "print(\"mlp_map size:\", count_parameters(model.mlp_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 30, 30]) torch.Size([1, 1, 1, 30, 30]) torch.Size([1, 6, 1, 30, 30]) torch.Size([1, 6, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "from dataloader_pairs import ARC_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 100,\n",
    "    'task_numbers': 1, #equal to the number of tasks\n",
    "    'task_data_num': 1,\n",
    "    'example_data_num': 5, #equal to inner model batch size\n",
    "    'inner_lr': 0.001,\n",
    "    'outer_lr': 0.001,\n",
    "    'embed_size': 1,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=kwargs['task_numbers'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=kwargs['task_numbers'], shuffle=False)\n",
    "\n",
    "ti, to, ei, eo = next(iter(train_loader))\n",
    "\n",
    "print(ti.shape, to.shape, ei.shape, eo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device  \n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 총 클래스의 수\n",
    "num_classes = 11\n",
    "\n",
    "# 0번과 1번 클래스에 부여할 가중치 (10% 이하로 설정)\n",
    "weight_0 = 0.04\n",
    "weight_1 = 0.05\n",
    "\n",
    "# 나머지 9개 클래스에 공평하게 가중치 부여\n",
    "remaining_weight = 1.0 - (weight_0 + weight_1)\n",
    "weight_other = remaining_weight / (num_classes - 2)\n",
    "\n",
    "# 가중치 리스트 생성\n",
    "class_weights = [weight_0, weight_1] + [weight_other] * (num_classes - 2)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "                                                                           \n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    ce = F.cross_entropy(y_pred, y, weight=class_weights_tensor)\n",
    "    # ce = F.cross_entropy(y_pred, y)\n",
    "    return ce\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor1 = torch.randn(1, 10).to(device)\n",
    "tensor2 = torch.randn(1, 10).to(device)\n",
    "\n",
    "concatenated_tensor = torch.cat((tensor1, tensor2), dim=1)\n",
    "print(concatenated_tensor.shape)  # Should print torch.Size([1, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z torch.Size([32, 1, 30, 30])\n",
      "z_one_hot torch.Size([32, 11, 30, 30])\n",
      "Encoded output shape: torch.Size([32, 512, 30, 30])\n",
      "Residual 1 shape: torch.Size([32, 256, 30, 30])\n",
      "Residual 2 shape: torch.Size([32, 512, 30, 30])\n",
      "Residual 3 shape: torch.Size([32, 512, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bnorm1 = nn.BatchNorm2d(C)\n",
    "        self.bnorm2 = nn.BatchNorm2d(C)\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.conv1(self.relu(self.bnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.bnorm2(r)))\n",
    "        return r + x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, mode: str, C_in: int, C_out: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(C_out)\n",
    "        if mode == \"down\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"up\":\n",
    "            self.conv = nn.ConvTranspose2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"same\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            raise ValueError(\"Wrong ConvBlock mode.\")\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv(z)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], latent_dim=512, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = ConvBlock(\"same\", 11,  channels[0], dropout)\n",
    "        self.res12 = ResBlock(channels[0], dropout)\n",
    "        self.conv2 = ConvBlock(\"same\", channels[0], channels[1], dropout)\n",
    "        self.res23 = ResBlock(channels[1], dropout)\n",
    "        self.conv3 = ConvBlock(\"same\", channels[1], channels[2], dropout)\n",
    "    \n",
    "    def one_hot_encode(self, z, num_classes=11):\n",
    "        \"\"\"\n",
    "        One-hot encode the input tensor and adjust the shape.\n",
    "        \"\"\"\n",
    "        return F.one_hot(z.squeeze(1).long(), num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # One-hot encode the input inside the encoder\n",
    "        print('z', z.shape)\n",
    "        z_one_hot = self.one_hot_encode(z)\n",
    "        print('z_one_hot', z_one_hot.shape)\n",
    "        residuals = [0] * 3\n",
    "        x = self.conv1(z_one_hot)\n",
    "        x = self.res12(x)\n",
    "        residuals[0] = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.res23(x)\n",
    "        residuals[1] = x\n",
    "        x = self.conv3(x)\n",
    "        residuals[2] = x\n",
    "        return x, residuals\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "encoder = Encoder(channels=[256, 512, 512], latent_dim=512, dropout=0.1)\n",
    "\n",
    "# 예시 입력 데이터 생성\n",
    "batch_size = 32\n",
    "input_height = 30\n",
    "input_width = 30\n",
    "# 입력 데이터 (batch_size, 1, height, width)\n",
    "input_data = torch.randint(0, 11, (batch_size, 1, input_height, input_width))\n",
    "\n",
    "# 모델에 예시 입력 데이터 넣어서 동작 확인\n",
    "encoded_output, residuals = encoder(input_data)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Encoded output shape: {encoded_output.shape}\")\n",
    "print(f\"Residual 1 shape: {residuals[0].shape}\")\n",
    "print(f\"Residual 2 shape: {residuals[1].shape}\")\n",
    "print(f\"Residual 3 shape: {residuals[2].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 30, 30])\n",
      "torch.Size([32, 512, 30, 30])\n",
      "torch.Size([32, 512, 30, 30])\n",
      "torch.Size([32, 1024, 30, 30])\n",
      "torch.Size([32, 256, 30, 30])\n",
      "torch.Size([32, 256, 30, 30])\n",
      "torch.Size([32, 512, 30, 30])\n",
      "torch.Size([32, 256, 30, 30])\n",
      "Decoder output shape: torch.Size([32, 11, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bnorm1 = nn.BatchNorm2d(C)\n",
    "        self.bnorm2 = nn.BatchNorm2d(C)\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.conv1(self.relu(self.bnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.bnorm2(r)))\n",
    "        return r + x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, mode: str, C_in: int, C_out: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(C_out)\n",
    "        if mode == \"down\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"up\":\n",
    "            self.conv = nn.ConvTranspose2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"same\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            raise ValueError(\"Wrong ConvBlock mode.\")\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv(z)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 'channels' 리스트는 Decoder의 각 단계에서 채널 수를 정의합니다.\n",
    "        # ConvBlock의 입력 채널 수를 정확히 맞춰야 합니다.\n",
    "        self.conv3 = ConvBlock(\"same\", channels[-1] * 2, channels[-2], dropout)  # 512 * 2 -> 512\n",
    "        self.res32 = ResBlock(channels[-2], dropout)  # 512\n",
    "        self.conv2 = ConvBlock(\"same\", channels[-2] * 2, channels[-3], dropout)  # 512 * 2 -> 256\n",
    "        self.res21 = ResBlock(channels[-3], dropout)  # 256\n",
    "        self.conv1 = ConvBlock(\"same\", channels[-3] * 2, channels[-3], dropout)  # 256 * 2 -> 256\n",
    "        self.conv0 = nn.Conv2d(channels[-3], 11, kernel_size=3, padding=1)  # Final output with 11 channels\n",
    "\n",
    "    def forward(self, x, residuals):\n",
    "        # Concatenate and process with conv3\n",
    "        x = torch.cat([x, residuals[2]], dim=1)  # [32, 512*2, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.conv3(x)  # [32, 512, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.res32(x)  # [32, 512, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        \n",
    "        # Concatenate and process with conv2\n",
    "        x = torch.cat([x, residuals[1]], dim=1)  # [32, 512*2, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.conv2(x)  # [32, 256, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.res21(x)  # [32, 256, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        \n",
    "        # Concatenate and process with conv1\n",
    "        x = torch.cat([x, residuals[0]], dim=1)  # [32, 256*2, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.conv1(x)  # [32, 256, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "\n",
    "        # Final output layer\n",
    "        x = self.conv0(x)  # [32, 11, 30, 30]\n",
    "        return x\n",
    "\n",
    "# 디코더 인스턴스 생성\n",
    "decoder = Decoder(channels=[256, 512, 512], dropout=0.1)\n",
    "\n",
    "# 인코더에서 얻은 출력 및 Residual\n",
    "encoded_output = torch.randn(32, 512, 30, 30)  # torch.Size([32, 512, 30, 30])\n",
    "residuals = [\n",
    "    torch.randn(32, 256, 30, 30),  # Residual 1: torch.Size([32, 256, 30, 30])\n",
    "    torch.randn(32, 512, 30, 30),  # Residual 2: torch.Size([32, 512, 30, 30])\n",
    "    torch.randn(32, 512, 30, 30)   # Residual 3: torch.Size([32, 512, 30, 30])\n",
    "]\n",
    "\n",
    "# 디코더에 인코더 출력과 residuals 넣어서 결과 확인\n",
    "decoder_output = decoder(encoded_output, residuals)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBAM output shape: torch.Size([32, 512, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CBAM 모듈 정의\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(channels, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x\n",
    "\n",
    "# 테스트 코드\n",
    "def test_cbam():\n",
    "    # CBAM 인스턴스 생성 (예를 들어 채널 수가 512인 경우)\n",
    "    cbam = CBAM(channels=512, ratio=16, kernel_size=7)\n",
    "\n",
    "    # 가상의 인코더 출력 생성 (배치 크기: 32, 채널: 512, 크기: 30x30)\n",
    "    encoder_output = torch.randn(32, 512, 30, 30)\n",
    "    \n",
    "    # CBAM 모듈을 통해 인코더 출력을 통과시킵니다.\n",
    "    cbam_output = cbam(encoder_output)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"CBAM output shape: {cbam_output.shape}\")\n",
    "\n",
    "test_cbam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z torch.Size([32, 1, 30, 30])\n",
      "z_one_hot torch.Size([32, 11, 30, 30])\n",
      "z torch.Size([32, 1, 30, 30])\n",
      "z_one_hot torch.Size([32, 11, 30, 30])\n",
      "z torch.Size([32, 1, 30, 30])\n",
      "z_one_hot torch.Size([32, 11, 30, 30])\n",
      "combined_edu shape: torch.Size([32, 1024, 30, 30])\n",
      "attended_edu shape: torch.Size([32, 1024, 30, 30])\n",
      "attended_edu shape after reduction: torch.Size([32, 512, 30, 30])\n",
      "task_edu_cat shape: torch.Size([32, 1024, 30, 30])\n",
      "task_causal_mix shape: torch.Size([32, 1024, 30, 30])\n",
      "task_causal_mix shape after reduction: torch.Size([32, 512, 30, 30])\n",
      "torch.Size([32, 1024, 30, 30])\n",
      "torch.Size([32, 512, 30, 30])\n",
      "torch.Size([32, 512, 30, 30])\n",
      "torch.Size([32, 1024, 30, 30])\n",
      "torch.Size([32, 256, 30, 30])\n",
      "torch.Size([32, 256, 30, 30])\n",
      "torch.Size([32, 512, 30, 30])\n",
      "torch.Size([32, 256, 30, 30])\n",
      "Result shape: torch.Size([32, 11, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CBAM 모듈 정의\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(channels, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bnorm1 = nn.BatchNorm2d(C)\n",
    "        self.bnorm2 = nn.BatchNorm2d(C)\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.conv1(self.relu(self.bnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.bnorm2(r)))\n",
    "        return r + x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, mode: str, C_in: int, C_out: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(C_out)\n",
    "        if mode == \"down\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"up\":\n",
    "            self.conv = nn.ConvTranspose2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"same\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            raise ValueError(\"Wrong ConvBlock mode.\")\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv(z)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = ConvBlock(\"same\", 11,  channels[0], dropout)\n",
    "        self.res12 = ResBlock(channels[0], dropout)\n",
    "        self.conv2 = ConvBlock(\"same\", channels[0], channels[1], dropout)\n",
    "        self.res23 = ResBlock(channels[1], dropout)\n",
    "        self.conv3 = ConvBlock(\"same\", channels[1], channels[2], dropout)\n",
    "    \n",
    "    def one_hot_encode(self, z, num_classes=11):\n",
    "        \"\"\"\n",
    "        One-hot encode the input tensor and adjust the shape.\n",
    "        \"\"\"\n",
    "        return F.one_hot(z.squeeze(1).long(), num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # One-hot encode the input inside the encoder\n",
    "        print('z', z.shape)\n",
    "        z_one_hot = self.one_hot_encode(z)\n",
    "        print('z_one_hot', z_one_hot.shape)\n",
    "        residuals = [0] * 3\n",
    "        x = self.conv1(z_one_hot)\n",
    "        x = self.res12(x)\n",
    "        residuals[0] = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.res23(x)\n",
    "        residuals[1] = x\n",
    "        x = self.conv3(x)\n",
    "        residuals[2] = x\n",
    "        return x, residuals\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 'channels' 리스트는 Decoder의 각 단계에서 채널 수를 정의합니다.\n",
    "        # ConvBlock의 입력 채널 수를 정확히 맞춰야 합니다.\n",
    "        self.conv3 = ConvBlock(\"same\", channels[-1] * 2, channels[-2], dropout)  # 512 * 2 -> 512\n",
    "        self.res32 = ResBlock(channels[-2], dropout)  # 512\n",
    "        self.conv2 = ConvBlock(\"same\", channels[-2] * 2, channels[-3], dropout)  # 512 * 2 -> 256\n",
    "        self.res21 = ResBlock(channels[-3], dropout)  # 256\n",
    "        self.conv1 = ConvBlock(\"same\", channels[-3] * 2, channels[-3], dropout)  # 256 * 2 -> 256\n",
    "        self.conv0 = nn.Conv2d(channels[-3], 11, kernel_size=3, padding=1)  # Final output with 11 channels\n",
    "\n",
    "    def forward(self, x, residuals):\n",
    "        # Concatenate and process with conv3\n",
    "        x = torch.cat([x, residuals[2]], dim=1)  # [32, 512*2, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.conv3(x)  # [32, 512, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.res32(x)  # [32, 512, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        \n",
    "        # Concatenate and process with conv2\n",
    "        x = torch.cat([x, residuals[1]], dim=1)  # [32, 512*2, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.conv2(x)  # [32, 256, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.res21(x)  # [32, 256, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        \n",
    "        # Concatenate and process with conv1\n",
    "        x = torch.cat([x, residuals[0]], dim=1)  # [32, 256*2, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "        x = self.conv1(x)  # [32, 256, 30, 30]\n",
    "        print(x.shape)  # Debug print\n",
    "\n",
    "        # Final output layer\n",
    "        x = self.conv0(x)  # [32, 11, 30, 30]\n",
    "        return x\n",
    "\n",
    "class CBAMAEmodel(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], embed_dim=512, dropout=0.1):\n",
    "        super(CBAMAEmodel, self).__init__()\n",
    "        self.preprocess_and_embed = Encoder(channels, dropout)\n",
    "        self.decoder = Decoder(channels, dropout)\n",
    "        self.cbam1 = CBAM(channels=embed_dim*2)  # edu_input과 edu_output 결합 후 적용\n",
    "        self.cbam2 = CBAM(channels=embed_dim*2)  \n",
    "        self.reduce_channels = nn.Conv2d(embed_dim*2, embed_dim*1, kernel_size=1)  # 채널 축소\n",
    "        self.reduce_channels2 = nn.Conv2d(embed_dim*2, embed_dim*1, kernel_size=1)  # 채널 축소\n",
    "\n",
    "    def forward(self, edu_input, edu_output, task_input):\n",
    "        # edu_input, edu_output, task_input 임베딩\n",
    "        embedded_edu_input, _ = self.preprocess_and_embed(edu_input)\n",
    "        embedded_edu_output, _ = self.preprocess_and_embed(edu_output)\n",
    "        embedded_task_input, residuals = self.preprocess_and_embed(task_input)\n",
    "\n",
    "        # edu_input과 edu_output 결합\n",
    "        combined_edu = torch.cat([embedded_edu_input, embedded_edu_output], dim=1)\n",
    "        \n",
    "        # combined_edu의 크기 출력\n",
    "        print(f\"combined_edu shape: {combined_edu.shape}\")\n",
    "\n",
    "        # CBAM 적용\n",
    "        attended_edu = self.cbam1(combined_edu)\n",
    "        print(f\"attended_edu shape: {attended_edu.shape}\")\n",
    "        attended_edu = self.reduce_channels(attended_edu)\n",
    "        print(f\"attended_edu shape after reduction: {attended_edu.shape}\")\n",
    "        # attended_edu와 task_input 결합\n",
    "        task_edu_cat = torch.cat([attended_edu, embedded_task_input], dim=1)\n",
    "        print(f\"task_edu_cat shape: {task_edu_cat.shape}\")\n",
    "        # task_causal_mix 처리 후 디코더 적용\n",
    "        task_causal_mix = self.cbam2(task_edu_cat)\n",
    "        print(f\"task_causal_mix shape: {task_causal_mix.shape}\")\n",
    "        task_causal_mix = self.reduce_channels2(task_causal_mix)\n",
    "        print(f\"task_causal_mix shape after reduction: {task_causal_mix.shape}\")\n",
    "        final_output = self.decoder(task_causal_mix, residuals)\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "model = CBAMAEmodel(channels=[256, 512, 512], embed_dim=512)\n",
    "batch_size = 32\n",
    "input_height = 30\n",
    "input_width = 30\n",
    "task_input = torch.randint(0, 11, (batch_size, 1, input_height, input_width))\n",
    "edu_input = torch.randint(0, 11, (batch_size, 1, input_height, input_width))\n",
    "edu_output = torch.randint(0, 11, (batch_size, 1, input_height, input_width))\n",
    "\n",
    "result = model(edu_input, edu_output, task_input)\n",
    "print(\"Result shape:\", result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 24M\n"
     ]
    }
   ],
   "source": [
    "channels    = [256, 512, 512]\n",
    "latent_dim  = 512\n",
    "hidden_size = 512\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model_size = count_parameters(model)\n",
    "print(f\"model {int(model_size/1e6)}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result shape: torch.Size([32, 11, 30, 30])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
