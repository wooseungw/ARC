{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, linewidth=0.5, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 예시:\n",
    "# predicted와 example_output이 [1, 1, 30, 30] 크기의 텐서라고 가정\n",
    "#show_grid_side_by_side(task_input, task_output, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from math import sqrt\n",
    "\n",
    "def cast_tuple(val, depth):\n",
    "    return val if isinstance(val, tuple) else (val,) * depth\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "## 컨볼루션 임베딩\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embed_dim, kernel_size, stride, padding, dropout=0.0):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Conv2d(1, \n",
    "                                   embed_dim, \n",
    "                                   kernel_size=kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "## 인코더\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, *, dim=124, num_heads=4, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim=124, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                MultiheadAttention(dim=dim, num_heads=num_heads, dropout=dropout),\n",
    "                FeedForwardNetwork(dim=dim, hidden_dim=dim * 4, dropout=dropout)\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        height = x.shape[2]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        for attn, mlp in self.layers:\n",
    "            x = attn(x)\n",
    "            x = mlp(x)\n",
    "\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=height)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, input_dim = 256 ,dim=128, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(input_dim , dim, kernel_size=1),  \n",
    "            nn.Conv2d(dim, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "## 디코더\n",
    "# Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # Shared MLP for both max and avg pooling paths\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out) * x\n",
    "\n",
    "# Spatial Attention Module\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Decoder with Channel and Spatial Attention\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.channel_att = ChannelAttention(dim)\n",
    "        self.spatial_att = SpatialAttention()\n",
    "\n",
    "        # Additional Conv layers for decoding\n",
    "        self.conv1 = nn.Conv2d(dim, dim // 2, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(dim // 2, dim // 2, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        spatial_attention = self.spatial_att(x)\n",
    "\n",
    "        # Apply conv layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        # Use spatial_attention for final output\n",
    "        return spatial_attention\n",
    "\n",
    "class ARC_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        *,\n",
    "        dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        num_classes=11,\n",
    "        dropout=0.1,\n",
    "        kernel_stride_padding=((1, 1, 0), (3, 1, 1))\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "\n",
    "        # 각 커널 크기, 스트라이드, 패딩을 튜플로 묶어서 처리\n",
    "        for (kernel_size, stride, padding) in kernel_stride_padding:\n",
    "            self.stages.append(\n",
    "                nn.ModuleList([\n",
    "                    Embedding(embed_dim=dim, kernel_size=kernel_size, stride=stride, padding=padding, dropout=dropout),\n",
    "                    Encoder(dim=dim, num_heads=num_heads, num_layers=num_layers, dropout=dropout)\n",
    "                ])\n",
    "            )\n",
    "        \n",
    "        # Decoder를 통해 attention을 적용하여 결합\n",
    "        self.decoder = Decoder(dim * len(kernel_stride_padding))  \n",
    "        self.head = Head(input_dim=dim * len(kernel_stride_padding) + 1, dim=dim, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_outputs = []\n",
    "\n",
    "        # 각 커널 크기, 스트라이드, 패딩에 대해 독립적으로 처리\n",
    "        for embedding, encoder in self.stages:\n",
    "            scale_x = embedding(x)\n",
    "            scale_x = encoder(scale_x)\n",
    "            all_outputs.append(scale_x)\n",
    "\n",
    "        # 다양한 커널 크기에서 추출된 특징을 병합\n",
    "        x = torch.cat(all_outputs, dim=1)\n",
    "\n",
    "        # Decoder를 통해 어텐션 적용 및 결과와 결합\n",
    "        ch_sp = self.decoder(x)\n",
    "\n",
    "        # Decoder의 출력을 원래 x에 concatenate\n",
    "        x = torch.cat([ch_sp,x], dim=1)\n",
    "\n",
    "        # 병합된 특징을 헤드에 전달\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 출력\n",
    "model_args =  {\n",
    "    \"img_size\": 30,\n",
    "    \"patch_size\": 3, # default: 16\n",
    "    \"in_channels\": 1,\n",
    "    \"embed_dim\": 256, # default: 768\n",
    "    \"num_heads\": 8, # default: 12\n",
    "    \"num_layers\": 8, # default: 12\n",
    "    \"mlp_dim\": 2048, # default: 3072\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_channels\": 101,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionTransformer(**model_args).to(device)\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPs: 7.599G\n",
      "파라미터 수: 8.449M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "outer_model = VisionTransformer(**model_args).to(device)\n",
    "\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "# FLOPs 및 파라미터 수 계산\n",
    "try:\n",
    "    flops, params = profile(outer_model, inputs=(x,))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "    print(f\"파라미터 수: {params}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during profiling: {e}\")\n",
    "    print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bw_net_maml import BWNet_MAML\n",
    "\n",
    "# model = BWNet_MAML(embed_size=1).to(device)\n",
    "\n",
    "# # 입력 텐서 생성\n",
    "# x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "# # FLOPs 및 파라미터 수 계산\n",
    "# try:\n",
    "#     flops, params = profile(model, inputs=(x,))\n",
    "#     flops, params = clever_format([flops, params], \"%.3f\")\n",
    "#     print(f\"FLOPs: {flops}\")\n",
    "#     print(f\"파라미터 수: {params}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during profiling: {e}\")\n",
    "#     print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0000e-04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weight = torch.ones(11).to('cuda')\n",
    "weight[0] = 0.0005  # 0은 무시\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop\n",
    "N개의 샘플 배치를 불러온다.\n",
    "1. 각 샘플에 대해 이너 모델을 아우터 모델에서 복사해 로스를 계산한다.\n",
    "2. 이너 모델의 파라미터를 업데이트한다.\n",
    "3. 업데이트 된 이너 모델을 바탕으로 테스크 셋에 대한 로스를 계산한다.\n",
    "4. 테스크 셋에 대한 로스를 저장한다.\n",
    "5. 모든 테스크 셋에 대해 로스가 구해지면 아우터 모델의 파라미터를 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14/14 [02:56<00:00, 12.62s/it]\n",
      "Validation:  93%|█████████▎| 13/14 [03:27<00:16, 16.01s/it]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x280 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [03:43<00:00, 15.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 6.137815568782834, Accuracy: 7.441506369588388%\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14/14 [02:54<00:00, 12.45s/it]\n",
      "Validation:  93%|█████████▎| 13/14 [03:28<00:15, 15.98s/it]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x280 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [03:43<00:00, 15.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, Loss: 6.139919976203709, Accuracy: 4.837841953002081%\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14/14 [02:56<00:00, 12.60s/it]\n",
      "Validation:  93%|█████████▎| 13/14 [03:27<00:15, 15.97s/it]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x280 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [03:42<00:00, 15.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500, Loss: 6.063077946835884, Accuracy: 3.6309191493681165%\n",
      "Epoch 4/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 14/14 [02:55<00:00, 12.53s/it]\n",
      "Validation:  93%|█████████▎| 13/14 [03:27<00:16, 16.01s/it]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x280 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 14/14 [03:43<00:00, 15.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500, Loss: 6.00408536352235, Accuracy: 5.722986347256763%\n",
      "Epoch 5/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 4/14 [00:50<02:07, 12.73s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_sw import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 500,\n",
    "    'task_numbers': 30,  # equal to the number of tasks\n",
    "    'task_data_num': 1,\n",
    "    'example_data_num': 5,  # equal to inner model batch size\n",
    "    'inner_lr': 0.01,\n",
    "    'outer_lr': 0.001,\n",
    "    'embed_size': 1,\n",
    "}\n",
    "\n",
    "\n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    weight = torch.ones(model_args['output_channels']).to(y.device)\n",
    "    weight[0] = 0.007\n",
    "    ce = F.cross_entropy(y_pred, y, weight=weight)\n",
    "    return ce\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device\n",
    "print(f'Using {device} device')\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=kwargs['task_numbers'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(eval_challenge, eval_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=kwargs['task_numbers'], shuffle=False)\n",
    "\n",
    "# Outer Model 정의\n",
    "outer_model = VisionTransformer(**model_args).to(device)\n",
    "outer_optimizer = optim.AdamW(outer_model.parameters(), lr=kwargs['outer_lr'])\n",
    "\n",
    "# Inner Loop 업데이트 함수\n",
    "def inner_loop_update(model, example_input, example_output, inner_optimizer, criterion, steps):\n",
    "    for _ in range(steps):\n",
    "        model.train()\n",
    "        prediction = model(example_input)\n",
    "        loss = criterion(prediction, example_output)\n",
    "\n",
    "        inner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        inner_optimizer.step()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(kwargs['epochs']):\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}')\n",
    "    total_loss = 0\n",
    "    outer_model.train()\n",
    "    \n",
    "    for data in tqdm(train_loader, desc='Training'):\n",
    "        input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "        \n",
    "        task_losses = []\n",
    "        for task_number in range(input_tensor.shape[0]):\n",
    "            # 모델의 가중치만 복사하여 이너 모델 초기화\n",
    "            inner_model = VisionTransformer(**model_args).to(device)\n",
    "            inner_model.load_state_dict(outer_model.state_dict())\n",
    "            \n",
    "            inner_optimizer = optim.AdamW(inner_model.parameters(), lr=kwargs['inner_lr'])\n",
    "            inner_loop_update(inner_model, example_input[task_number], example_output[task_number],\n",
    "                              inner_optimizer, criterion, kwargs['example_data_num'])\n",
    "            \n",
    "            inner_model.eval()\n",
    "            task_prediction = inner_model(input_tensor[task_number])\n",
    "            task_loss = criterion(task_prediction, output_tensor[task_number])\n",
    "            task_losses.append(task_loss)\n",
    "        \n",
    "        meta_loss = torch.stack(task_losses).mean()\n",
    "        outer_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        outer_optimizer.step()\n",
    "        \n",
    "        del meta_loss, task_losses\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Validation Loop\n",
    "    outer_model.eval()\n",
    "    validation_correct = 0\n",
    "    validation_total_samples = 0\n",
    "    total_loss = []\n",
    "\n",
    "    for batch_idx, data in enumerate(tqdm(eval_loader, desc='Validation')):\n",
    "        input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "\n",
    "        for task_number in range(input_tensor.shape[0]):\n",
    "            inner_model = VisionTransformer(**model_args).to(device)\n",
    "            inner_model.load_state_dict(outer_model.state_dict())\n",
    "            \n",
    "            inner_optimizer = optim.AdamW(inner_model.parameters(), lr=kwargs['inner_lr'])\n",
    "            inner_loop_update(inner_model, example_input[task_number], example_output[task_number],\n",
    "                            inner_optimizer, criterion, kwargs['example_data_num'])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inner_model.eval()\n",
    "                task_input = input_tensor[task_number]\n",
    "                task_output = output_tensor[task_number]\n",
    "                task_prediction = inner_model(task_input)\n",
    "                task_loss = criterion(task_prediction, task_output)\n",
    "                total_loss.append(task_loss.item())\n",
    "\n",
    "                prediction_class = torch.argmax(task_prediction, dim=1, keepdim=True)\n",
    "\n",
    "                mask = task_output != 0\n",
    "                correct_predictions = (prediction_class == task_output) & mask\n",
    "                validation_correct += correct_predictions.sum().item()\n",
    "                validation_total_samples += mask.sum().item()\n",
    "\n",
    "                if batch_idx == len(eval_loader) - 1 and task_number == input_tensor.shape[0] - 1:\n",
    "                    show_grid_side_by_side(task_input.cpu(), task_output.cpu(), prediction_class.cpu())\n",
    "\n",
    "            del inner_model, inner_optimizer, task_input, task_output, task_prediction, mask, correct_predictions\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    mean_loss = sum(total_loss) / len(total_loss) if total_loss else 0\n",
    "    accuracy = 100 * validation_correct / validation_total_samples if validation_total_samples > 0 else 0\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}, Loss: {mean_loss}, Accuracy: {accuracy}%')\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
