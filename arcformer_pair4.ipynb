{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class DsConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias=bias),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "## 디코더\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from math import sqrt\n",
    "\n",
    "def cast_tuple(val, depth):\n",
    "    return val if isinstance(val, tuple) else (val,) * depth\n",
    "\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads,\n",
    "        reduction_ratio\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim * 2, reduction_ratio, stride = reduction_ratio, bias = False)\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = heads), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h = heads, x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class MixFeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        expansion_factor\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DsConv2d(hidden_dim, hidden_dim, 3, padding = 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "## 컨볼루션 임베딩\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dim_in,dim_out, kernel_size, stride, padding):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Conv2d(dim_in, \n",
    "                                   dim_out, \n",
    "                                   kernel_size=kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "'''\n",
    "5. MiT (Mixer Transformer)\n",
    "이미지를 여러 스테이지로 처리합니다. 각 스테이지는 이미지를 패치로 나누고, 패치를 임베딩한 후, 여러 개의 Transformer 레이어를 적용합니다.\n",
    "이 과정은 이미지의 다양한 해상도에서 특징을 추출합니다. \n",
    "'''    \n",
    "class MiT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        channels,\n",
    "        dims,\n",
    "        heads,\n",
    "        ff_expansion,\n",
    "        reduction_ratio,\n",
    "        num_layers,\n",
    "        stage_kernel_stride_pad = ((7, 4, 3),  \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1))\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        dims = (channels, *dims)\n",
    "        dim_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        self.stages = nn.ModuleList([])\n",
    "\n",
    "        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(\n",
    "            dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n",
    "            #여기서 너비와 높이가 같은 정사각형 패치를 사용합니다.\n",
    "            get_overlap_patches = nn.Unfold(kernel, stride = stride, padding = padding)\n",
    "            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n",
    "\n",
    "            layers = nn.ModuleList([])\n",
    "\n",
    "            for _ in range(num_layers):\n",
    "                layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim_out, EfficientSelfAttention(dim = dim_out, heads = heads, reduction_ratio = reduction_ratio)),\n",
    "                    PreNorm(dim_out, MixFeedForward(dim = dim_out, expansion_factor = ff_expansion)),\n",
    "                ]))\n",
    "\n",
    "            self.stages.append(nn.ModuleList([\n",
    "                get_overlap_patches,\n",
    "                overlap_patch_embed,\n",
    "                layers\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        return_layer_outputs = False\n",
    "    ):\n",
    "        h, w = x.shape[-2:]\n",
    "        \n",
    "        \n",
    "        layer_outputs = []\n",
    "        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n",
    "            x = get_overlap_patches(x)\n",
    "            \n",
    "            num_patches = x.shape[-1]\n",
    "            ratio = int(sqrt((h * w) / num_patches))\n",
    "            \n",
    "            x = rearrange(x, 'b c (h w) -> b c h w', h = h // ratio)\n",
    "\n",
    "            x = overlap_embed(x)\n",
    "            for (attn, ff) in layers:\n",
    "                x = attn(x) + x\n",
    "                x = ff(x) + x\n",
    "\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        ret = x if not return_layer_outputs else layer_outputs\n",
    "        return ret\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, input_dim = 256 ,dim=128, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(input_dim , (input_dim//3)*2, kernel_size=1),  \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d((input_dim//3)*2 , input_dim//3, kernel_size=1),  \n",
    "            nn.Conv2d(dim, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "'''\n",
    "6. Segformer\n",
    "MiT를 통해 추출된 여러 스케일의 특징을 결합하고, 최종적으로 세그멘테이션 맵을 생성합니다.\n",
    "각 스테이지의 출력을 디코더 차원으로 매핑하고, 업샘플링하여 동일한 해상도로 만든 후, 이를 결합합니다.\n",
    "결합된 특징 맵을 사용하여 최종 세그멘테이션 맵을 생성합니다.\n",
    "'''\n",
    "class ARC_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dims=(32, 64, 160, 256),\n",
    "        heads=(1, 2, 5, 8),\n",
    "        ff_expansion=(8, 8, 4, 4),\n",
    "        reduction_ratio=(8, 4, 2, 1),\n",
    "        num_layers=2,\n",
    "        channels=11,\n",
    "        num_classes=11,\n",
    "        kernel_stride_paddings = ((1, 1, 0),(3, 2, 1), (3, 2, 1), (3, 2, 1))\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        decoder_dim = dims[-1]\n",
    "        \n",
    "        dims, heads, ff_expansion, reduction_ratio, num_layers = map(\n",
    "            partial(cast_tuple, depth=len(kernel_stride_paddings)), (dims, heads, ff_expansion, reduction_ratio, num_layers))\n",
    "\n",
    "        \n",
    "        self.mit = MiT(\n",
    "            channels=channels,\n",
    "            dims=dims,\n",
    "            heads=heads,\n",
    "            ff_expansion=ff_expansion,\n",
    "            reduction_ratio=reduction_ratio,\n",
    "            num_layers=num_layers,\n",
    "            stage_kernel_stride_pad=kernel_stride_paddings\n",
    "        )\n",
    "\n",
    "        self.to_fused = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(dim, decoder_dim, 1),\n",
    "            nn.Upsample(scale_factor=2 ** i)\n",
    "        ) for i, dim in enumerate(dims)])\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.cbam = CBAM(gate_channels = decoder_dim * (len(kernel_stride_paddings)-1) * 2)  \n",
    "        \n",
    "        self.cbam2 = CBAM(gate_channels = decoder_dim * (len(kernel_stride_paddings)-1) * 2)\n",
    "        \n",
    "        self.reduce = nn.Conv2d(decoder_dim * (len(kernel_stride_paddings)-1) * 2, decoder_dim * (len(kernel_stride_paddings)-1), 1)        \n",
    "        \n",
    "        self.to_segmentation = Head(decoder_dim*(len(kernel_stride_paddings)-1)* 3, decoder_dim*(len(kernel_stride_paddings)-1) , num_classes)\n",
    "        \n",
    "    def _fusion(self, x):\n",
    "        x = self.mit(x, return_layer_outputs=True)\n",
    "        # 드랍아웃 \n",
    "        x = [self.gelu(x) for x in x]\n",
    "        x = [self.dropout(x) for x in x]\n",
    "        fused = []\n",
    "        for output, to_fused in zip(x[1:], self.to_fused[1:]):\n",
    "            x = to_fused(output)  # Conv2d 적용\n",
    "            # 업샘플링하여 공간 크기를 맞춥니다.\n",
    "            fused.append(x)\n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        \n",
    "        return fused    \n",
    "    \n",
    "    def forward(self, x, ex_inputs, ex_outputs):\n",
    "        x = self._fusion(x) # [b,1,H,W] -> [b,dim_0,H,W],...,[b,dim_n, H//(n+1), W//(n+1)] \n",
    "                            # -> [b,dim_n, H, W],...,[b,dim_n, H, W] -> [b, dim_n * 2, H, W]\n",
    "                            # [b, 256, 32, 32]\n",
    "                            \n",
    "        # 예제 입력과 출력을 채널 차원으로 분할\n",
    "        n_examples = ex_inputs.size(1)//self.channels  # n_examples * channels (여기서는 3)\n",
    "        \n",
    "        # 예제 수만큼 반복하면서 예제 입력과 출력을 처리\n",
    "        fused = []\n",
    "        for i in range(n_examples):\n",
    "            ex_i = ex_inputs[:, i*self.channels:(i+1)*self.channels, :, :]  # [batch_size, 1, H, W]\n",
    "            ex_o = ex_outputs[:, i*self.channels:(i+1)*self.channels, :, :]  # [batch_size, 1, H, W]\n",
    "            \n",
    "            ex_i = self._fusion(ex_i)\n",
    "            ex_o = self._fusion(ex_o)\n",
    "            \n",
    "            ex_f = torch.cat([ex_i, ex_o], dim=1)\n",
    "            ex_f = self.cbam(ex_f)\n",
    "            ex_f = self.reduce(ex_f)\n",
    "            fused.append(ex_f)\n",
    "        \n",
    "        for i, ex_f in enumerate(fused):\n",
    "            ex_f = torch.cat([x, ex_f], dim=1)\n",
    "            ex_f = self.cbam2(ex_f)\n",
    "            ex_f = self.reduce(ex_f)\n",
    "            fused[i] = ex_f\n",
    "        \n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        output = self.to_segmentation(fused)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 출력\n",
    "model_args = {\n",
    "    'dims': (32, 64, 128,256),\n",
    "    'heads': 4,\n",
    "    'ff_expansion': 4,\n",
    "    'reduction_ratio': (4,2,2,2),\n",
    "    'num_layers': (1,2,3,4),\n",
    "    'channels': 11,\n",
    "    'num_classes': 11,\n",
    "    'kernel_stride_paddings': ((3, 1, 1),(3, 2, 1),(3, 2, 1),(3, 2, 1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims, heads, ff_expansion, reduction_ratio, num_layers = map(\n",
    "#             partial(cast_tuple, depth=4), (model_args['dims'], \n",
    "#                                            model_args['heads'], \n",
    "#                                            model_args['ff_expansion'], \n",
    "#                                            model_args['reduction_ratio'], \n",
    "#                                            model_args['num_layers']))\n",
    "# mit = MiT(\n",
    "#         dims=dims,\n",
    "#         heads=heads,\n",
    "#         ff_expansion=ff_expansion,\n",
    "#         reduction_ratio=reduction_ratio,\n",
    "#         num_layers=num_layers,\n",
    "#         channels=model_args['channels'],\n",
    "#         stage_kernel_stride_pad=model_args['kernel_stride_paddings'],\n",
    "        \n",
    "#           )\n",
    "# x = torch.randn(10, 1, 30, 30)\n",
    "\n",
    "# print(\"Input shape:\", x.shape)\n",
    "# x = mit(x,return_layer_outputs = True)\n",
    "\n",
    "# print(\"Output shape:\", x[0].shape)\n",
    "# print(\"Output shape:\", x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = torch.randn(10, 1, 30, 30)\n",
    "# i = 0\n",
    "# h = tensor[:, i:i+11, :, :]\n",
    "# h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse = [torch.randn(10, 256, 32, 32), torch.randn(10, 256, 32, 32), torch.randn(10, 256, 32, 32)]\n",
    "\n",
    "# x= torch.randn(10, 256, 32, 32)\n",
    "# for i , ex in enumerate(fuse) :\n",
    "#     print(\"fuse shape:\", ex.shape)\n",
    "#     ex = torch.cat([ex, x], dim=1)\n",
    "#     print(\"fuse shape:\", ex.shape)\n",
    "#     fuse[i] = ex\n",
    "#     print(\"fuse shape:\", fuse[i].shape)\n",
    "#     print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwoo/anaconda3/envs/dust/lib/python3.12/site-packages/torch/nn/functional.py:4790: UserWarning: The operator 'aten::im2col' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025535429/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 11, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device\n",
    "print(f'Using {device} device')\n",
    "model = ARC_Net(**model_args).to(device)\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(10, 11, 32, 32).to(device)\n",
    "e_i, e_o = torch.randn(10, 33, 32, 32).to(device), torch.randn(10, 33, 32, 32).to(device)\n",
    "print(model(x,e_i,e_o).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
       "========================================================================================================================\n",
       "ARC_Net                                                                [1, 11, 32, 32]           8,448\n",
       "├─MiT: 1-1                                                             [1, 32, 32, 32]           --\n",
       "│    └─ModuleList: 2-29                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-69                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-70                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-71                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-72                                           --                        (recursive)\n",
       "├─GELU: 1-2                                                            [1, 32, 32, 32]           --\n",
       "├─GELU: 1-3                                                            [1, 64, 16, 16]           --\n",
       "├─GELU: 1-4                                                            [1, 128, 8, 8]            --\n",
       "├─GELU: 1-5                                                            [1, 256, 4, 4]            --\n",
       "├─Dropout: 1-6                                                         [1, 32, 32, 32]           --\n",
       "├─Dropout: 1-7                                                         [1, 64, 16, 16]           --\n",
       "├─Dropout: 1-8                                                         [1, 128, 8, 8]            --\n",
       "├─Dropout: 1-9                                                         [1, 256, 4, 4]            --\n",
       "├─ModuleList: 1-74                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-2                                                 [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-5                                                [1, 256, 16, 16]          16,640\n",
       "│    │    └─Upsample: 3-6                                              [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-3                                                 [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-7                                                [1, 256, 8, 8]            33,024\n",
       "│    │    └─Upsample: 3-8                                              [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-4                                                 [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-9                                                [1, 256, 4, 4]            65,792\n",
       "│    │    └─Upsample: 3-10                                             [1, 256, 32, 32]          --\n",
       "├─MiT: 1-11                                                            [1, 32, 32, 32]           (recursive)\n",
       "│    └─ModuleList: 2-29                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-69                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-70                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-71                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-72                                           --                        (recursive)\n",
       "├─GELU: 1-12                                                           [1, 32, 32, 32]           --\n",
       "├─GELU: 1-13                                                           [1, 64, 16, 16]           --\n",
       "├─GELU: 1-14                                                           [1, 128, 8, 8]            --\n",
       "├─GELU: 1-15                                                           [1, 256, 4, 4]            --\n",
       "├─Dropout: 1-16                                                        [1, 32, 32, 32]           --\n",
       "├─Dropout: 1-17                                                        [1, 64, 16, 16]           --\n",
       "├─Dropout: 1-18                                                        [1, 128, 8, 8]            --\n",
       "├─Dropout: 1-19                                                        [1, 256, 4, 4]            --\n",
       "├─ModuleList: 1-74                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-6                                                 [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-15                                               [1, 256, 16, 16]          (recursive)\n",
       "│    │    └─Upsample: 3-16                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-7                                                 [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-17                                               [1, 256, 8, 8]            (recursive)\n",
       "│    │    └─Upsample: 3-18                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-8                                                 [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-19                                               [1, 256, 4, 4]            (recursive)\n",
       "│    │    └─Upsample: 3-20                                             [1, 256, 32, 32]          --\n",
       "├─MiT: 1-21                                                            [1, 32, 32, 32]           (recursive)\n",
       "│    └─ModuleList: 2-29                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-69                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-70                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-71                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-72                                           --                        (recursive)\n",
       "├─GELU: 1-22                                                           [1, 32, 32, 32]           --\n",
       "├─GELU: 1-23                                                           [1, 64, 16, 16]           --\n",
       "├─GELU: 1-24                                                           [1, 128, 8, 8]            --\n",
       "├─GELU: 1-25                                                           [1, 256, 4, 4]            --\n",
       "├─Dropout: 1-26                                                        [1, 32, 32, 32]           --\n",
       "├─Dropout: 1-27                                                        [1, 64, 16, 16]           --\n",
       "├─Dropout: 1-28                                                        [1, 128, 8, 8]            --\n",
       "├─Dropout: 1-29                                                        [1, 256, 4, 4]            --\n",
       "├─ModuleList: 1-74                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-10                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-25                                               [1, 256, 16, 16]          (recursive)\n",
       "│    │    └─Upsample: 3-26                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-11                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-27                                               [1, 256, 8, 8]            (recursive)\n",
       "│    │    └─Upsample: 3-28                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-12                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-29                                               [1, 256, 4, 4]            (recursive)\n",
       "│    │    └─Upsample: 3-30                                             [1, 256, 32, 32]          --\n",
       "├─CBAM: 1-31                                                           [1, 1536, 32, 32]         --\n",
       "│    └─ChannelGate: 2-13                                               [1, 1536, 32, 32]         --\n",
       "│    │    └─Sequential: 3-31                                           [1, 1536]                 296,544\n",
       "│    │    └─Sequential: 3-32                                           [1, 1536]                 (recursive)\n",
       "│    └─SpatialGate: 2-14                                               [1, 1536, 32, 32]         --\n",
       "│    │    └─ChannelPool: 3-33                                          [1, 2, 32, 32]            --\n",
       "│    │    └─BasicConv: 3-34                                            [1, 1, 32, 32]            100\n",
       "├─Conv2d: 1-32                                                         [1, 768, 32, 32]          1,180,416\n",
       "├─MiT: 1-33                                                            [1, 32, 32, 32]           (recursive)\n",
       "│    └─ModuleList: 2-29                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-69                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-70                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-71                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-72                                           --                        (recursive)\n",
       "├─GELU: 1-34                                                           [1, 32, 32, 32]           --\n",
       "├─GELU: 1-35                                                           [1, 64, 16, 16]           --\n",
       "├─GELU: 1-36                                                           [1, 128, 8, 8]            --\n",
       "├─GELU: 1-37                                                           [1, 256, 4, 4]            --\n",
       "├─Dropout: 1-38                                                        [1, 32, 32, 32]           --\n",
       "├─Dropout: 1-39                                                        [1, 64, 16, 16]           --\n",
       "├─Dropout: 1-40                                                        [1, 128, 8, 8]            --\n",
       "├─Dropout: 1-41                                                        [1, 256, 4, 4]            --\n",
       "├─ModuleList: 1-74                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-16                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-39                                               [1, 256, 16, 16]          (recursive)\n",
       "│    │    └─Upsample: 3-40                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-17                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-41                                               [1, 256, 8, 8]            (recursive)\n",
       "│    │    └─Upsample: 3-42                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-18                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-43                                               [1, 256, 4, 4]            (recursive)\n",
       "│    │    └─Upsample: 3-44                                             [1, 256, 32, 32]          --\n",
       "├─MiT: 1-43                                                            [1, 32, 32, 32]           (recursive)\n",
       "│    └─ModuleList: 2-29                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-69                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-70                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-71                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-72                                           --                        (recursive)\n",
       "├─GELU: 1-44                                                           [1, 32, 32, 32]           --\n",
       "├─GELU: 1-45                                                           [1, 64, 16, 16]           --\n",
       "├─GELU: 1-46                                                           [1, 128, 8, 8]            --\n",
       "├─GELU: 1-47                                                           [1, 256, 4, 4]            --\n",
       "├─Dropout: 1-48                                                        [1, 32, 32, 32]           --\n",
       "├─Dropout: 1-49                                                        [1, 64, 16, 16]           --\n",
       "├─Dropout: 1-50                                                        [1, 128, 8, 8]            --\n",
       "├─Dropout: 1-51                                                        [1, 256, 4, 4]            --\n",
       "├─ModuleList: 1-74                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-20                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-49                                               [1, 256, 16, 16]          (recursive)\n",
       "│    │    └─Upsample: 3-50                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-21                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-51                                               [1, 256, 8, 8]            (recursive)\n",
       "│    │    └─Upsample: 3-52                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-22                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-53                                               [1, 256, 4, 4]            (recursive)\n",
       "│    │    └─Upsample: 3-54                                             [1, 256, 32, 32]          --\n",
       "├─CBAM: 1-53                                                           [1, 1536, 32, 32]         (recursive)\n",
       "│    └─ChannelGate: 2-23                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─Sequential: 3-55                                           [1, 1536]                 (recursive)\n",
       "│    │    └─Sequential: 3-56                                           [1, 1536]                 (recursive)\n",
       "│    └─SpatialGate: 2-24                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─ChannelPool: 3-57                                          [1, 2, 32, 32]            --\n",
       "│    │    └─BasicConv: 3-58                                            [1, 1, 32, 32]            (recursive)\n",
       "├─Conv2d: 1-54                                                         [1, 768, 32, 32]          (recursive)\n",
       "├─MiT: 1-55                                                            [1, 32, 32, 32]           (recursive)\n",
       "│    └─ModuleList: 2-29                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-69                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-70                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-71                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-72                                           --                        (recursive)\n",
       "├─GELU: 1-56                                                           [1, 32, 32, 32]           --\n",
       "├─GELU: 1-57                                                           [1, 64, 16, 16]           --\n",
       "├─GELU: 1-58                                                           [1, 128, 8, 8]            --\n",
       "├─GELU: 1-59                                                           [1, 256, 4, 4]            --\n",
       "├─Dropout: 1-60                                                        [1, 32, 32, 32]           --\n",
       "├─Dropout: 1-61                                                        [1, 64, 16, 16]           --\n",
       "├─Dropout: 1-62                                                        [1, 128, 8, 8]            --\n",
       "├─Dropout: 1-63                                                        [1, 256, 4, 4]            --\n",
       "├─ModuleList: 1-74                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-26                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-63                                               [1, 256, 16, 16]          (recursive)\n",
       "│    │    └─Upsample: 3-64                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-27                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-65                                               [1, 256, 8, 8]            (recursive)\n",
       "│    │    └─Upsample: 3-66                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-28                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-67                                               [1, 256, 4, 4]            (recursive)\n",
       "│    │    └─Upsample: 3-68                                             [1, 256, 32, 32]          --\n",
       "├─MiT: 1-65                                                            [1, 32, 32, 32]           (recursive)\n",
       "│    └─ModuleList: 2-29                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-69                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-70                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-71                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-72                                           --                        (recursive)\n",
       "├─GELU: 1-66                                                           [1, 32, 32, 32]           --\n",
       "├─GELU: 1-67                                                           [1, 64, 16, 16]           --\n",
       "├─GELU: 1-68                                                           [1, 128, 8, 8]            --\n",
       "├─GELU: 1-69                                                           [1, 256, 4, 4]            --\n",
       "├─Dropout: 1-70                                                        [1, 32, 32, 32]           --\n",
       "├─Dropout: 1-71                                                        [1, 64, 16, 16]           --\n",
       "├─Dropout: 1-72                                                        [1, 128, 8, 8]            --\n",
       "├─Dropout: 1-73                                                        [1, 256, 4, 4]            --\n",
       "├─ModuleList: 1-74                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-30                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-73                                               [1, 256, 16, 16]          (recursive)\n",
       "│    │    └─Upsample: 3-74                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-31                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-75                                               [1, 256, 8, 8]            (recursive)\n",
       "│    │    └─Upsample: 3-76                                             [1, 256, 32, 32]          --\n",
       "│    └─Sequential: 2-32                                                [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Conv2d: 3-77                                               [1, 256, 4, 4]            (recursive)\n",
       "│    │    └─Upsample: 3-78                                             [1, 256, 32, 32]          --\n",
       "├─CBAM: 1-75                                                           [1, 1536, 32, 32]         (recursive)\n",
       "│    └─ChannelGate: 2-33                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─Sequential: 3-79                                           [1, 1536]                 (recursive)\n",
       "│    │    └─Sequential: 3-80                                           [1, 1536]                 (recursive)\n",
       "│    └─SpatialGate: 2-34                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─ChannelPool: 3-81                                          [1, 2, 32, 32]            --\n",
       "│    │    └─BasicConv: 3-82                                            [1, 1, 32, 32]            (recursive)\n",
       "├─Conv2d: 1-76                                                         [1, 768, 32, 32]          (recursive)\n",
       "├─CBAM: 1-77                                                           [1, 1536, 32, 32]         --\n",
       "│    └─ChannelGate: 2-35                                               [1, 1536, 32, 32]         --\n",
       "│    │    └─Sequential: 3-83                                           [1, 1536]                 296,544\n",
       "│    │    └─Sequential: 3-84                                           [1, 1536]                 (recursive)\n",
       "│    └─SpatialGate: 2-36                                               [1, 1536, 32, 32]         --\n",
       "│    │    └─ChannelPool: 3-85                                          [1, 2, 32, 32]            --\n",
       "│    │    └─BasicConv: 3-86                                            [1, 1, 32, 32]            100\n",
       "├─Conv2d: 1-78                                                         [1, 768, 32, 32]          (recursive)\n",
       "├─CBAM: 1-79                                                           [1, 1536, 32, 32]         (recursive)\n",
       "│    └─ChannelGate: 2-37                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─Sequential: 3-87                                           [1, 1536]                 (recursive)\n",
       "│    │    └─Sequential: 3-88                                           [1, 1536]                 (recursive)\n",
       "│    └─SpatialGate: 2-38                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─ChannelPool: 3-89                                          [1, 2, 32, 32]            --\n",
       "│    │    └─BasicConv: 3-90                                            [1, 1, 32, 32]            (recursive)\n",
       "├─Conv2d: 1-80                                                         [1, 768, 32, 32]          (recursive)\n",
       "├─CBAM: 1-81                                                           [1, 1536, 32, 32]         (recursive)\n",
       "│    └─ChannelGate: 2-39                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─Sequential: 3-91                                           [1, 1536]                 (recursive)\n",
       "│    │    └─Sequential: 3-92                                           [1, 1536]                 (recursive)\n",
       "│    └─SpatialGate: 2-40                                               [1, 1536, 32, 32]         (recursive)\n",
       "│    │    └─ChannelPool: 3-93                                          [1, 2, 32, 32]            --\n",
       "│    │    └─BasicConv: 3-94                                            [1, 1, 32, 32]            (recursive)\n",
       "├─Conv2d: 1-82                                                         [1, 768, 32, 32]          (recursive)\n",
       "├─Head: 1-83                                                           [1, 11, 32, 32]           --\n",
       "│    └─Sequential: 2-41                                                [1, 11, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-95                                               [1, 1536, 32, 32]         3,540,480\n",
       "│    │    └─GELU: 3-96                                                 [1, 1536, 32, 32]         --\n",
       "│    │    └─Dropout: 3-97                                              [1, 1536, 32, 32]         --\n",
       "│    │    └─Conv2d: 3-98                                               [1, 768, 32, 32]          1,180,416\n",
       "│    │    └─Conv2d: 3-99                                               [1, 11, 32, 32]           8,459\n",
       "========================================================================================================================\n",
       "Total params: 18,022,771\n",
       "Trainable params: 18,022,771\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 14.37\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.32\n",
       "Forward/backward pass size (MB): 168.90\n",
       "Params size (MB): 72.06\n",
       "Estimated Total Size (MB): 241.28\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device\n",
    "print(f'Using {device} device')\n",
    "outer_model = ARC_Net(**model_args).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(outer_model, input_size=((1, 11, 32, 32), (1, 33, 32, 32), (1, 33, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = torch.zeros(1, 30, 30)\n",
    "# tensor = F.one_hot(tensor.long(), num_classes=11)\n",
    "# print(tensor.shape)\n",
    "# tensor = tensor.permute(0, 3, 1, 2)\n",
    "# print(tensor.shape)\n",
    "# tensor = tensor.squeeze(0)    \n",
    "# print(tensor.shape)\n",
    "# print(tensor.dtype)\n",
    "# tensor = tensor.to(torch.float32)  # 수정된 부분\n",
    "# print(tensor.dtype)\n",
    "# tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 11, 32, 32]) torch.Size([10, 1, 32, 32]) torch.Size([10, 33, 32, 32]) torch.Size([10, 33, 32, 32])\n",
      "torch.float32 torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "def one_hot_encoding(tensor):\n",
    "    tensor = F.one_hot(tensor.long(), num_classes=11)\n",
    "    tensor = tensor.permute(0, 3, 1, 2)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    # 여기서 명시적으로 dtype을 float32로 설정합니다\n",
    "    tensor = tensor.to(torch.float32)  # 수정된 부분\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class ARC_Dataset(Dataset):\n",
    "    def __init__(self, challenges, solution, task_data_num=1, example_data_num=3, max_combinations=10):\n",
    "        challenges = load_json(challenges)\n",
    "        solution = load_json(solution)\n",
    "        self.data = []\n",
    "        self.task_data_num = task_data_num\n",
    "        self.example_data_num = example_data_num\n",
    "        self.max_combinations = max_combinations\n",
    "\n",
    "        for key, value in challenges.items():\n",
    "            for i in range(len(value['test'])):\n",
    "                task_input = value['test'][i]['input']\n",
    "                task_output = solution[key][i]\n",
    "                example_list = value['train'].copy()\n",
    "\n",
    "                n_examples = len(example_list)  # 원래 예제의 개수\n",
    "\n",
    "                # 예제 수가 example_data_num보다 적으면 현재 작업의 예제를 복제하여 채움\n",
    "                if n_examples < self.example_data_num:\n",
    "                    while len(example_list) < self.example_data_num:\n",
    "                        example_list.append(random.choice(example_list))\n",
    "\n",
    "                    # 조합은 예제 리스트 자체로 설정\n",
    "                    ex_combinations = [tuple(example_list)]\n",
    "                else:\n",
    "                    # 예제의 조합 생성\n",
    "                    ex_combinations = list(combinations(example_list, self.example_data_num))\n",
    "\n",
    "                    # 조합의 수를 제한\n",
    "                    if len(ex_combinations) > self.max_combinations:\n",
    "                        ex_combinations = random.sample(ex_combinations, self.max_combinations)\n",
    "\n",
    "                for ex_combo in ex_combinations:\n",
    "                    ex_input = [ex['input'] for ex in ex_combo]\n",
    "                    ex_output = [ex['output'] for ex in ex_combo]\n",
    "\n",
    "                    # 데이터에 추가하면서 원래 예제의 개수도 포함\n",
    "                    self.data.append({\n",
    "                        'id': key,\n",
    "                        'input': task_input,\n",
    "                        'output': task_output,\n",
    "                        'ex_input': ex_input,\n",
    "                        'ex_output': ex_output,\n",
    "                        'num_original_examples': n_examples  # 원래 예제 개수 추가\n",
    "                    })\n",
    "\n",
    "        # 리스트를 데이터프레임으로 변환\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def pad_to_32x32(self, tensor):\n",
    "        if tensor.dim() == 2:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        c, h, w = tensor.shape\n",
    "        pad_h = max(0, 32 - h)\n",
    "        pad_w = max(0, 32 - w)\n",
    "        \n",
    "        # 좌우 및 상하 패딩을 반반씩 나눠서 적용\n",
    "        padding = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n",
    "        tensor = F.pad(tensor, padding, mode='constant', value=0)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    def mapping_input(self, tensor):\n",
    "        mapping = {\n",
    "            1: random.randint(1, 10),\n",
    "            2: random.randint(11, 20),\n",
    "            3: random.randint(21, 30),\n",
    "            4: random.randint(31, 40),\n",
    "            5: random.randint(41, 50),\n",
    "            6: random.randint(51, 60),\n",
    "            7: random.randint(61, 70),\n",
    "            8: random.randint(71, 80),\n",
    "            9: random.randint(81, 90),\n",
    "            10: random.randint(91, 100)\n",
    "        }\n",
    "        temp_tensor = tensor.clone()\n",
    "        for k in mapping:\n",
    "            temp_tensor[temp_tensor == k] = -k  # 임시로 기존 값에 음수를 취해 중복을 피함\n",
    "\n",
    "        # 최종 매핑 적용\n",
    "        for k, v in mapping.items():\n",
    "            temp_tensor[temp_tensor == -k] = v\n",
    "        return temp_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task = self.df.iloc[idx]\n",
    "        \n",
    "        # task_input과 task_output 변환 및 패딩 추가\n",
    "        task_input = self.pad_to_32x32((torch.tensor(task['input'], dtype=torch.float32) + 1)) # [1, 32, 32]\n",
    "        task_output = self.pad_to_32x32((torch.tensor(task['output'], dtype=torch.float32) + 1)) # [1, 32, 32]\n",
    "        task_input = one_hot_encoding(task_input)\n",
    "        \n",
    "        # 입력 채널 차원 추가\n",
    "        if task_input.dim() == 2:\n",
    "            task_input = task_input.unsqueeze(0)  # [1, H, W]\n",
    "        if task_output.dim() == 2:\n",
    "            task_output = task_output.unsqueeze(0)  # [1, H, W]\n",
    "        \n",
    "        # 예제 입력과 출력 변환 및 패딩 추가\n",
    "        example_input = []\n",
    "        example_output = []\n",
    "        for ex_in, ex_out in zip(task['ex_input'], task['ex_output']):\n",
    "            ex_in_tensor = self.pad_to_32x32(torch.tensor(ex_in, dtype=torch.float32) + 1)\n",
    "            ex_out_tensor = self.pad_to_32x32(torch.tensor(ex_out, dtype=torch.float32) + 1)\n",
    "            ex_in_tensor = one_hot_encoding(ex_in_tensor)\n",
    "            ex_out_tensor = one_hot_encoding(ex_out_tensor)\n",
    "            \n",
    "            # 입력 채널 차원 추가\n",
    "            if ex_in_tensor.dim() == 2:\n",
    "                ex_in_tensor = ex_in_tensor.unsqueeze(0)  # [1, H, W]\n",
    "            if ex_out_tensor.dim() == 2:\n",
    "                ex_out_tensor = ex_out_tensor.unsqueeze(0)  # [1, H, W]\n",
    "            \n",
    "            example_input.append(ex_in_tensor)\n",
    "            example_output.append(ex_out_tensor)\n",
    "        \n",
    "        # 예제 입력과 출력을 채널 차원으로 결합\n",
    "        ex_inputs = torch.cat(example_input, dim=0)  # [n_examples * channels, H, W]\n",
    "        ex_outputs = torch.cat(example_output, dim=0)  # [n_examples * channels, H, W]\n",
    "        \n",
    "        return task_input, task_output, ex_inputs, ex_outputs\n",
    "\n",
    "# 사용 예제\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "ti, to, ei, eo = next(iter(train_loader))\n",
    "print(ti.shape, to.shape, ei.shape, eo.shape)\n",
    "print(ti.dtype, to.dtype, ei.dtype, eo.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치별로 처리되는 정확도 계산 함수 (ignore_index 없이)\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    # 예측값을 argmax로 [batch_size, H, W] 형태로 변환\n",
    "    pred_classes = predictions.argmax(dim=1).long()  # [batch_size, H, W]\n",
    "    \n",
    "    # 타겟 값을 [batch_size, H, W]로 변환 (필요 시 squeeze)\n",
    "    targets = targets.squeeze(1)  # [batch_size, H, W]\n",
    "\n",
    "    # 픽셀 단위로 맞은 부분 계산\n",
    "    correct_pixels = (pred_classes == targets)  # [batch_size, H, W]\n",
    "    \n",
    "    # 맞은 픽셀 수 계산\n",
    "    correct_pixel_count = correct_pixels.sum().item()  # 맞은 픽셀 수\n",
    "    total_pixel_count = targets.numel()  # 전체 픽셀 수\n",
    "\n",
    "    # 각 이미지별로 모든 픽셀이 맞는지 확인\n",
    "    correct_images = correct_pixels.view(targets.size(0), -1).all(dim=1)  # 각 이미지별로 모든 픽셀이 맞는지 확인\n",
    "    correct_image_count = correct_images.sum().item()  # 맞은 이미지 수\n",
    "    total_image_count = targets.size(0)  # 전체 이미지 수\n",
    "    \n",
    "    return correct_image_count, total_image_count, correct_pixel_count, total_pixel_count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import wandb\n",
    "\n",
    "def visualize_predictions(inputs, targets, predictions, condition=False):\n",
    "    if condition:\n",
    "        # 입력 이미지와 예측 결과를 CPU로 이동\n",
    "        inputs = inputs.cpu().numpy()\n",
    "        targets = targets.cpu().numpy()\n",
    "        predictions = predictions.detach().cpu().numpy()  # detach()로 그래디언트 추적 중단\n",
    "\n",
    "        # 시각화할 샘플의 수 (최대 3개의 샘플)\n",
    "        num_images = min(3, inputs.shape[0])\n",
    "\n",
    "        # 컬러 맵 정의 (0~10 값을 위한 11개의 색상)\n",
    "        color_list = ['black', 'blue', 'red', 'green', 'yellow', 'purple', \n",
    "                      'orange', 'pink', 'gray', 'brown', 'cyan']\n",
    "        cmap = ListedColormap(color_list)\n",
    "\n",
    "        # 시각화를 위한 플롯 생성 (각 줄에 입력, 타겟, 예측 이미지를 표시)\n",
    "        fig, axes = plt.subplots(num_images, 3, figsize=(12, 4 * num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            # 입력 이미지 처리\n",
    "            input_image = inputs.argmax(axis=1)[i]  # shape: (H, W)\n",
    "            if input_image.ndim == 3 and input_image.shape[0] == 1:\n",
    "                input_image = input_image.squeeze(0)  # 단일 채널 이미지 (H, W)\n",
    "\n",
    "            # 타겟 이미지 처리\n",
    "            target_image = targets[i].squeeze(0)  # shape: (H, W)\n",
    "            target_image = target_image.astype(int)\n",
    "\n",
    "            # 예측 이미지 처리\n",
    "            prediction_image = predictions.argmax(axis=1)[i]  # shape: (H, W)\n",
    "            prediction_image = prediction_image.astype(int)\n",
    "\n",
    "            # 입력 이미지 표시\n",
    "            axes[i, 0].imshow(input_image, cmap=cmap, vmin=0, vmax=10)\n",
    "            axes[i, 0].set_title(f'Input Image {i+1}')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            # 타겟 이미지 표시\n",
    "            axes[i, 1].imshow(target_image, cmap=cmap, vmin=0, vmax=10)\n",
    "            axes[i, 1].set_title(f'Ground Truth {i+1}')\n",
    "            axes[i, 1].axis('off')\n",
    "\n",
    "            # 예측 이미지 표시\n",
    "            axes[i, 2].imshow(prediction_image, cmap=cmap, vmin=0, vmax=10)\n",
    "            axes[i, 2].set_title(f'Prediction {i+1}')\n",
    "            axes[i, 2].axis('off')\n",
    "\n",
    "        plt.tight_layout()  # 레이아웃을 자동으로 조정\n",
    "        \n",
    "        # WandB에 이미지 업로드\n",
    "        wandb.log({\"Predictions\": wandb.Image(plt)})\n",
    "        \n",
    "        # 범례를 플롯 내에 표시\n",
    "        patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_list[j], markersize=10, label=str(j)) for j in range(11)]\n",
    "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 11, 32, 32]) torch.float32\n",
      "torch.Size([4, 32, 32]) torch.int64\n",
      "============\n",
      "torch.Size([4, 32, 32]) torch.int64\n",
      "Correct samples: 1/4\n",
      "Correct pixels: 1311/4096\n"
     ]
    }
   ],
   "source": [
    "# 예시 테스트 코드\n",
    "batch_size = 4\n",
    "height = 32\n",
    "width = 32\n",
    "num_classes = 11  # 11개의 클래스를 가정\n",
    "ignore_index = 0\n",
    "# 임의의 예측값 생성 (logits 형태로, [batch_size, num_classes, H, W]의 크기)\n",
    "predictions = torch.randn(batch_size, num_classes, height, width)\n",
    "print(predictions.shape, predictions.dtype)\n",
    "predictions = predictions.argmax(dim=1)\n",
    "\n",
    "print(predictions.shape, predictions.dtype)\n",
    "print(\"============\")\n",
    "# 임의의 타겟값 생성 (정수값으로, [batch_size, 1, H, W]의 크기)\n",
    "targets = torch.randint(0, num_classes, (batch_size, 1, height, width))\n",
    "targets = targets.squeeze(1)  # [batch_size, H, W]\n",
    "print(targets.shape, targets.dtype)\n",
    "\n",
    "if ignore_index is not None:\n",
    "    mask = (targets != ignore_index)\n",
    "else:\n",
    "    mask = torch.ones_like(targets, dtype=torch.bool)\n",
    "    \n",
    "# 첫 번째와 두 번째 샘플의 예측값을 타겟값과 완전히 동일하게 설정\n",
    "predictions[0] = targets[0]\n",
    "# predictions[1] = targets[1]\n",
    "\n",
    "# 배치별로 완전히 동일한 행렬 확인\n",
    "correct_samples = (predictions == targets).all(dim=(1, 2)).sum().item()\n",
    "total_samples = targets.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "correct_pixels = (predictions == targets).sum().item()\n",
    "total_pixels = targets.numel()\n",
    "\n",
    "print(f'Correct samples: {correct_samples}/{total_samples}')\n",
    "print(f'Correct pixels: {correct_pixels}/{total_pixels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 11, 32, 32]) torch.float32\n",
      "torch.Size([4, 1, 32, 32]) torch.int64\n",
      "이미지 단위 정확도: 2/4\n",
      "픽셀 단위 정확도: 2233/4096\n"
     ]
    }
   ],
   "source": [
    "# 예시 테스트 코드\n",
    "batch_size = 4\n",
    "height = 32\n",
    "width = 32\n",
    "num_classes = 11  # 11개의 클래스를 가정\n",
    "\n",
    "# 임의의 예측값 생성 (logits 형태로, [batch_size, num_classes, H, W]의 크기)\n",
    "predictions = torch.randn(batch_size, num_classes, height, width)\n",
    "print(predictions.shape, predictions.dtype)\n",
    "\n",
    "# 임의의 타겟값 생성 (정수값으로, [batch_size, 1, H, W]의 크기)\n",
    "targets = torch.randint(0, num_classes, (batch_size, 1, height, width))\n",
    "print(targets.shape, targets.dtype)\n",
    "# 첫 번째와 두 번째 샘플의 예측값을 타겟값과 완전히 동일하게 설정\n",
    "for i in range(2):  # 첫 번째와 두 번째 샘플을 타겟과 맞춤\n",
    "    predictions[i] = torch.zeros_like(predictions[i])  # logits을 0으로 초기화\n",
    "    for c in range(num_classes):\n",
    "        predictions[i, c] = (targets[i].squeeze(0) == c).float() * 1000.0  # 타겟과 동일하게 맞춤\n",
    "\n",
    "# 정확도 계산 함수 호출\n",
    "correct_samples, total_samples, correct_pixels, total_pixels = calculate_accuracy(predictions, targets)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"이미지 단위 정확도: {correct_samples}/{total_samples}\")\n",
    "print(f\"픽셀 단위 정확도: {correct_pixels}/{total_pixels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 배치별로 처리되는 정확도 계산 함수 (앞서 작성한 함수)\n",
    "# def calculate_accuracy(predictions, targets, ignore_index=0):\n",
    "#     pred_classes = predictions.argmax(dim=1).long()  # [batch_size, H, W]\n",
    "#     targets = targets.squeeze(1)  # [batch_size, H, W]\n",
    "    \n",
    "#     if ignore_index is not None:\n",
    "#         mask = (targets != ignore_index)\n",
    "#     else:\n",
    "#         mask = torch.ones_like(targets, dtype=torch.bool)\n",
    "    \n",
    "#     correct_pixels = (pred_classes == targets) & mask  # [batch_size, H, W]\n",
    "#     correct_pixel_count = correct_pixels.sum().item()  # 맞은 픽셀 수\n",
    "#     total_pixel_count = mask.sum().item()  # 유효한 전체 픽셀 수\n",
    "    \n",
    "#     correct_images = correct_pixels.view(targets.size(0), -1).all(dim=1)  # 각 이미지별로 모든 픽셀이 맞는지 확인\n",
    "#     correct_image_count = correct_images.sum().item()  # 맞은 이미지 수\n",
    "#     total_image_count = targets.size(0)  # 전체 이미지 수\n",
    "    \n",
    "#     return correct_image_count, total_image_count, correct_pixel_count, total_pixel_count\n",
    "\n",
    "# # 누적된 정확도를 계산하고 테스트하는 함수\n",
    "# def test_batch_accuracy():\n",
    "#     batch_size = 4\n",
    "#     height = 32\n",
    "#     width = 32\n",
    "#     num_classes = 11\n",
    "#     num_batches = 3  # 3개의 배치 처리 가정\n",
    "\n",
    "#     total_correct_samples = 0\n",
    "#     total_samples = 0\n",
    "#     total_correct_pixels = 0\n",
    "#     total_pixels = 0\n",
    "\n",
    "#     for batch_idx in range(num_batches):\n",
    "#         # 예측값과 타겟값 생성 (여기서는 간단하게 무작위로 생성)\n",
    "#         predictions = torch.randn(batch_size, num_classes, height, width)\n",
    "#         targets = torch.randint(0, num_classes, (batch_size, 1, height, width))\n",
    "\n",
    "#         # 첫 번째 배치는 완전히 정확하게 설정\n",
    "#         if batch_idx == 0:\n",
    "#             for i in range(batch_size):\n",
    "#                 predictions[i] = torch.zeros_like(predictions[i])\n",
    "#                 for c in range(num_classes):\n",
    "#                     predictions[i, c] = (targets[i].squeeze(0) == c).float() * 1000.0\n",
    "\n",
    "#         # 두 번째 배치는 절반만 정확하게 설정\n",
    "#         elif batch_idx == 1:\n",
    "#             for i in range(batch_size // 2):  # 첫 번째 절반만 정확하게\n",
    "#                 predictions[i] = torch.zeros_like(predictions[i])\n",
    "#                 for c in range(num_classes):\n",
    "#                     predictions[i, c] = (targets[i].squeeze(0) == c).float() * 1000.0\n",
    "        \n",
    "#         # 세 번째 배치는 전부 틀리게 설정\n",
    "#         else:\n",
    "#             predictions = torch.zeros_like(predictions)\n",
    "#             targets = torch.ones(batch_size, 1, height, width).long() * (num_classes - 1)\n",
    "\n",
    "#         # 배치별 정확도 계산\n",
    "#         correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(predictions, targets, ignore_index=None)\n",
    "        \n",
    "#         # 누적 정확도 계산\n",
    "#         total_correct_samples += correct_samples\n",
    "#         total_samples += batch_total_samples\n",
    "#         total_correct_pixels += correct_pixels\n",
    "#         total_pixels += batch_total_pixels\n",
    "\n",
    "#     # 전체 배치에서 누적된 정확도 계산\n",
    "#     avg_sample_accuracy = 100. * float(total_correct_samples) / float(total_samples)\n",
    "#     avg_pixel_accuracy = 100. * float(total_correct_pixels) / float(total_pixels)\n",
    "\n",
    "#     print(f\"전체 이미지 단위 정확도: {avg_sample_accuracy:.2f}%\")\n",
    "#     print(f\"전체 픽셀 단위 정확도: {avg_pixel_accuracy:.2f}%\")\n",
    "\n",
    "# # 테스트 함수 실행\n",
    "# test_batch_accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/seungwoo/Workspace/ARC/wandb/run-20240922_182340-rrpk2jov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waooang/arc-agi-training/runs/rrpk2jov' target=\"_blank\">northern-glade-4</a></strong> to <a href='https://wandb.ai/waooang/arc-agi-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/waooang/arc-agi-training' target=\"_blank\">https://wandb.ai/waooang/arc-agi-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/waooang/arc-agi-training/runs/rrpk2jov' target=\"_blank\">https://wandb.ai/waooang/arc-agi-training/runs/rrpk2jov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  10%|▉         | 2/21 [00:05<00:49,  2.61s/it, loss=2.1957]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "train_args = {\n",
    "    'challenges': train_challenge,\n",
    "    'solution': train_solution,\n",
    "    'num_classes': 11,\n",
    "    'batch_size': 48,\n",
    "    'epochs': 1000,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'dims': (16,32,64,128),\n",
    "    'heads': 4,\n",
    "    'ff_expansion': 4,\n",
    "    'reduction_ratio': (4,2,2,2),\n",
    "    'num_layers': (1,2,3,4),\n",
    "    'channels': 11,\n",
    "    'num_classes': 11,\n",
    "    'kernel_stride_paddings': ((3, 1, 1),(3, 2, 1),(3, 2, 1),(3, 2, 1))\n",
    "    }\n",
    "# WandB 설정\n",
    "wandb.init(project=\"arc-agi-training\")\n",
    "\n",
    "# train_args와 model_args를 config에 추가\n",
    "wandb.config.update(train_args)\n",
    "wandb.config.update(model_args)\n",
    "\n",
    "best_test_accuracy = 0  # 최고 test 정확도를 기록할 변수\n",
    "\n",
    "# 모델 저장 함수\n",
    "def save_checkpoint(state, filename=\"best_model.pth\"):\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    weight = torch.ones(train_args['num_classes']).to(y.device)\n",
    "    weight[0] = 0.008\n",
    "    weight[1] = 0.8\n",
    "    ce = F.cross_entropy(y_pred, y, weight=weight) \n",
    "    return ce\n",
    "\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_args['batch_size'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(eval_challenge, eval_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=train_args['batch_size'], shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "model = ARC_Net(**model_args).to(device)\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.AdamW(model.parameters(), lr=train_args['learning_rate'])\n",
    "\n",
    "# train 함수 수정\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct_samples = 0\n",
    "    total_samples = 0\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    last_task_inputs = None\n",
    "    last_task_outputs = None\n",
    "    last_output = None\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{train_args[\"epochs\"]}', leave=False)\n",
    "\n",
    "    for batch_idx, (task_inputs, task_outputs, ex_inputs, ex_outputs) in progress_bar:\n",
    "        task_inputs = task_inputs.to(device, non_blocking=True)\n",
    "        task_outputs = task_outputs.to(device, non_blocking=True)\n",
    "        ex_inputs = ex_inputs.to(device, non_blocking=True)\n",
    "        ex_outputs = ex_outputs.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(task_inputs, ex_inputs, ex_outputs)\n",
    "\n",
    "        # 손실 함수 계산\n",
    "        loss = criterion(output, task_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(output, task_outputs)\n",
    "        total_correct_samples += correct_samples\n",
    "        total_samples += batch_total_samples\n",
    "        total_correct_pixels += correct_pixels\n",
    "        total_pixels += batch_total_pixels\n",
    "\n",
    "        # 마지막 배치의 데이터 저장 (clone 제거)\n",
    "        last_task_inputs = task_inputs.detach()\n",
    "        last_task_outputs = task_outputs.detach()\n",
    "        last_output = output.detach()\n",
    "        \n",
    "        # 프로그레스 바 업데이트\n",
    "        progress_bar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    \n",
    "    avg_sample_accuracy = 100. * float(total_correct_samples) / float(total_samples)\n",
    "    avg_pixel_accuracy = 100. * float(total_correct_pixels) / float(total_pixels)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # WandB 로그 기록\n",
    "    wandb.log({\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"train_sample_accuracy\": avg_sample_accuracy,\n",
    "        \"train_pixel_accuracy\": avg_pixel_accuracy,\n",
    "        \"epoch\": epoch+1\n",
    "    })\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{train_args[\"epochs\"]}] Complete | Average Loss: {avg_loss:.6f} | Training Sample Accuracy: {avg_sample_accuracy:.4f}% | Training Pixel Accuracy: {avg_pixel_accuracy:.2f}%')\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        visualize_predictions(last_task_inputs, last_task_outputs, last_output, condition=True)\n",
    "\n",
    "# test 함수 수정\n",
    "def test(epoch):\n",
    "    global best_test_accuracy\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct_samples = 0\n",
    "    total_samples = 0\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    last_task_inputs = None\n",
    "    last_task_outputs = None\n",
    "    last_output = None\n",
    "\n",
    "    progress_bar = tqdm(enumerate(eval_loader), total=len(eval_loader), desc='Testing', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (task_inputs, task_outputs, ex_inputs, ex_outputs) in progress_bar:\n",
    "            task_inputs = task_inputs.to(device, non_blocking=True)\n",
    "            task_outputs = task_outputs.to(device, non_blocking=True)\n",
    "            ex_inputs = ex_inputs.to(device, non_blocking=True)\n",
    "            ex_outputs = ex_outputs.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(task_inputs, ex_inputs, ex_outputs)\n",
    "\n",
    "            # 손실 함수 계산\n",
    "            loss = criterion(output, task_outputs)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(output, task_outputs)\n",
    "            total_correct_samples += correct_samples\n",
    "            total_samples += batch_total_samples\n",
    "            total_correct_pixels += correct_pixels\n",
    "            total_pixels += batch_total_pixels\n",
    "\n",
    "            # 마지막 배치의 데이터 저장 (clone 제거)\n",
    "            last_task_inputs = task_inputs.detach()\n",
    "            last_task_outputs = task_outputs.detach()\n",
    "            last_output = output.detach()\n",
    "            \n",
    "            # 프로그레스 바 업데이트\n",
    "            progress_bar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    \n",
    "    avg_sample_accuracy = 100. * float(total_correct_samples) / float(total_samples)\n",
    "    avg_pixel_accuracy = 100. * float(total_correct_pixels) / float(total_pixels)\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "\n",
    "    # WandB 로그 기록\n",
    "    wandb.log({\n",
    "        \"test_loss\": avg_loss,\n",
    "        \"test_sample_accuracy\": avg_sample_accuracy,\n",
    "        \"test_pixel_accuracy\": avg_pixel_accuracy,\n",
    "        \"epoch\": epoch+1\n",
    "    })\n",
    "\n",
    "    print(f'Test Average Loss: {avg_loss:.6f} | Test Sample Accuracy: {avg_sample_accuracy:.4f}% | Test Pixel Accuracy: {avg_pixel_accuracy:.2f}%')\n",
    "\n",
    "    # 최고 성능 모델 저장\n",
    "    if avg_sample_accuracy > best_test_accuracy:\n",
    "        print(f\"New best model found! Saving model with accuracy {avg_sample_accuracy:.4f}%\")\n",
    "        best_test_accuracy = avg_sample_accuracy\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_test_accuracy': best_test_accuracy,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, filename=\"best_model.pth\")\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        visualize_predictions(last_task_inputs, last_task_outputs, last_output, condition=True)\n",
    "\n",
    "# 학습 실행\n",
    "for epoch in range(train_args['epochs']):  \n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "# WandB 종료\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
