{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class DsConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias=bias),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "## 디코더\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from math import sqrt\n",
    "\n",
    "def cast_tuple(val, depth):\n",
    "    return val if isinstance(val, tuple) else (val,) * depth\n",
    "\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads,\n",
    "        reduction_ratio\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim * 2, reduction_ratio, stride = reduction_ratio, bias = False)\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = heads), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h = heads, x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class MixFeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        expansion_factor\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DsConv2d(hidden_dim, hidden_dim, 3, padding = 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "## 컨볼루션 임베딩\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dim_in,dim_out, kernel_size, stride, padding):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Conv2d(dim_in, \n",
    "                                   dim_out, \n",
    "                                   kernel_size=kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "'''\n",
    "5. MiT (Mixer Transformer)\n",
    "이미지를 여러 스테이지로 처리합니다. 각 스테이지는 이미지를 패치로 나누고, 패치를 임베딩한 후, 여러 개의 Transformer 레이어를 적용합니다.\n",
    "이 과정은 이미지의 다양한 해상도에서 특징을 추출합니다. \n",
    "'''    \n",
    "class MiT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        channels,\n",
    "        dims,\n",
    "        heads,\n",
    "        ff_expansion,\n",
    "        reduction_ratio,\n",
    "        num_layers,\n",
    "        stage_kernel_stride_pad = ((7, 4, 3),  \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1))\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        dims = (channels, *dims)\n",
    "        dim_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        self.stages = nn.ModuleList([])\n",
    "\n",
    "        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(\n",
    "            dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n",
    "            #여기서 너비와 높이가 같은 정사각형 패치를 사용합니다.\n",
    "            get_overlap_patches = nn.Unfold(kernel, stride = stride, padding = padding)\n",
    "            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n",
    "\n",
    "            layers = nn.ModuleList([])\n",
    "\n",
    "            for _ in range(num_layers):\n",
    "                layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim_out, EfficientSelfAttention(dim = dim_out, heads = heads, reduction_ratio = reduction_ratio)),\n",
    "                    PreNorm(dim_out, MixFeedForward(dim = dim_out, expansion_factor = ff_expansion)),\n",
    "                ]))\n",
    "\n",
    "            self.stages.append(nn.ModuleList([\n",
    "                get_overlap_patches,\n",
    "                overlap_patch_embed,\n",
    "                layers\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        return_layer_outputs = False\n",
    "    ):\n",
    "        h, w = x.shape[-2:]\n",
    "        \n",
    "        \n",
    "        layer_outputs = []\n",
    "        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n",
    "            x = get_overlap_patches(x)\n",
    "            \n",
    "            num_patches = x.shape[-1]\n",
    "            ratio = int(sqrt((h * w) / num_patches))\n",
    "            \n",
    "            x = rearrange(x, 'b c (h w) -> b c h w', h = h // ratio)\n",
    "\n",
    "            x = overlap_embed(x)\n",
    "            for (attn, ff) in layers:\n",
    "                x = attn(x) + x\n",
    "                x = ff(x) + x\n",
    "\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        ret = x if not return_layer_outputs else layer_outputs\n",
    "        return ret\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, input_dim = 256 ,dim=128, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(input_dim , (input_dim//3)*2, kernel_size=1),  \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d((input_dim//3)*2 , input_dim//3, kernel_size=1),  \n",
    "            nn.Conv2d(dim, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "'''\n",
    "6. Segformer\n",
    "MiT를 통해 추출된 여러 스케일의 특징을 결합하고, 최종적으로 세그멘테이션 맵을 생성합니다.\n",
    "각 스테이지의 출력을 디코더 차원으로 매핑하고, 업샘플링하여 동일한 해상도로 만든 후, 이를 결합합니다.\n",
    "결합된 특징 맵을 사용하여 최종 세그멘테이션 맵을 생성합니다.\n",
    "'''\n",
    "class ARC_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dims=(32, 64, 160, 256),\n",
    "        heads=(1, 2, 5, 8),\n",
    "        ff_expansion=(8, 8, 4, 4),\n",
    "        reduction_ratio=(8, 4, 2, 1),\n",
    "        num_layers=2,\n",
    "        channels=11,\n",
    "        num_classes=11,\n",
    "        kernel_stride_paddings = ((1, 1, 0),(3, 2, 1), (3, 2, 1), (3, 2, 1))\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        decoder_dim = dims[-1]\n",
    "        \n",
    "        dims, heads, ff_expansion, reduction_ratio, num_layers = map(\n",
    "            partial(cast_tuple, depth=len(kernel_stride_paddings)), (dims, heads, ff_expansion, reduction_ratio, num_layers))\n",
    "\n",
    "        \n",
    "        self.mit = MiT(\n",
    "            channels=channels,\n",
    "            dims=dims,\n",
    "            heads=heads,\n",
    "            ff_expansion=ff_expansion,\n",
    "            reduction_ratio=reduction_ratio,\n",
    "            num_layers=num_layers,\n",
    "            stage_kernel_stride_pad=kernel_stride_paddings\n",
    "        )\n",
    "\n",
    "        self.to_fused = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(dim, decoder_dim, 1),\n",
    "            nn.Upsample(scale_factor=2 ** i)\n",
    "        ) for i, dim in enumerate(dims)])\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.cbam = CBAM(gate_channels = decoder_dim * len(kernel_stride_paddings) * 2)  \n",
    "        \n",
    "        self.cbam2 = CBAM(gate_channels = decoder_dim * len(kernel_stride_paddings) * 2)\n",
    "        \n",
    "        self.reduce = nn.Conv2d(decoder_dim * len(dims) * 2, decoder_dim * len(dims), 1)        \n",
    "        \n",
    "        self.to_segmentation = Head(decoder_dim*len(dims) * 3, decoder_dim*len(dims) , num_classes)\n",
    "        \n",
    "    def _fusion(self, x):\n",
    "        x = self.mit(x, return_layer_outputs=True)\n",
    "        # 드랍아웃 \n",
    "        x = [self.gelu(x) for x in x]\n",
    "        x = [self.dropout(x) for x in x]\n",
    "        fused = []\n",
    "        for output, to_fused in zip(x, self.to_fused):\n",
    "            x = to_fused(output)  # Conv2d 적용\n",
    "            # 업샘플링하여 공간 크기를 맞춥니다.\n",
    "            fused.append(x)\n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        \n",
    "        return fused    \n",
    "    \n",
    "    def forward(self, x, ex_inputs, ex_outputs):\n",
    "        x = self._fusion(x) # [b,1,H,W] -> [b,dim_0,H,W],...,[b,dim_n, H//(n+1), W//(n+1)] \n",
    "                            # -> [b,dim_n, H, W],...,[b,dim_n, H, W] -> [b, dim_n * 2, H, W]\n",
    "                            # [b, 256, 32, 32]\n",
    "                            \n",
    "        # 예제 입력과 출력을 채널 차원으로 분할\n",
    "        n_examples = ex_inputs.size(1)//self.channels  # n_examples * channels (여기서는 3)\n",
    "        \n",
    "        # 예제 수만큼 반복하면서 예제 입력과 출력을 처리\n",
    "        fused = []\n",
    "        for i in range(n_examples):\n",
    "            ex_i = ex_inputs[:, i*self.channels:(i+1)*self.channels, :, :]  # [batch_size, 1, H, W]\n",
    "            ex_o = ex_outputs[:, i*self.channels:(i+1)*self.channels, :, :]  # [batch_size, 1, H, W]\n",
    "            \n",
    "            ex_i = self._fusion(ex_i)\n",
    "            ex_o = self._fusion(ex_o)\n",
    "            \n",
    "            ex_f = torch.cat([ex_i, ex_o], dim=1)\n",
    "            ex_f = self.cbam(ex_f)\n",
    "            ex_f = self.reduce(ex_f)\n",
    "            fused.append(ex_f)\n",
    "        \n",
    "        for i, ex_f in enumerate(fused):\n",
    "            ex_f = torch.cat([x, ex_f], dim=1)\n",
    "            ex_f = self.cbam2(ex_f)\n",
    "            ex_f = self.reduce(ex_f)\n",
    "            fused[i] = ex_f\n",
    "        \n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        output = self.to_segmentation(fused)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 생성 및 출력\n",
    "# model_args = {\n",
    "#     'dims': (16,32, 64, 128),\n",
    "#     'heads': 4,\n",
    "#     'ff_expansion': 4,\n",
    "#     'reduction_ratio': (4,2,2,2),\n",
    "#     'num_layers': (1,2,3,4),\n",
    "#     'channels': 11,\n",
    "#     'num_classes': 11,\n",
    "#     'kernel_stride_paddings': ((3, 1, 1),(3, 2, 1),(3, 2, 1),(3, 2, 1))\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims, heads, ff_expansion, reduction_ratio, num_layers = map(\n",
    "#             partial(cast_tuple, depth=4), (model_args['dims'], \n",
    "#                                            model_args['heads'], \n",
    "#                                            model_args['ff_expansion'], \n",
    "#                                            model_args['reduction_ratio'], \n",
    "#                                            model_args['num_layers']))\n",
    "# mit = MiT(\n",
    "#         dims=dims,\n",
    "#         heads=heads,\n",
    "#         ff_expansion=ff_expansion,\n",
    "#         reduction_ratio=reduction_ratio,\n",
    "#         num_layers=num_layers,\n",
    "#         channels=model_args['channels'],\n",
    "#         stage_kernel_stride_pad=model_args['kernel_stride_paddings'],\n",
    "        \n",
    "#           )\n",
    "# x = torch.randn(10, 1, 30, 30)\n",
    "\n",
    "# print(\"Input shape:\", x.shape)\n",
    "# x = mit(x,return_layer_outputs = True)\n",
    "\n",
    "# print(\"Output shape:\", x[0].shape)\n",
    "# print(\"Output shape:\", x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = torch.randn(10, 1, 30, 30)\n",
    "# i = 0\n",
    "# h = tensor[:, i:i+11, :, :]\n",
    "# h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse = [torch.randn(10, 256, 32, 32), torch.randn(10, 256, 32, 32), torch.randn(10, 256, 32, 32)]\n",
    "\n",
    "# x= torch.randn(10, 256, 32, 32)\n",
    "# for i , ex in enumerate(fuse) :\n",
    "#     print(\"fuse shape:\", ex.shape)\n",
    "#     ex = torch.cat([ex, x], dim=1)\n",
    "#     print(\"fuse shape:\", ex.shape)\n",
    "#     fuse[i] = ex\n",
    "#     print(\"fuse shape:\", fuse[i].shape)\n",
    "#     print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CUDA 사용 가능 여부 확인\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# device = 'cuda' if torch.cuda.is_available() else device\n",
    "# print(f'Using {device} device')\n",
    "# model = ARC_Net(**model_args).to(device)\n",
    "# # 입력 텐서 생성\n",
    "# x = torch.randn(10, 11, 32, 32).to(device)\n",
    "# e_i, e_o = torch.randn(10, 33, 32, 32).to(device), torch.randn(10, 33, 32, 32).to(device)\n",
    "# print(model(x,e_i,e_o).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thop import profile\n",
    "# from thop import clever_format\n",
    "\n",
    "# # CUDA 사용 가능 여부 확인\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# device = 'cuda' if torch.cuda.is_available() else device\n",
    "# print(f'Using {device} device')\n",
    "# outer_model = ARC_Net(**model_args).to(device)\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "# summary(outer_model, input_size=((1, 11, 32, 32), (1, 33, 32, 32), (1, 33, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = torch.zeros(1, 30, 30)\n",
    "# tensor = F.one_hot(tensor.long(), num_classes=11)\n",
    "# print(tensor.shape)\n",
    "# tensor = tensor.permute(0, 3, 1, 2)\n",
    "# print(tensor.shape)\n",
    "# tensor = tensor.squeeze(0)    \n",
    "# print(tensor.shape)\n",
    "# print(tensor.dtype)\n",
    "# tensor = tensor.to(torch.float32)  # 수정된 부분\n",
    "# print(tensor.dtype)\n",
    "# tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 11, 32, 32]) torch.Size([10, 1, 32, 32]) torch.Size([10, 33, 32, 32]) torch.Size([10, 33, 32, 32])\n",
      "torch.float32 torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "def one_hot_encoding(tensor):\n",
    "    tensor = F.one_hot(tensor.long(), num_classes=11)\n",
    "    tensor = tensor.permute(0, 3, 1, 2)\n",
    "    tensor = tensor.squeeze(0)\n",
    "    # 여기서 명시적으로 dtype을 float32로 설정합니다\n",
    "    tensor = tensor.to(torch.float32)  # 수정된 부분\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class ARC_Dataset(Dataset):\n",
    "    def __init__(self, challenges, solution, task_data_num=1, example_data_num=3, max_combinations=10):\n",
    "        challenges = load_json(challenges)\n",
    "        solution = load_json(solution)\n",
    "        self.data = []\n",
    "        self.task_data_num = task_data_num\n",
    "        self.example_data_num = example_data_num\n",
    "        self.max_combinations = max_combinations\n",
    "\n",
    "        for key, value in challenges.items():\n",
    "            for i in range(len(value['test'])):\n",
    "                task_input = value['test'][i]['input']\n",
    "                task_output = solution[key][i]\n",
    "                example_list = value['train'].copy()\n",
    "\n",
    "                n_examples = len(example_list)  # 원래 예제의 개수\n",
    "\n",
    "                # 예제 수가 example_data_num보다 적으면 현재 작업의 예제를 복제하여 채움\n",
    "                if n_examples < self.example_data_num:\n",
    "                    while len(example_list) < self.example_data_num:\n",
    "                        example_list.append(random.choice(example_list))\n",
    "\n",
    "                    # 조합은 예제 리스트 자체로 설정\n",
    "                    ex_combinations = [tuple(example_list)]\n",
    "                else:\n",
    "                    # 예제의 조합 생성\n",
    "                    ex_combinations = list(combinations(example_list, self.example_data_num))\n",
    "\n",
    "                    # 조합의 수를 제한\n",
    "                    if len(ex_combinations) > self.max_combinations:\n",
    "                        ex_combinations = random.sample(ex_combinations, self.max_combinations)\n",
    "\n",
    "                for ex_combo in ex_combinations:\n",
    "                    ex_input = [ex['input'] for ex in ex_combo]\n",
    "                    ex_output = [ex['output'] for ex in ex_combo]\n",
    "\n",
    "                    # 데이터에 추가하면서 원래 예제의 개수도 포함\n",
    "                    self.data.append({\n",
    "                        'id': key,\n",
    "                        'input': task_input,\n",
    "                        'output': task_output,\n",
    "                        'ex_input': ex_input,\n",
    "                        'ex_output': ex_output,\n",
    "                        'num_original_examples': n_examples  # 원래 예제 개수 추가\n",
    "                    })\n",
    "\n",
    "        # 리스트를 데이터프레임으로 변환\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def pad_to_32x32(self, tensor):\n",
    "        if tensor.dim() == 2:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        c, h, w = tensor.shape\n",
    "        pad_h = max(0, 32 - h)\n",
    "        pad_w = max(0, 32 - w)\n",
    "        \n",
    "        # 좌우 및 상하 패딩을 반반씩 나눠서 적용\n",
    "        padding = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n",
    "        tensor = F.pad(tensor, padding, mode='constant', value=0)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    def mapping_input(self, tensor):\n",
    "        mapping = {\n",
    "            1: random.randint(1, 10),\n",
    "            2: random.randint(11, 20),\n",
    "            3: random.randint(21, 30),\n",
    "            4: random.randint(31, 40),\n",
    "            5: random.randint(41, 50),\n",
    "            6: random.randint(51, 60),\n",
    "            7: random.randint(61, 70),\n",
    "            8: random.randint(71, 80),\n",
    "            9: random.randint(81, 90),\n",
    "            10: random.randint(91, 100)\n",
    "        }\n",
    "        temp_tensor = tensor.clone()\n",
    "        for k in mapping:\n",
    "            temp_tensor[temp_tensor == k] = -k  # 임시로 기존 값에 음수를 취해 중복을 피함\n",
    "\n",
    "        # 최종 매핑 적용\n",
    "        for k, v in mapping.items():\n",
    "            temp_tensor[temp_tensor == -k] = v\n",
    "        return temp_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task = self.df.iloc[idx]\n",
    "        \n",
    "        # task_input과 task_output 변환 및 패딩 추가\n",
    "        task_input = self.pad_to_32x32((torch.tensor(task['input'], dtype=torch.float32) + 1)) # [1, 32, 32]\n",
    "        task_output = self.pad_to_32x32((torch.tensor(task['output'], dtype=torch.float32) + 1)) # [1, 32, 32]\n",
    "        task_input = one_hot_encoding(task_input)\n",
    "        \n",
    "        # 입력 채널 차원 추가\n",
    "        if task_input.dim() == 2:\n",
    "            task_input = task_input.unsqueeze(0)  # [1, H, W]\n",
    "        if task_output.dim() == 2:\n",
    "            task_output = task_output.unsqueeze(0)  # [1, H, W]\n",
    "        \n",
    "        # 예제 입력과 출력 변환 및 패딩 추가\n",
    "        example_input = []\n",
    "        example_output = []\n",
    "        for ex_in, ex_out in zip(task['ex_input'], task['ex_output']):\n",
    "            ex_in_tensor = self.pad_to_32x32(torch.tensor(ex_in, dtype=torch.float32) + 1)\n",
    "            ex_out_tensor = self.pad_to_32x32(torch.tensor(ex_out, dtype=torch.float32) + 1)\n",
    "            ex_in_tensor = one_hot_encoding(ex_in_tensor)\n",
    "            ex_out_tensor = one_hot_encoding(ex_out_tensor)\n",
    "            \n",
    "            # 입력 채널 차원 추가\n",
    "            if ex_in_tensor.dim() == 2:\n",
    "                ex_in_tensor = ex_in_tensor.unsqueeze(0)  # [1, H, W]\n",
    "            if ex_out_tensor.dim() == 2:\n",
    "                ex_out_tensor = ex_out_tensor.unsqueeze(0)  # [1, H, W]\n",
    "            \n",
    "            example_input.append(ex_in_tensor)\n",
    "            example_output.append(ex_out_tensor)\n",
    "        \n",
    "        # 예제 입력과 출력을 채널 차원으로 결합\n",
    "        ex_inputs = torch.cat(example_input, dim=0)  # [n_examples * channels, H, W]\n",
    "        ex_outputs = torch.cat(example_output, dim=0)  # [n_examples * channels, H, W]\n",
    "        \n",
    "        return task_input, task_output, ex_inputs, ex_outputs\n",
    "\n",
    "# 사용 예제\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "ti, to, ei, eo = next(iter(train_loader))\n",
    "print(ti.shape, to.shape, ei.shape, eo.shape)\n",
    "print(ti.dtype, to.dtype, ei.dtype, eo.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import wandb\n",
    "\n",
    "def visualize_predictions(inputs, targets, predictions, condition=False):\n",
    "    if condition:\n",
    "        # 입력 이미지와 예측 결과를 CPU로 이동\n",
    "        inputs = inputs.cpu().numpy()\n",
    "        targets = targets.cpu().numpy()\n",
    "        predictions = predictions.detach().cpu().numpy()  # detach()로 그래디언트 추적 중단\n",
    "\n",
    "        # 시각화할 샘플의 수 (최대 3개의 샘플)\n",
    "        num_images = min(3, inputs.shape[0])\n",
    "\n",
    "        # 컬러 맵 정의 (0~10 값을 위한 11개의 색상)\n",
    "        color_list = ['black', 'blue', 'red', 'green', 'yellow', 'purple', \n",
    "                      'orange', 'pink', 'gray', 'brown', 'cyan']\n",
    "        cmap = ListedColormap(color_list)\n",
    "\n",
    "        # 시각화를 위한 플롯 생성 (각 줄에 입력, 타겟, 예측 이미지를 표시)\n",
    "        fig, axes = plt.subplots(num_images, 3, figsize=(12, 4 * num_images))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            # 입력 이미지 처리\n",
    "            input_image = inputs.argmax(axis=1)[i]  # shape: (H, W)\n",
    "            if input_image.ndim == 3 and input_image.shape[0] == 1:\n",
    "                input_image = input_image.squeeze(0)  # 단일 채널 이미지 (H, W)\n",
    "\n",
    "            # 타겟 이미지 처리\n",
    "            target_image = targets[i].squeeze(0)  # shape: (H, W)\n",
    "            target_image = target_image.astype(int)\n",
    "\n",
    "            # 예측 이미지 처리\n",
    "            prediction_image = predictions.argmax(axis=1)[i]  # shape: (H, W)\n",
    "            prediction_image = prediction_image.astype(int)\n",
    "\n",
    "            # 입력 이미지 표시\n",
    "            axes[i, 0].imshow(input_image, cmap=cmap, vmin=0, vmax=10)\n",
    "            axes[i, 0].set_title(f'Input Image {i+1}')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            # 타겟 이미지 표시\n",
    "            axes[i, 1].imshow(target_image, cmap=cmap, vmin=0, vmax=10)\n",
    "            axes[i, 1].set_title(f'Ground Truth {i+1}')\n",
    "            axes[i, 1].axis('off')\n",
    "\n",
    "            # 예측 이미지 표시\n",
    "            axes[i, 2].imshow(prediction_image, cmap=cmap, vmin=0, vmax=10)\n",
    "            axes[i, 2].set_title(f'Prediction {i+1}')\n",
    "            axes[i, 2].axis('off')\n",
    "\n",
    "        plt.tight_layout()  # 레이아웃을 자동으로 조정\n",
    "        \n",
    "        # WandB에 이미지 업로드\n",
    "        wandb.log({\"Predictions\": wandb.Image(plt)})\n",
    "        \n",
    "        # 범례를 플롯 내에 표시\n",
    "        patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_list[j], markersize=10, label=str(j)) for j in range(11)]\n",
    "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 정확도 계산 함수 (픽셀 단위와 이미지 단위)\n",
    "def calculate_accuracy(predictions, targets, ignore_index=0):\n",
    "    # 예측된 클래스 선택 (argmax 사용) -> 정수값으로 변환\n",
    "    pred_classes = predictions.argmax(dim=1).long()  # [batch_size, H, W]\n",
    "    \n",
    "    # 타겟 차원 맞추기 (타겟은 [batch_size, 1, H, W] 형태일 수 있으므로 squeeze)\n",
    "    targets = targets.squeeze(1)  # [batch_size, H, W]\n",
    "    \n",
    "    # 무시할 인덱스가 있는 경우 (ignore_index) 해당 픽셀을 계산에서 제외\n",
    "    if ignore_index is not None:\n",
    "        mask = (targets != ignore_index)\n",
    "    else:\n",
    "        mask = torch.ones_like(targets, dtype=torch.bool)\n",
    "    \n",
    "    # 픽셀 단위 정확도 계산\n",
    "    correct_pixels = (pred_classes == targets) & mask  # [batch_size, H, W]\n",
    "    correct_pixel_count = correct_pixels.sum().item()  # 맞은 픽셀 수\n",
    "    total_pixel_count = mask.sum().item()  # 유효한 전체 픽셀 수\n",
    "    \n",
    "    # 이미지 단위 정확도 계산 (모든 픽셀이 맞아야 해당 이미지를 정확히 예측한 것으로 간주)\n",
    "    correct_images = correct_pixels.view(targets.size(0), -1).all(dim=1)  # 각 이미지별로 모든 픽셀이 맞는지 확인\n",
    "    correct_image_count = correct_images.sum().item()  # 맞은 이미지 수\n",
    "    total_image_count = targets.size(0)  # 전체 이미지 수\n",
    "    # print(pred_classes, '\\n', targets)\n",
    "    return correct_image_count, total_image_count, correct_pixel_count, total_pixel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 예시 테스트 코드\n",
    "# batch_size = 4\n",
    "# height = 2\n",
    "# width = 2\n",
    "# num_classes = 4  # 11개의 클래스를 가정\n",
    "\n",
    "# # 임의의 예측값 생성 (logits 형태로, [batch_size, num_classes, H, W]의 크기)\n",
    "# predictions = torch.randn(batch_size, num_classes, height, width)\n",
    "\n",
    "# # 임의의 타겟값 생성 (정수값으로, [batch_size, 1, H, W]의 크기)\n",
    "# targets = torch.randint(0, num_classes, (batch_size, 1, height, width))\n",
    "\n",
    "# # 첫 번째와 두 번째 샘플의 예측값을 타겟값과 완전히 동일하게 설정\n",
    "# for i in range(2):  # 첫 번째와 두 번째 샘플을 타겟과 맞춤\n",
    "#     predictions[i] = torch.zeros_like(predictions[i])  # logits을 0으로 초기화\n",
    "#     for c in range(num_classes):\n",
    "#         predictions[i, c] = (targets[i].squeeze(0) == c).float() * 1000.0  # 타겟과 동일하게 맞춤\n",
    "\n",
    "# # 정확도 계산 함수 호출\n",
    "# correct_samples, total_samples, correct_pixels, total_pixels = calculate_accuracy(predictions, targets, ignore_index=None)\n",
    "\n",
    "# # 결과 출력\n",
    "# print(f\"이미지 단위 정확도: {correct_samples}/{total_samples}\")\n",
    "# print(f\"픽셀 단위 정확도: {correct_pixels}/{total_pixels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 배치별로 처리되는 정확도 계산 함수 (앞서 작성한 함수)\n",
    "# def calculate_accuracy(predictions, targets, ignore_index=0):\n",
    "#     pred_classes = predictions.argmax(dim=1).long()  # [batch_size, H, W]\n",
    "#     targets = targets.squeeze(1)  # [batch_size, H, W]\n",
    "    \n",
    "#     if ignore_index is not None:\n",
    "#         mask = (targets != ignore_index)\n",
    "#     else:\n",
    "#         mask = torch.ones_like(targets, dtype=torch.bool)\n",
    "    \n",
    "#     correct_pixels = (pred_classes == targets) & mask  # [batch_size, H, W]\n",
    "#     correct_pixel_count = correct_pixels.sum().item()  # 맞은 픽셀 수\n",
    "#     total_pixel_count = mask.sum().item()  # 유효한 전체 픽셀 수\n",
    "    \n",
    "#     correct_images = correct_pixels.view(targets.size(0), -1).all(dim=1)  # 각 이미지별로 모든 픽셀이 맞는지 확인\n",
    "#     correct_image_count = correct_images.sum().item()  # 맞은 이미지 수\n",
    "#     total_image_count = targets.size(0)  # 전체 이미지 수\n",
    "    \n",
    "#     return correct_image_count, total_image_count, correct_pixel_count, total_pixel_count\n",
    "\n",
    "# # 누적된 정확도를 계산하고 테스트하는 함수\n",
    "# def test_batch_accuracy():\n",
    "#     batch_size = 4\n",
    "#     height = 32\n",
    "#     width = 32\n",
    "#     num_classes = 11\n",
    "#     num_batches = 3  # 3개의 배치 처리 가정\n",
    "\n",
    "#     total_correct_samples = 0\n",
    "#     total_samples = 0\n",
    "#     total_correct_pixels = 0\n",
    "#     total_pixels = 0\n",
    "\n",
    "#     for batch_idx in range(num_batches):\n",
    "#         # 예측값과 타겟값 생성 (여기서는 간단하게 무작위로 생성)\n",
    "#         predictions = torch.randn(batch_size, num_classes, height, width)\n",
    "#         targets = torch.randint(0, num_classes, (batch_size, 1, height, width))\n",
    "\n",
    "#         # 첫 번째 배치는 완전히 정확하게 설정\n",
    "#         if batch_idx == 0:\n",
    "#             for i in range(batch_size):\n",
    "#                 predictions[i] = torch.zeros_like(predictions[i])\n",
    "#                 for c in range(num_classes):\n",
    "#                     predictions[i, c] = (targets[i].squeeze(0) == c).float() * 1000.0\n",
    "\n",
    "#         # 두 번째 배치는 절반만 정확하게 설정\n",
    "#         elif batch_idx == 1:\n",
    "#             for i in range(batch_size // 2):  # 첫 번째 절반만 정확하게\n",
    "#                 predictions[i] = torch.zeros_like(predictions[i])\n",
    "#                 for c in range(num_classes):\n",
    "#                     predictions[i, c] = (targets[i].squeeze(0) == c).float() * 1000.0\n",
    "        \n",
    "#         # 세 번째 배치는 전부 틀리게 설정\n",
    "#         else:\n",
    "#             predictions = torch.zeros_like(predictions)\n",
    "#             targets = torch.ones(batch_size, 1, height, width).long() * (num_classes - 1)\n",
    "\n",
    "#         # 배치별 정확도 계산\n",
    "#         correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(predictions, targets, ignore_index=None)\n",
    "        \n",
    "#         # 누적 정확도 계산\n",
    "#         total_correct_samples += correct_samples\n",
    "#         total_samples += batch_total_samples\n",
    "#         total_correct_pixels += correct_pixels\n",
    "#         total_pixels += batch_total_pixels\n",
    "\n",
    "#     # 전체 배치에서 누적된 정확도 계산\n",
    "#     avg_sample_accuracy = 100. * float(total_correct_samples) / float(total_samples)\n",
    "#     avg_pixel_accuracy = 100. * float(total_correct_pixels) / float(total_pixels)\n",
    "\n",
    "#     print(f\"전체 이미지 단위 정확도: {avg_sample_accuracy:.2f}%\")\n",
    "#     print(f\"전체 픽셀 단위 정확도: {avg_pixel_accuracy:.2f}%\")\n",
    "\n",
    "# # 테스트 함수 실행\n",
    "# test_batch_accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpook0612\u001b[0m (\u001b[33mlimbw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Workspace\\ARC\\wandb\\run-20240920_203604-mzizq46j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/limbw/ARC-prize-2024/runs/mzizq46j' target=\"_blank\">dry-puddle-1</a></strong> to <a href='https://wandb.ai/limbw/ARC-prize-2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/limbw/ARC-prize-2024' target=\"_blank\">https://wandb.ai/limbw/ARC-prize-2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/limbw/ARC-prize-2024/runs/mzizq46j' target=\"_blank\">https://wandb.ai/limbw/ARC-prize-2024/runs/mzizq46j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/126 [00:00<?, ?it/s]c:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Complete | Average Loss: 2.123436 | Training Sample Accuracy: 0.0000% | Training Pixel Accuracy: 38.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Average Loss: 1.600591 | Test Sample Accuracy: 0.0000% | Test Pixel Accuracy: 51.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000] Complete | Average Loss: 1.389881 | Training Sample Accuracy: 0.0000% | Training Pixel Accuracy: 49.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Average Loss: 1.448743 | Test Sample Accuracy: 0.0000% | Test Pixel Accuracy: 47.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000] Complete | Average Loss: 1.266895 | Training Sample Accuracy: 0.0000% | Training Pixel Accuracy: 52.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  83%|████████▎ | 130/156 [00:42<00:08,  3.10it/s, loss=1.4669]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_challenge = '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "train_args = {\n",
    "    'challenges': train_challenge,\n",
    "    'solution': train_solution,\n",
    "    'num_classes': 11,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 1000,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n",
    "# wandb 설정\n",
    "wandb.init(project=\"ARC-prize-2024\", config={\n",
    "    \"epochs\": train_args[\"epochs\"],\n",
    "    \"batch_size\": train_args[\"batch_size\"],\n",
    "    \"learning_rate\": train_args[\"learning_rate\"],\n",
    "})\n",
    "\n",
    "model_args = {\n",
    "    'dims': (32,64,128, 256),\n",
    "    'heads': 4,\n",
    "    'ff_expansion': 4,\n",
    "    'reduction_ratio': (4,2,2,2),\n",
    "    'num_layers': (2,3,4,5),\n",
    "    'channels': 11,\n",
    "    'num_classes': 11,\n",
    "    'kernel_stride_paddings': ((3, 1, 1),(3, 2, 1),(3, 2, 1),(3, 2, 1))\n",
    "    }\n",
    "\n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    weight = torch.ones(train_args['num_classes']).to(y.device)\n",
    "    weight[0] = 0.04\n",
    "    weight[1] = 0.8\n",
    "    ce = F.cross_entropy(y_pred, y, weight=weight) \n",
    "    return ce\n",
    "\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_args['batch_size'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(eval_challenge, eval_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=train_args['batch_size'], shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "model = ARC_Net(**model_args).to(device)\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.AdamW(model.parameters(), lr=train_args['learning_rate'])\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct_samples = 0\n",
    "    total_samples = 0\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    last_task_inputs = None\n",
    "    last_task_outputs = None\n",
    "    last_output = None\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{train_args[\"epochs\"]}', leave=False)\n",
    "\n",
    "    for batch_idx, (task_inputs, task_outputs, ex_inputs, ex_outputs) in progress_bar:\n",
    "        task_inputs = task_inputs.to(device, non_blocking=True)\n",
    "        task_outputs = task_outputs.to(device, non_blocking=True)\n",
    "        ex_inputs = ex_inputs.to(device, non_blocking=True)\n",
    "        ex_outputs = ex_outputs.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(task_inputs, ex_inputs, ex_outputs)\n",
    "\n",
    "        # 손실 함수 계산\n",
    "        loss = criterion(output, task_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(output, task_outputs, ignore_index=None)\n",
    "        total_correct_samples += correct_samples\n",
    "        total_samples += batch_total_samples\n",
    "        total_correct_pixels += correct_pixels\n",
    "        total_pixels += batch_total_pixels\n",
    "\n",
    "        # 마지막 배치의 데이터 저장 (clone 제거)\n",
    "        last_task_inputs = task_inputs.detach()\n",
    "        last_task_outputs = task_outputs.detach()\n",
    "        last_output = output.detach()\n",
    "        \n",
    "        # 프로그레스 바 업데이트\n",
    "        progress_bar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    \n",
    "    # 에포크가 끝난 후\n",
    "    avg_sample_accuracy = 100. * float(total_correct_samples) / float(total_samples)\n",
    "    avg_pixel_accuracy = 100. * float(total_correct_pixels) / float(total_pixels)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # wandb에 기록\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"train_sample_accuracy\": avg_sample_accuracy,\n",
    "        \"train_pixel_accuracy\": avg_pixel_accuracy,\n",
    "    })\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{train_args[\"epochs\"]}] Complete | Average Loss: {avg_loss:.6f} | Training Sample Accuracy: {avg_sample_accuracy:.4f}% | Training Pixel Accuracy: {avg_pixel_accuracy:.2f}%')\n",
    "\n",
    "    # 에포크가 10의 배수일 때만 시각화 수행\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        visualize_predictions(last_task_inputs, last_task_outputs, last_output, condition=True)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct_samples = 0\n",
    "    total_samples = 0\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    last_task_inputs = None\n",
    "    last_task_outputs = None\n",
    "    last_output = None\n",
    "\n",
    "    progress_bar = tqdm(enumerate(eval_loader), total=len(eval_loader), desc='Testing', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (task_inputs, task_outputs, ex_inputs, ex_outputs) in progress_bar:\n",
    "            task_inputs = task_inputs.to(device, non_blocking=True)\n",
    "            task_outputs = task_outputs.to(device, non_blocking=True)\n",
    "            ex_inputs = ex_inputs.to(device, non_blocking=True)\n",
    "            ex_outputs = ex_outputs.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(task_inputs, ex_inputs, ex_outputs)\n",
    "\n",
    "            # 손실 함수 계산\n",
    "            loss = criterion(output, task_outputs)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(output, task_outputs, ignore_index=None)\n",
    "            total_correct_samples += correct_samples\n",
    "            total_samples += batch_total_samples\n",
    "            total_correct_pixels += correct_pixels\n",
    "            total_pixels += batch_total_pixels\n",
    "\n",
    "            # 마지막 배치의 데이터 저장 (clone 제거)\n",
    "            last_task_inputs = task_inputs.detach()\n",
    "            last_task_outputs = task_outputs.detach()\n",
    "            last_output = output.detach()\n",
    "            \n",
    "            # 프로그레스 바 업데이트\n",
    "            progress_bar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    \n",
    "    avg_sample_accuracy = 100. * float(total_correct_samples) / float(total_samples)\n",
    "    avg_pixel_accuracy = 100. * float(total_correct_pixels) / float(total_pixels)\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "\n",
    "    # wandb에 기록\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"test_loss\": avg_loss,\n",
    "        \"test_sample_accuracy\": avg_sample_accuracy,\n",
    "        \"test_pixel_accuracy\": avg_pixel_accuracy,\n",
    "    })\n",
    "\n",
    "    print(f'Test Average Loss: {avg_loss:.6f} | Test Sample Accuracy: {avg_sample_accuracy:.4f}% | Test Pixel Accuracy: {avg_pixel_accuracy:.2f}%')\n",
    "\n",
    "    # 에포크가 10의 배수일 때만 시각화 수행\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        visualize_predictions(last_task_inputs, last_task_outputs, last_output, condition=True)\n",
    "\n",
    "# 학습 실행\n",
    "for epoch in range(train_args['epochs']):  \n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: torch.Size([1, 12, 32, 32])\n",
      "Output Data Shape: torch.Size([1, 12, 30, 30])\n",
      "Output for Image 1:\n",
      "tensor([[[ 0.7622, -0.4987,  0.2735,  ..., -0.3110, -0.2715, -0.2489],\n",
      "         [-0.6131, -0.4821, -0.3485,  ..., -0.8630, -0.6175,  0.3085],\n",
      "         [-0.0589,  0.3684,  0.5319,  ..., -0.4180, -0.3927,  0.1955],\n",
      "         ...,\n",
      "         [-0.8675, -0.6286, -0.2479,  ..., -0.3956,  0.4399, -0.3250],\n",
      "         [-1.1127, -0.5818, -0.0204,  ...,  0.5543, -1.0572,  0.2366],\n",
      "         [ 0.4062,  0.6461,  0.1130,  ..., -0.7741,  0.0230, -0.1579]],\n",
      "\n",
      "        [[-1.3723,  0.2421, -0.5187,  ..., -0.1431,  0.6820,  0.0888],\n",
      "         [-1.0427,  1.2488, -0.2798,  ..., -0.4199,  0.1656, -1.4125],\n",
      "         [ 0.1055, -0.8361, -0.3579,  ...,  0.1795,  0.6087, -0.6232],\n",
      "         ...,\n",
      "         [ 0.3909, -0.2279, -0.2979,  ...,  0.1907,  0.0381,  0.9474],\n",
      "         [ 1.1889, -0.1191,  0.2853,  ..., -0.5867,  0.4127,  1.2147],\n",
      "         [ 0.8997,  0.2243,  0.9249,  ..., -0.1260,  0.6925, -0.5946]],\n",
      "\n",
      "        [[-0.5866,  0.4487, -0.5265,  ..., -0.4435,  0.4344, -0.3041],\n",
      "         [-1.1439,  1.1880, -0.4138,  ...,  0.3977,  0.5673, -1.7536],\n",
      "         [ 0.1330,  0.0548,  0.0037,  ...,  0.1615,  0.1795, -0.5142],\n",
      "         ...,\n",
      "         [ 1.0562,  0.7689, -0.1802,  ..., -0.1780, -0.2101,  0.8032],\n",
      "         [ 2.1391, -0.1255, -0.0345,  ..., -0.3030, -0.0170,  0.7587],\n",
      "         [-0.0211, -0.9594,  0.3252,  ..., -0.2996,  1.0038,  0.4906]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Output for Image 2:\n",
      "tensor([[[-0.1721,  1.1140,  0.6245,  ...,  0.5332,  0.2999,  0.1949],\n",
      "         [-0.1975,  0.9601,  0.7106,  ...,  1.1311,  1.2031, -0.0037],\n",
      "         [-0.8171,  0.4793,  0.4726,  ...,  0.7302, -0.0249, -0.1485],\n",
      "         ...,\n",
      "         [-0.1620, -0.7946,  0.6500,  ..., -0.2643, -0.3993, -0.3828],\n",
      "         [-0.3845,  0.5169,  0.2090,  ...,  0.0800,  0.2635,  0.7309],\n",
      "         [-0.2547,  1.0700,  0.2788,  ...,  0.6842,  0.0657, -0.2977]],\n",
      "\n",
      "        [[ 0.0828,  0.9562,  0.1622,  ..., -0.4034,  0.1245,  0.8068],\n",
      "         [ 0.4830,  1.4452, -0.1278,  ...,  0.3115,  0.8801,  0.3758],\n",
      "         [ 0.0037,  0.5850,  0.2081,  ...,  0.5944,  0.2394,  0.4053],\n",
      "         ...,\n",
      "         [ 0.2737, -1.2331, -0.5468,  ..., -0.2633, -0.8187, -0.9750],\n",
      "         [ 0.0068, -0.2295, -0.2089,  ...,  0.8409,  0.3652,  1.1682],\n",
      "         [-0.1452,  0.3225,  0.4844,  ...,  0.6538, -0.3629, -0.8493]],\n",
      "\n",
      "        [[-0.0648,  1.2767,  0.5570,  ...,  0.4087, -0.3193,  0.0527],\n",
      "         [ 0.7466, -0.0473, -0.7265,  ...,  0.5190, -0.0374,  0.0357],\n",
      "         [ 0.0572,  1.0565, -0.2658,  ...,  1.5109, -0.1443,  0.6653],\n",
      "         ...,\n",
      "         [ 0.3142, -0.5227, -0.1594,  ..., -0.8084,  0.1427,  0.2014],\n",
      "         [ 0.2027, -0.0078,  0.0928,  ...,  0.4687, -0.6146,  0.3940],\n",
      "         [ 1.1462,  0.5857, -0.5401,  ...,  0.4433,  0.4578,  1.1238]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Output for Image 3:\n",
      "tensor([[[-6.4616e-01, -6.2804e-01,  4.1336e-01,  ...,  1.3300e-01,\n",
      "           1.3412e-01, -4.0130e-01],\n",
      "         [ 5.8336e-01, -6.5215e-01,  1.2102e+00,  ...,  3.4768e-01,\n",
      "          -2.5959e-01,  3.7166e-01],\n",
      "         [ 4.1918e-01,  6.7104e-01,  1.3373e-01,  ..., -8.3874e-01,\n",
      "           7.6898e-02, -2.7187e-03],\n",
      "         ...,\n",
      "         [ 3.6974e-01, -3.9292e-02,  4.6042e-01,  ..., -1.2330e-01,\n",
      "           7.3031e-01, -3.1776e-01],\n",
      "         [ 2.2030e-01,  2.8574e-01, -4.9345e-01,  ..., -2.8033e-01,\n",
      "           1.7041e+00,  9.3244e-01],\n",
      "         [-8.0189e-01,  3.7670e-01,  9.7302e-01,  ...,  3.8702e-02,\n",
      "          -4.1722e-02,  9.5050e-01]],\n",
      "\n",
      "        [[ 1.1847e+00,  3.1370e-01, -4.4375e-01,  ...,  8.4149e-01,\n",
      "          -4.8191e-01,  3.7138e-01],\n",
      "         [-1.0640e-01,  6.9155e-01, -8.4680e-01,  ...,  2.2363e-02,\n",
      "           8.6660e-02, -1.6342e-01],\n",
      "         [ 2.0745e-01, -1.8158e-01,  8.3570e-01,  ...,  7.0244e-01,\n",
      "          -1.1243e+00, -2.1601e-01],\n",
      "         ...,\n",
      "         [ 1.3435e-01,  1.2383e-01,  6.2519e-01,  ...,  2.1634e-01,\n",
      "          -7.2315e-01,  2.4070e-02],\n",
      "         [-3.2518e-01,  2.5690e-01,  8.0897e-01,  ...,  3.3623e-01,\n",
      "          -6.1308e-01,  7.2541e-01],\n",
      "         [ 1.4894e-01,  5.2947e-02, -7.4006e-02,  ...,  5.3049e-01,\n",
      "           4.0558e-01, -7.1708e-01]],\n",
      "\n",
      "        [[ 4.4439e-01, -1.0728e+00,  1.0593e+00,  ...,  5.9333e-01,\n",
      "          -1.9004e-01,  9.2386e-02],\n",
      "         [ 6.2022e-01,  8.2923e-02,  1.3813e-01,  ...,  7.3512e-01,\n",
      "          -5.5507e-01,  5.9381e-01],\n",
      "         [-8.1324e-01,  2.4326e-02, -5.6575e-01,  ..., -6.6914e-01,\n",
      "           8.9257e-02, -1.1219e+00],\n",
      "         ...,\n",
      "         [-5.1423e-01, -1.7153e-01, -2.7558e-01,  ..., -2.9266e-01,\n",
      "          -3.6087e-01, -2.7066e-01],\n",
      "         [-6.2318e-01, -1.1387e-01,  6.9478e-01,  ...,  2.7731e-01,\n",
      "          -7.7932e-01, -7.2679e-01],\n",
      "         [ 1.6638e-03, -5.0823e-02,  2.0880e-01,  ...,  8.1131e-01,\n",
      "          -5.4233e-01,  2.5480e-01]]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Does Image 1 output influence Image 2 output? No\n",
      "Does Image 1 output influence Image 3 output? No\n",
      "Does Image 2 output influence Image 3 output? No\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Group Convolution 정의 (입력 채널 수와 그룹 수를 맞춤)\n",
    "# # 9개의 채널을 가지고 있고, 각 이미지당 3개의 채널이므로 groups=3으로 설정\n",
    "# conv = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, groups=4)\n",
    "\n",
    "# # 3개의 이미지를 cat한 입력 데이터 생성 (3, 3, 32, 32) -> (9, 32, 32)\n",
    "# image1 = torch.randn(4, 32, 32)  # 이미지 1\n",
    "# image2 = torch.randn(4, 32, 32)  # 이미지 2\n",
    "# image3 = torch.randn(4, 32, 32)  # 이미지 3\n",
    "\n",
    "# # 이미지들을 cat하여 (9, 32, 32)로 만듭니다.\n",
    "# input_data = torch.cat((image1, image2, image3), dim=0).unsqueeze(0)  # [1, 9, 32, 32]\n",
    "\n",
    "# # 컨볼루션 수행\n",
    "# output = conv(input_data)\n",
    "\n",
    "# # 각 이미지의 출력 결과 확인\n",
    "# print(\"Input Data Shape:\", input_data.shape)\n",
    "# print(\"Output Data Shape:\", output.shape)\n",
    "\n",
    "# # 각 이미지의 출력 확인\n",
    "# for i in range(3):\n",
    "#     output_image = output[0, i * 3:(i + 1) * 3, :, :]  # 각 이미지에 대한 출력\n",
    "#     print(f\"Output for Image {i + 1}:\\n{output_image}\\n\")\n",
    "\n",
    "# # 각 이미지의 출력이 서로 영향을 주지 않는지 확인\n",
    "# influence_check_1_2 = (output[0, :3, :, :] == output[0, 3:6, :, :]).all().item()\n",
    "# influence_check_1_3 = (output[0, :3, :, :] == output[0, 6:9, :, :]).all().item()\n",
    "# influence_check_2_3 = (output[0, 3:6, :, :] == output[0, 6:9, :, :]).all().item()\n",
    "\n",
    "# print(f\"Does Image 1 output influence Image 2 output? {'Yes' if influence_check_1_2 else 'No'}\")\n",
    "# print(f\"Does Image 1 output influence Image 3 output? {'Yes' if influence_check_1_3 else 'No'}\")\n",
    "# print(f\"Does Image 2 output influence Image 3 output? {'Yes' if influence_check_2_3 else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
