{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#000000','#1E93FF','#F93C31','#4FCC30','#FFDC00',\n",
    "          '#999999','#E53AA3','#FF851B','#87D8F1','#921231','#555555']\n",
    "colormap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, edgecolors=colors[-1], linewidth=0.5, cmap=colormap, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 예시:\n",
    "# predicted와 example_output이 [1, 1, 30, 30] 크기의 텐서라고 가정\n",
    "#show_grid_side_by_side(task_input, task_output, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=1, padding=1)\n",
    "        self.norm = nn.LayerNorm(embed_dim)  # BatchNorm2d 대신 LayerNorm 사용\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)  # (B, E, P)\n",
    "        x = x.transpose(1, 2)  # (B, P, E)\n",
    "        x = self.norm(x)  # LayerNorm을 여기에서 사용\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layernorm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm1 = self.layernorm1(x)\n",
    "        attn_output, _ = self.attention(x_norm1, x_norm1, x_norm1)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        x_mlp_output = self.mlp(x)\n",
    "        x = x + x_mlp_output\n",
    "        x = self.layernorm3(x)\n",
    "        return x\n",
    "\n",
    "class TransformerHead(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, num_classes, output_size=30):\n",
    "        super(TransformerHead, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Reshape [batch, 36, 512] -> [batch, 6, 6, 512] -> [batch, 512, 6, 6]\n",
    "        self.conv1 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, stride=1,)\n",
    "        self.conv2 = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, 36, 512]\n",
    "        batch_size = x.size(0)\n",
    "        num_patches = int(x.size(1) ** 0.5)  # Assuming x.size(1) = 36, num_patches = 6\n",
    "\n",
    "        # Reshape to [batch, embed_dim, height, width]\n",
    "        x = x.view(batch_size, num_patches, num_patches, self.embed_dim)  # [batch, 6, 6, 512]\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, 512, 30, 30]\n",
    "\n",
    "        # Apply convolutions to aggregate spatial information\n",
    "        # x = self.conv1(x)  # [batch, 512, 30, 30]\n",
    "        # x = nn.functional.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "        x = self.conv2(x)  # [batch, 11, 30, 30]\n",
    "\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=30, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, num_layers=12, output_channels=11, output_size=30, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len=self.patch_embed.num_patches)\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "        self.transformer_encoders = nn.Sequential(\n",
    "            *[TransformerEncoder(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = TransformerHead(embed_dim, self.patch_embed.num_patches, output_channels, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        # print(\"Patch embedding output shape:\", x.shape)\n",
    "        # print(\"Patch embedding output sample:\", x[0, 0, :5].detach().cpu().numpy())  # Sample of first patch embedding\n",
    "\n",
    "        x = self.pos_encoding(x)\n",
    "        # print(\"After positional encoding shape:\", x.shape)\n",
    "        # print(\"After positional encoding sample:\", x[0, 0, :5].detach().cpu().numpy())  # Sample of first patch after positional encoding\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.transformer_encoders(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # print(\"After transformer encoder shape:\", x.shape)\n",
    "        \n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 출력\n",
    "model_args =  {\n",
    "    \"img_size\": 30,\n",
    "    \"patch_size\": 3, # default: 16\n",
    "    \"in_channels\": 1,\n",
    "    \"embed_dim\": 256, # default: 768\n",
    "    \"num_heads\": 8, # default: 12\n",
    "    \"num_layers\": 8, # default: 12\n",
    "    \"mlp_dim\": 2048, # default: 3072\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_channels\": 11,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionTransformer(**model_args).to(device)\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPs: 7.578G\n",
      "파라미터 수: 8.426M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "outer_model = VisionTransformer(**model_args).to(device)\n",
    "\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "# FLOPs 및 파라미터 수 계산\n",
    "try:\n",
    "    flops, params = profile(outer_model, inputs=(x,))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "    print(f\"파라미터 수: {params}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during profiling: {e}\")\n",
    "    print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bw_net_maml import BWNet_MAML\n",
    "\n",
    "# model = BWNet_MAML(embed_size=1).to(device)\n",
    "\n",
    "# # 입력 텐서 생성\n",
    "# x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "# # FLOPs 및 파라미터 수 계산\n",
    "# try:\n",
    "#     flops, params = profile(model, inputs=(x,))\n",
    "#     flops, params = clever_format([flops, params], \"%.3f\")\n",
    "#     print(f\"FLOPs: {flops}\")\n",
    "#     print(f\"파라미터 수: {params}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during profiling: {e}\")\n",
    "#     print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0000e-04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weight = torch.ones(11).to('cuda')\n",
    "weight[0] = 0.0005  # 0은 무시\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [09:31<00:00, 13.61s/it]\n",
      "Validation:  98%|█████████▊| 41/42 [09:49<00:14, 14.20s/it]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x280 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 42/42 [09:58<00:00, 14.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 2.2678132561536937, Accuracy: 56.81818181818182%\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [09:14<00:00, 13.20s/it]\n",
      "Validation:  98%|█████████▊| 41/42 [09:42<00:14, 14.21s/it]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x280 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 42/42 [09:51<00:00, 14.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, Loss: 2.2264115455058904, Accuracy: 57.89631564088086%\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [09:14<00:00, 13.21s/it]\n",
      "Validation:  71%|███████▏  | 30/42 [07:06<02:51, 14.25s/it]"
     ]
    }
   ],
   "source": [
    "from bw_net_maml import BWNet_MAML\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_sw import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "\n",
    "colors = ['#000000','#1E93FF','#F93C31','#4FCC30','#FFDC00',\n",
    "          '#999999','#E53AA3','#FF851B','#87D8F1','#921231','#555555']\n",
    "colormap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 500,\n",
    "    'task_numbers': 10, #equal to the number of tasks\n",
    "    'task_data_num': 1,\n",
    "    'example_data_num': 20, #equal to inner model batch size\n",
    "    'inner_lr': 0.01,\n",
    "    'outer_lr': 0.001,\n",
    "    'embed_size': 1,\n",
    "}\n",
    "\n",
    "\n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    weight = torch.ones(model_args['output_channels']).to(y.device)\n",
    "    weight[0] = 0.005\n",
    "    ce = F.cross_entropy(y_pred, y, weight=weight)\n",
    "    return ce\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device  \n",
    "print(f'Using {device} device')\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=kwargs['task_numbers'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=kwargs['task_numbers'], shuffle=False)\n",
    "\n",
    "# Outer Model 정의\n",
    "outer_model =  VisionTransformer(**model_args).to(device)\n",
    "outer_optimizer = optim.AdamW(outer_model.parameters(), lr=kwargs['outer_lr'])\n",
    "\n",
    "# Inner Loop 업데이트 함수\n",
    "def inner_loop_update(model, example_input, example_output, inner_optimizer, criterion, steps):\n",
    "    for _ in range(steps):\n",
    "        model.train()\n",
    "        prediction = model(example_input)\n",
    "        loss = criterion(prediction, example_output)\n",
    "\n",
    "        inner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        inner_optimizer.step()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(kwargs['epochs']):\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}')\n",
    "    total_loss = 0\n",
    "    outer_model.train()\n",
    "    \n",
    "    for data in tqdm(train_loader, desc='Training'):\n",
    "        input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "\n",
    "        task_losses = []\n",
    "        for task_number in range(input_tensor.shape[0]):\n",
    "            inner_model = deepcopy(outer_model)\n",
    "            inner_optimizer = optim.AdamW(inner_model.parameters(), lr=kwargs['inner_lr'])\n",
    "\n",
    "            inner_loop_update(inner_model, example_input[task_number], example_output[task_number],\n",
    "                              inner_optimizer, criterion, kwargs['example_data_num'])\n",
    "            \n",
    "            inner_model.eval()\n",
    "            task_prediction = inner_model(input_tensor[task_number])\n",
    "            task_loss = criterion(task_prediction, output_tensor[task_number])\n",
    "            task_losses.append(task_loss)\n",
    "        \n",
    "        meta_loss = torch.stack(task_losses).mean()\n",
    "        outer_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        outer_optimizer.step()\n",
    "\n",
    "        del meta_loss, task_losses\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Validation Loop\n",
    "    outer_model.eval()\n",
    "    validation_correct = 0\n",
    "    validation_total_samples = 0\n",
    "    total_loss = []\n",
    "\n",
    "    for batch_idx, data in enumerate(tqdm(eval_loader, desc='Validation')):\n",
    "        input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "\n",
    "        for task_number in range(input_tensor.shape[0]):\n",
    "            inner_model = deepcopy(outer_model)\n",
    "            inner_optimizer = optim.AdamW(inner_model.parameters(), lr=kwargs['inner_lr'])\n",
    "\n",
    "            inner_loop_update(inner_model, example_input[task_number], example_output[task_number],\n",
    "                            inner_optimizer, criterion, kwargs['example_data_num'])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inner_model.eval()\n",
    "                task_input = input_tensor[task_number]\n",
    "                task_output = output_tensor[task_number]\n",
    "                task_prediction = inner_model(task_input)\n",
    "                task_loss = criterion(task_prediction, task_output)\n",
    "                total_loss.append(task_loss.item())  # task_loss.item()을 리스트에 추가\n",
    "\n",
    "                prediction_class = torch.argmax(task_prediction, dim=1, keepdim=True)\n",
    "\n",
    "                mask = task_output != 0\n",
    "                correct_predictions = (prediction_class == task_output) & mask\n",
    "                validation_correct += correct_predictions.sum().item()\n",
    "                validation_total_samples += mask.sum().item()\n",
    "\n",
    "                if batch_idx == len(eval_loader) - 1 and task_number == input_tensor.shape[0] - 1:\n",
    "                    show_grid_side_by_side(task_input.cpu(), task_output.cpu(), prediction_class.cpu())\n",
    "\n",
    "            del inner_model, inner_optimizer, task_input, task_output, task_prediction, mask, correct_predictions\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 손실 값들의 평균 계산\n",
    "    mean_loss = sum(total_loss) / len(total_loss) if total_loss else 0\n",
    "    accuracy = 100 * validation_correct / validation_total_samples if validation_total_samples > 0 else 0\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}, Loss: {mean_loss}, Accuracy: {accuracy}%')\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
