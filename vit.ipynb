{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#000000','#1E93FF','#F93C31','#4FCC30','#FFDC00',\n",
    "          '#999999','#E53AA3','#FF851B','#87D8F1','#921231','#555555']\n",
    "colormap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, edgecolors=colors[-1], linewidth=0.5, cmap=colormap, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 예시:\n",
    "# predicted와 example_output이 [1, 1, 30, 30] 크기의 텐서라고 가정\n",
    "#show_grid_side_by_side(task_input, task_output, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=1)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)  # (B, E, P)\n",
    "        x = x.transpose(1, 2)  # (B, P, E)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layernorm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm1 = self.layernorm1(x)\n",
    "        attn_output, _ = self.attention(x_norm1, x_norm1, x_norm1)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        x_mlp_output = self.mlp(x)\n",
    "        x = x + x_mlp_output\n",
    "        x = self.layernorm3(x)\n",
    "        return x\n",
    "\n",
    "class ConvHead(nn.Module):\n",
    "    def __init__(self, embed_dim=768, output_channels=11, output_size=30):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(embed_dim, output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, P, E = x.shape\n",
    "        H = W = int(P ** 0.5)  # Assumes square grid of patches\n",
    "        x = x.transpose(1, 2).reshape(B, E, H, W)  # (B, E, H, W)\n",
    "        x = self.conv(x)  # (B, output_channels, H, W)\n",
    "        return nn.functional.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, num_layers=12, output_channels=11, output_size=30, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        #self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "        self.transformer_encoders = nn.Sequential(\n",
    "            *[TransformerEncoder(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = ConvHead(embed_dim, output_channels, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        #x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.transformer_encoders(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 1:]  # Remove cls_token before passing to the head\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "# # 모델 생성 및 출력\n",
    "# model_args =  {\n",
    "#     \"img_size\": 30,\n",
    "#     \"patch_size\": 3,\n",
    "#     \"in_channels\": 1,\n",
    "#     \"embed_dim\": 256,\n",
    "#     \"num_heads\": 8,\n",
    "#     \"num_layers\": 8,\n",
    "#     \"mlp_dim\": 2048,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"output_channels\": 11,\n",
    "# }\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tensor = torch.randn(1, 1, 30, 30)\n",
    "# model = VisionTransformer(**model_args)\n",
    "# x = model(tensor)\n",
    "# print(x.shape)  # Expected output: torch.Size([1, 11, 30, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [08:04<00:00, 11.54s/it]\n",
      "Validation:  29%|██▊       | 12/42 [00:13<00:35,  1.18s/it]"
     ]
    }
   ],
   "source": [
    "from bw_net_maml import BWNet_MAML\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "\n",
    "colors = ['#000000','#1E93FF','#F93C31','#4FCC30','#FFDC00',\n",
    "          '#999999','#E53AA3','#FF851B','#87D8F1','#921231','#555555']\n",
    "colormap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, edgecolors=colors[-1], linewidth=0.5, cmap=colormap, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    ce = F.cross_entropy(y_pred, y, ignore_index=0)\n",
    "    return ce\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 50,\n",
    "    'task_numbers': 10, #equal to the number of tasks\n",
    "    'task_data_num': 1,\n",
    "    'example_data_num': 20, #equal to inner model batch size\n",
    "    'inner_lr': 0.01,\n",
    "    'outer_lr': 0.001,\n",
    "    'embed_size': 1,\n",
    "}\n",
    "model_args =  {\n",
    "    \"img_size\": 30,\n",
    "    \"patch_size\": 3,\n",
    "    \"in_channels\": 1,\n",
    "    \"embed_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 8,\n",
    "    \"mlp_dim\": 2048,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_channels\": 11,\n",
    "}\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device  \n",
    "print(f'Using {device} device')\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=kwargs['task_numbers'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=kwargs['task_numbers'], shuffle=False)\n",
    "\n",
    "# Outer Model 정의\n",
    "outer_model = VisionTransformer(**model_args).to(device)\n",
    "outer_optimizer = optim.AdamW(outer_model.parameters(), lr=kwargs['outer_lr'])\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(kwargs['epochs']):\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}')\n",
    "    task_loss = 0\n",
    "    total_samples = 0\n",
    "    outer_model.train()\n",
    "    \n",
    "    for data in tqdm(train_loader, desc='Training'):\n",
    "        input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "        total_samples += input_tensor.shape[0]\n",
    "        total_loss = 0\n",
    "\n",
    "        for task_number in range(input_tensor.shape[0]):\n",
    "            # Inner loop\n",
    "            inner_model = deepcopy(outer_model)\n",
    "            inner_optimizer = optim.AdamW(inner_model.parameters(), lr=kwargs['inner_lr'])\n",
    "\n",
    "            ex_input = example_input[task_number]\n",
    "            ex_output = example_output[task_number]\n",
    "            task_input = input_tensor[task_number]\n",
    "            task_output = output_tensor[task_number]\n",
    "\n",
    "            for _ in range(kwargs['example_data_num']):  # 여러 번의 Inner Update 수행\n",
    "                inner_model.train()\n",
    "                prediction = inner_model(ex_input)\n",
    "                loss = criterion(prediction, ex_output)\n",
    "\n",
    "                inner_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                inner_optimizer.step()\n",
    "            \n",
    "            # Outer loop: outer_model 업데이트를 위한 손실 계산\n",
    "            inner_model.eval()\n",
    "            task_prediction = inner_model(task_input)\n",
    "            task_loss = criterion(task_prediction, task_output)\n",
    "            total_loss += task_loss\n",
    "\n",
    "            # 메모리 해제\n",
    "            del inner_model, inner_optimizer, ex_input, ex_output, task_input, task_output\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Outer loop에서 모델 업데이트\n",
    "        outer_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        outer_optimizer.step()\n",
    "\n",
    "        # 메모리 해제\n",
    "        del total_loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "   # Validation Loop\n",
    "    outer_model.eval()\n",
    "    validation_correct = 0\n",
    "    validation_total_samples = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(eval_loader, desc='Validation')):\n",
    "            input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "            validation_total_samples += output_tensor.size(0)  # 전체 샘플 수를 누적\n",
    "\n",
    "            for task_number in range(input_tensor.shape[0]):\n",
    "                ex_input = example_input[task_number]\n",
    "                ex_output = example_output[task_number]\n",
    "                task_input = input_tensor[task_number]\n",
    "                task_output = output_tensor[task_number]\n",
    "\n",
    "                eval_prediction = outer_model(ex_input)\n",
    "                loss = criterion(eval_prediction, ex_output)\n",
    "                \n",
    "                task_prediction = outer_model(task_input)\n",
    "                task_loss = criterion(task_prediction, task_output)\n",
    "                total_loss += task_loss\n",
    "\n",
    "                prediction_class = torch.argmax(task_prediction, dim=1, keepdim=True)\n",
    "                validation_correct += (prediction_class == task_output).sum().item()\n",
    "\n",
    "                # 시각화: 마지막 배치의 마지막 task에서만 실행\n",
    "                if batch_idx == len(eval_loader) - 1 and task_number == input_tensor.shape[0] - 1:\n",
    "                    show_grid_side_by_side(task_input.cpu(), task_output.cpu(), prediction_class.cpu())\n",
    "\n",
    "                # 메모리 해제\n",
    "                del ex_input, ex_output, task_input, task_output, eval_prediction, task_prediction\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = 100 * validation_correct / validation_total_samples  # 정확도 계산\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}, Loss: {total_loss.item()}, Accuracy: {accuracy}%')\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
