{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#000000','#1E93FF','#F93C31','#4FCC30','#FFDC00',\n",
    "          '#999999','#E53AA3','#FF851B','#87D8F1','#921231','#555555']\n",
    "colormap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, edgecolors=colors[-1], linewidth=0.5, cmap=colormap, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 예시:\n",
    "# predicted와 example_output이 [1, 1, 30, 30] 크기의 텐서라고 가정\n",
    "#show_grid_side_by_side(task_input, task_output, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=1)\n",
    "        self.norm = nn.BatchNorm2d(embed_dim)  # LayerNorm 대신 BatchNorm2d 사용\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.flatten(2)  # (B, E, P)\n",
    "        x = x.transpose(1, 2)  # (B, P, E)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layernorm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm1 = self.layernorm1(x)\n",
    "        attn_output, _ = self.attention(x_norm1, x_norm1, x_norm1)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        x_mlp_output = self.mlp(x)\n",
    "        x = x + x_mlp_output\n",
    "        x = self.layernorm3(x)\n",
    "        return x\n",
    "\n",
    "class ConvHead(nn.Module):\n",
    "    def __init__(self, embed_dim=768, output_channels=11, output_size=30):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(embed_dim, output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, P, E = x.shape\n",
    "        H = W = int(P ** 0.5)  # Assumes square grid of patches\n",
    "        x = x.transpose(1, 2).reshape(B, E, H, W)  # (B, E, H, W)\n",
    "        x = self.conv(x)  # (B, output_channels, H, W)\n",
    "        return nn.functional.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, num_layers=12, output_channels=11, output_size=30, mlp_dim=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        #self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "        self.transformer_encoders = nn.Sequential(\n",
    "            *[TransformerEncoder(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = ConvHead(embed_dim, output_channels, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        #x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.transformer_encoders(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 1:]  # Remove cls_token before passing to the head\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPs: 6.628G\n",
      "파라미터 수: 8.448M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "# 모델 생성 및 출력\n",
    "model_args =  {\n",
    "    \"img_size\": 30,\n",
    "    \"patch_size\": 3,\n",
    "    \"in_channels\": 1,\n",
    "    \"embed_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 8,\n",
    "    \"mlp_dim\": 2048,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_channels\": 11,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionTransformer(**model_args).to(device)\n",
    "\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "# FLOPs 및 파라미터 수 계산\n",
    "try:\n",
    "    flops, params = profile(model, inputs=(x,))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "    print(f\"파라미터 수: {params}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during profiling: {e}\")\n",
    "    print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPs: 170.407M\n",
      "파라미터 수: 141.277M\n"
     ]
    }
   ],
   "source": [
    "from bw_net_maml import BWNet_MAML\n",
    "\n",
    "model = BWNet_MAML(embed_size=1).to(device)\n",
    "\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "# FLOPs 및 파라미터 수 계산\n",
    "try:\n",
    "    flops, params = profile(model, inputs=(x,))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "    print(f\"파라미터 수: {params}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during profiling: {e}\")\n",
    "    print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [08:06<00:00, 11.57s/it]\n",
      "Validation:  98%|█████████▊| 41/42 [00:48<00:01,  1.26s/it]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x280 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 42/42 [00:49<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1130.820556640625, Accuracy: 0.3052653867871259%\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "\n",
    "colors = ['#000000','#1E93FF','#F93C31','#4FCC30','#FFDC00',\n",
    "          '#999999','#E53AA3','#FF851B','#87D8F1','#921231','#555555']\n",
    "colormap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, edgecolors=colors[-1], linewidth=0.5, cmap=colormap, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    ce = F.cross_entropy(y_pred, y, ignore_index=0)\n",
    "    return ce\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 500,\n",
    "    'task_numbers': 10, #equal to the number of tasks\n",
    "    'task_data_num': 1,\n",
    "    'example_data_num': 20, #equal to inner model batch size\n",
    "    'inner_lr': 0.01,\n",
    "    'outer_lr': 0.001,\n",
    "    'embed_size': 1,\n",
    "}\n",
    "model_args =  {\n",
    "    \"img_size\": 30,\n",
    "    \"patch_size\": 3,\n",
    "    \"in_channels\": 1,\n",
    "    \"embed_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 8,\n",
    "    \"mlp_dim\": 2048,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_channels\": 11,\n",
    "}\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device  \n",
    "print(f'Using {device} device')\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=kwargs['task_numbers'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=kwargs['task_numbers'], shuffle=False)\n",
    "\n",
    "# Outer Model 정의\n",
    "outer_model = VisionTransformer(**model_args).to(device)\n",
    "outer_optimizer = optim.AdamW(outer_model.parameters(), lr=kwargs['outer_lr'])\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(kwargs['epochs']):\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}')\n",
    "    total_loss = 0\n",
    "    outer_model.train()\n",
    "    \n",
    "    for data in tqdm(train_loader, desc='Training'):\n",
    "        input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "\n",
    "        task_losses = []\n",
    "        for task_number in range(input_tensor.shape[0]):\n",
    "            # Inner loop\n",
    "            inner_model = deepcopy(outer_model)\n",
    "            inner_optimizer = optim.AdamW(inner_model.parameters(), lr=kwargs['inner_lr'])\n",
    "\n",
    "            for _ in range(kwargs['example_data_num']):  # 여러 번의 Inner Update 수행\n",
    "                inner_model.train()\n",
    "                prediction = inner_model(example_input[task_number])\n",
    "                loss = criterion(prediction, example_output[task_number])\n",
    "\n",
    "                inner_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                inner_optimizer.step()\n",
    "            \n",
    "            # Inner loop가 끝난 후, outer_model의 손실 계산을 위해 inner_model의 파라미터를 사용\n",
    "            inner_model.eval()\n",
    "            task_prediction = inner_model(input_tensor[task_number])\n",
    "            task_loss = criterion(task_prediction, output_tensor[task_number])\n",
    "            task_losses.append(task_loss)\n",
    "        \n",
    "        # Outer loop에서 모든 task의 손실을 누적하여 업데이트\n",
    "        meta_loss = torch.stack(task_losses).mean()  # 모든 task의 손실 평균\n",
    "        outer_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        outer_optimizer.step()\n",
    "\n",
    "        # 메모리 해제\n",
    "        del meta_loss, task_losses\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Validation Loop\n",
    "    outer_model.eval()\n",
    "    validation_correct = 0\n",
    "    validation_total_samples = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(eval_loader, desc='Validation')):\n",
    "            input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "\n",
    "            for task_number in range(input_tensor.shape[0]):\n",
    "                ex_input = example_input[task_number]\n",
    "                ex_output = example_output[task_number]\n",
    "                task_input = input_tensor[task_number]\n",
    "                task_output = output_tensor[task_number]\n",
    "\n",
    "                eval_prediction = outer_model(ex_input)\n",
    "                loss = criterion(eval_prediction, ex_output)\n",
    "                \n",
    "                task_prediction = outer_model(task_input)\n",
    "                task_loss = criterion(task_prediction, task_output)\n",
    "                total_loss += task_loss\n",
    "\n",
    "                prediction_class = torch.argmax(task_prediction, dim=1, keepdim=True)\n",
    "\n",
    "                # 정답이 0이 아닌 부분만 선택하여 정확도 계산\n",
    "                mask = task_output != 0\n",
    "                correct_predictions = (prediction_class == task_output) & mask\n",
    "                validation_correct += correct_predictions.sum().item()\n",
    "                validation_total_samples += mask.sum().item()  # 0이 아닌 부분만 계산\n",
    "\n",
    "                # 시각화: 마지막 배치의 마지막 task에서만 실행\n",
    "                if batch_idx == len(eval_loader) - 1 and task_number == input_tensor.shape[0] - 1:\n",
    "                    show_grid_side_by_side(task_input.cpu(), task_output.cpu(), prediction_class.cpu())\n",
    "\n",
    "                # 메모리 해제\n",
    "                del ex_input, ex_output, task_input, task_output, eval_prediction, task_prediction, mask, correct_predictions\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = 100 * validation_correct / validation_total_samples if validation_total_samples > 0 else 0  # 정확도 계산\n",
    "    print(f'Epoch {epoch+1}/{kwargs[\"epochs\"]}, Loss: {total_loss.item()}, Accuracy: {accuracy}%')\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
