{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "class ARC_Dataset(Dataset):\n",
    "    def __init__(self, challenges, solution, task_data_num=1, example_data_num=10):\n",
    "        challenges = load_json(challenges)\n",
    "        solution = load_json(solution)\n",
    "        self.data = []\n",
    "        self.task_data_num = task_data_num\n",
    "        self.example_data_num = example_data_num\n",
    "        \n",
    "        for key, value in challenges.items():\n",
    "            for i in range(len(value['test'])):\n",
    "                task_input = value['test'][i]['input']\n",
    "                task_output = solution[key][i]\n",
    "                example_input = [ex['input'] for ex in value['train']]\n",
    "                example_output = [ex['output'] for ex in value['train']]\n",
    "                \n",
    "                # 데이터프레임으로 변환될 데이터를 리스트에 저장\n",
    "                self.data.append({\n",
    "                    'id': key,\n",
    "                    'input': task_input,\n",
    "                    'output': task_output,\n",
    "                    'ex_input': example_input,\n",
    "                    'ex_output': example_output\n",
    "                })\n",
    "\n",
    "        # 리스트를 데이터프레임으로 변환\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def pad_to_30x30(self, tensor):\n",
    "        if tensor.dim() == 2:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        c, h, w = tensor.shape\n",
    "        pad_h = max(0, 30 - h)\n",
    "        pad_w = max(0, 30 - w)\n",
    "        \n",
    "        # 좌우 및 상하 패딩을 반반씩 나눠서 적용\n",
    "        padding = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n",
    "        tensor = F.pad(tensor, padding, mode='constant', value=0)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    def mapping_input(self, tensor):\n",
    "        mapping = {\n",
    "            1: random.randint(1, 10),\n",
    "            2: random.randint(11, 20),\n",
    "            3: random.randint(21, 30),\n",
    "            4: random.randint(31, 40),\n",
    "            5: random.randint(41, 50),\n",
    "            6: random.randint(51, 60),\n",
    "            7: random.randint(61, 70),\n",
    "            8: random.randint(71, 80),\n",
    "            9: random.randint(81, 90),\n",
    "            10: random.randint(91, 100)\n",
    "        }\n",
    "        temp_tensor = tensor.clone()\n",
    "        for k in mapping:\n",
    "            temp_tensor[temp_tensor == k] = -k  # 임시로 기존 값에 음수를 취해 중복을 피함\n",
    "\n",
    "        # 최종 매핑 적용\n",
    "        for k, v in mapping.items():\n",
    "            temp_tensor[temp_tensor == -k] = v\n",
    "        return temp_tensor\n",
    "    \n",
    "    def augment_example_output(self, tensor):\n",
    "        # 출력 데이터 증강 (아직 구현 필요)\n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        '''\n",
    "        1. 데이터의 인덱스(idx)를 받아서 해당 인덱스(idx)의 데이터를 불러온다.\n",
    "        2. 데이터를 텐서형으로 변환하며, 클래스 번호에 +1을 해준다. (제로 패딩을 위해)\n",
    "        3. 패딩을 추가한다. (30x30 zero padding)\n",
    "        4. 샘플에 증강을 수행한다.\n",
    "            4-1. task_input은 증강된 데이터가 self.task_data_num 개가 될 때까지 증강을 수행한다.\n",
    "            4-2. example_input은 증강된 데이터가 self.example_data_num 개가 될 때까지 증강을 수행한다.\n",
    "        5. 증강된 데이터를 스택으로 변환한다.\n",
    "        6. 반환한다.\n",
    "        \n",
    "        최종 출력 형태:\n",
    "        [task_number, inner_batch_size, channel, height, width]\n",
    "        '''\n",
    "        task = self.df.iloc[idx]\n",
    "        \n",
    "        # task_input과 task_output 변환 및 패딩 추가\n",
    "        task_input = [self.pad_to_30x30((torch.tensor(task['input'],dtype=torch.float32) + 1))]\n",
    "        task_output = [self.pad_to_30x30((torch.tensor(task['output'],dtype=torch.float32) + 1))]\n",
    "        \n",
    "        # 예제 입력과 출력 변환 및 패딩 추가\n",
    "        example_input = [self.pad_to_30x30(torch.tensor(ex,dtype=torch.float32) + 1) for ex in task['ex_input']]\n",
    "        example_output = [self.pad_to_30x30(torch.tensor(ex,dtype=torch.float32) + 1) for ex in task['ex_output']]\n",
    "        \n",
    "        task_size = len(task_input)\n",
    "        for i in range(self.task_data_num):\n",
    "            random_index = random.randint(0, task_size - 1)\n",
    "            task_input.append(self.mapping_input(task_input[random_index].clone()))\n",
    "            task_output.append(task_output[0].clone())\n",
    "        \n",
    "        size = len(example_input)\n",
    "        for i in range(self.example_data_num):\n",
    "            random_index = random.randint(0, size - 1)\n",
    "            example_input.append(self.mapping_input(example_input[random_index]))\n",
    "            example_output.append(example_output[random_index])\n",
    "        \n",
    "        \n",
    "        task_input = task_input[task_size:]\n",
    "        task_output = task_output[task_size:]\n",
    "        task_input = torch.stack(task_input)\n",
    "        task_output = torch.stack(task_output)\n",
    "        \n",
    "        example_input = example_input[size:]\n",
    "        example_output = example_output[size:]\n",
    "        example_input = torch.stack(example_input,)\n",
    "        example_output = torch.stack(example_output)\n",
    "        \n",
    "        # 최종 출력 형태: [task_number, inner_batch_size, channel, height, width]\n",
    "        return task_input, task_output, example_input, example_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 5,\n",
    "    'task_numbers': 10, #equal to the number of tasks\n",
    "    'task_data_num': 1,\n",
    "    'example_data_num': 20, #equal to inner model batch size\n",
    "    'inner_lr': 0.01,\n",
    "    'outer_lr': 0.001,\n",
    "    \n",
    "}\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=kwargs['task_numbers'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=kwargs['task_numbers'], shuffle=False)\n",
    "\n",
    "outer_model = Model()\n",
    "outer_optimizer= optim.AdamW(outer_model.parameters(),lr=kwargs['outer_lr'])\n",
    "'''\n",
    "작동 방식\n",
    "1. epoch 만큼 반복\n",
    "2. all task=400 을 task_num 만큼 나눔\n",
    "3. task_num 만큼 반복\n",
    "4. inner ep에 사용될 모델 복사 (inner model 생성)\n",
    "    (inner model)\n",
    "    1. inner epcohs 만큼 반복\n",
    "    2. loss 계산\n",
    "    3. backward, optimizer update\n",
    "    4. inner model의 loss 초기화\n",
    "    학습종료\n",
    "    ===============================\n",
    "    모든 task에 대해 inner model을 학습시킴\n",
    "    \n",
    "5. \n",
    "6. 모든 task가 끝나면 outer model loss를 ti,to에서 나온 inner model loss 들로 업데이트\n",
    "'''\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in train_loader:\n",
    "        total_loss = 0\n",
    "        inner_model = deepcopy(outer_model)\n",
    "        inner_optimizer = optim.AdamW(inner_model.parameters(),lr=kwargs['inner_lr'])\n",
    "        \n",
    "        for task_number in range(kwargs['task_numbers']):\n",
    "            ex_input = data[2][task_number]\n",
    "            ex_output = data[3][task_number]\n",
    "            loss = run(inner_model, ex_input, ex_output)\n",
    "            loss.backward()\n",
    "            \n",
    "            inner_optimizer.step()\n",
    "            inner_optimizer.zero_grad()\n",
    "        for task_number in range(kwargs['task_numbers']):\n",
    "            task_input = data[0][task_number]\n",
    "            task_output = data[1][task_number]\n",
    "            total_loss += run(inner_model, task_input, task_output)\n",
    "    \n",
    "        total_loss.backward()\n",
    "        outer_optimizer.step()\n",
    "        outer_optimizer.zero_grad()\n",
    "        \n",
    "    for data in eval_loader:\n",
    "        total_loss = 0\n",
    "        for task_number in range(kwargs['task_numbers']):\n",
    "            eval_model = deepcopy(outer_model)\n",
    "            ex_input = data[2][task_number]\n",
    "            ex_output = data[3][task_number]\n",
    "            loss = run(eval_model, ex_input, ex_output)\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "            inner_optimizer.zero_grad()\n",
    "            \n",
    "        \n",
    "        \n",
    "        print(total_loss*kwargs['other_lr'])\n",
    "        \n",
    "    print('Epoch:',epoch,'Loss:',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 21, 1, 30, 30]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def criterion(y_pred,y):\n",
    "    ce = F.cross_entropy(y_pred,y)\n",
    "    return ce\n",
    "\n",
    "def run(model,x,y):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred,y)\n",
    "    \n",
    "    return loss, y_pred\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "[10, 21, 1, 30, 30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
