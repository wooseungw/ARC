{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 900, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 900, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, embed_dim=128):\n",
    "        super(ConvEmbedding, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten(2)  # Flatten the spatial dimensions (H, W)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        x = self.flatten(x)  # Flatten to (batch_size, embed_dim, H*W)\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, H*W, embed_dim)\n",
    "        return x\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, seq_len=30*30, add_cls_token=True):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.embedding = ConvEmbedding(embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if add_cls_token else None\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len+1 if add_cls_token else seq_len, embed_dim))\n",
    "        self.attention = nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim * 4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.attention, num_layers=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_channels, H, W)\n",
    "        x = x.squeeze(1)  # Remove in_channels dimension\n",
    "        x = self.embedding(x)  # Convert to tokens\n",
    "        print(x.shape)\n",
    "        if self.cls_token is not None:\n",
    "            batch_size = x.size(0)\n",
    "            cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)  # Prepend cls_token\n",
    "        print(x.shape)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # Apply self-attention\n",
    "        print(x.shape)\n",
    "        cls_feature = x[:, 0, :]  # Extract cls token feature\n",
    "        token_features = x[:, 1:, :]  # Extract other token features\n",
    "        \n",
    "        return cls_feature, token_features\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.randn(10, 1, 1, 30, 30)  # Batch size of 1, single channel, 30x30 input\n",
    "model = FeatureExtractor()\n",
    "cls_feature, token_features = model(input_tensor)\n",
    "\n",
    "print(cls_feature.shape)  # Expected output: torch.Size([1, 128])\n",
    "print(token_features.shape)  # Expected output: torch.Size([1, 900, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference - Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_heads=4):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(feature_dim, num_heads)\n",
    "        self.new_cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))  # New Cls token\n",
    "    \n",
    "    def forward(self, example_input_cls, example_output_cls):\n",
    "        # example_input_cls, example_output_cls: Shape (batch_size, embed_dim)\n",
    "        \n",
    "        # Expand new_cls_token to match batch size\n",
    "        batch_size = example_input_cls.size(0)\n",
    "        query = self.new_cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Reshape to (sequence_length, batch_size, embed_dim) as expected by MultiheadAttention\n",
    "        query = query.permute(1, 0, 2)  # (1, batch_size, embed_dim)\n",
    "        key = torch.stack([example_input_cls, example_output_cls], dim=0)  # (2, batch_size, embed_dim)\n",
    "        value = torch.stack([example_input_cls, example_output_cls], dim=0)  # (2, batch_size, embed_dim)\n",
    "        \n",
    "        # Apply cross attention\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        \n",
    "        # Return the output as the new Cls token, shape: (batch_size, embed_dim)\n",
    "        return attn_output.squeeze(0)\n",
    "\n",
    "# Example usage:\n",
    "example_input_cls = torch.randn(1, 128)  # Example input Cls token\n",
    "example_output_cls = torch.randn(1, 128)  # Example output Cls token\n",
    "\n",
    "cross_attention_v1 = CrossAttention()\n",
    "output_cls_v1 = cross_attention_v1(example_input_cls, example_output_cls)\n",
    "\n",
    "print(output_cls_v1.shape)  # Expected output: torch.Size([1, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference - Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "torch.Size([10, 3, 128])\n",
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionWithThreeTokens(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_heads=4):\n",
    "        super(SelfAttentionWithThreeTokens, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(feature_dim, num_heads)\n",
    "        self.new_cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))  # New Cls token\n",
    "    \n",
    "    def forward(self, example_input_cls, example_output_cls):\n",
    "        # example_input_cls, example_output_cls: Shape (batch_size, embed_dim)\n",
    "        \n",
    "        # Expand new_cls_token to match batch size\n",
    "        batch_size = example_input_cls.size(0)\n",
    "        new_cls_token_expanded = self.new_cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Combine all Cls tokens: shape (3, batch_size, embed_dim)\n",
    "        combined_cls_tokens = torch.cat([new_cls_token_expanded, \n",
    "                                         example_input_cls.unsqueeze(1), \n",
    "                                         example_output_cls.unsqueeze(1)], dim=1)\n",
    "        print(\"c\")\n",
    "        print(combined_cls_tokens.shape)\n",
    "        # Apply self-attention\n",
    "        combined_cls_tokens = combined_cls_tokens.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
    "        attn_output, _ = self.multihead_attn(combined_cls_tokens, combined_cls_tokens, combined_cls_tokens)\n",
    "        \n",
    "        # Return the output as the new Cls token, shape: (batch_size, embed_dim)\n",
    "        return attn_output[0]  # The first token corresponds to the new_cls_token\n",
    "\n",
    "# Example usage:\n",
    "example_input_cls = torch.randn(10, 128)  # Example input Cls token\n",
    "example_output_cls = torch.randn(10, 128)  # Example output Cls token\n",
    "\n",
    "self_attention_v2 = SelfAttentionWithThreeTokens()\n",
    "output_cls_v2 = self_attention_v2(example_input_cls, example_output_cls)\n",
    "\n",
    "print(output_cls_v2.shape)  # Expected output: torch.Size([1, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causals torch.Size([10, 1, 128])\n",
      "new_cls_token_expanded torch.Size([1, 1, 128])\n",
      "attn_output torch.Size([1, 1, 128])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombineModule(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(CombineModule, self).__init__()\n",
    "        \n",
    "        # Self-Attention Layer for combining causals\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=4)\n",
    "        self.new_cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))  # New Cls token\n",
    "        # Fully connected layer to produce the final causal representation\n",
    "        self.fc = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "    def forward(self, causals):\n",
    "        # causals: shape (num_causals, batch_size, feature_dim)\n",
    "        causals = causals.unsqueeze(1)  # Add a sequence dimension\n",
    "        print(\"causals\",causals.shape)\n",
    "        new_cls_token_expanded = self.new_cls_token.expand(1, -1, -1)\n",
    "        print(\"new_cls_token_expanded\",new_cls_token_expanded.shape)\n",
    "        \n",
    "        # cls_causals = torch.cat([new_cls_token_expanded, \n",
    "        #                                  causals], dim=0)\n",
    "\n",
    "        attn_output, _ = self.self_attention(new_cls_token_expanded, causals, causals)\n",
    "        print(\"attn_output\",attn_output.shape)\n",
    "        # Mean pooling over the sequence dimension (num_causals)\n",
    "        combined_causal = attn_output.squeeze(1)  # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        # Pass through a fully connected layer to get the final causal representation\n",
    "        final_causal = self.fc(combined_causal)  # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        return final_causal\n",
    "\n",
    "# Example usage:\n",
    "feature_dim = 128\n",
    "num_causals = 10\n",
    "\n",
    "# Assume causals is the output from the Causal Inference module\n",
    "causals = torch.randn(num_causals, feature_dim)\n",
    "\n",
    "combine_module = CombineModule(feature_dim=feature_dim)\n",
    "final_causal = combine_module(causals)\n",
    "\n",
    "print(final_causal.shape)  # Expected output: torch.Size([batch_size, feature_dim])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_causal_proj torch.Size([1, 128])\n",
      "cls_combined torch.Size([1, 128])\n",
      "combined_features torch.Size([1, 256])\n",
      "x torch.Size([1, 900])\n",
      "x torch.Size([1, 1, 30, 30])\n",
      "logits torch.Size([1, 11, 30, 30])\n",
      "torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_dim=128, output_dim=1, seq_len=30*30, num_classes=11):\n",
    "        super(Head, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # FC layers to transform features\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim)  # Project final_causal to match cls_token\n",
    "        self.fc2 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc3 = nn.Linear(embed_dim*2, seq_len)  # Combine Cls and final_causal\n",
    "        self.fc4 = nn.Linear(embed_dim + (seq_len * embed_dim), seq_len)  # Combine with flattened token features\n",
    "        \n",
    "        # Convolution layer to map to class logits\n",
    "        self.conv = nn.Conv2d(1, num_classes, kernel_size=1)  # 1x1 Convolution for class logits\n",
    "        \n",
    "        # Output reshape (no upsample since the size is already 30x30)\n",
    "        self.output_reshape = nn.Sequential(\n",
    "            nn.Unflatten(1, (1, int(seq_len ** 0.5), int(seq_len ** 0.5)))  # (batch_size, 1, 30, 30)\n",
    "        )\n",
    "        \n",
    "        # LogSoftmax for multi-class classification\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, cls_token, token_features, final_causal):\n",
    "        # Project final_causal to the same dimension as cls_token\n",
    "        final_causal_proj = self.fc1(final_causal)\n",
    "        print(\"final_causal_proj\",final_causal_proj.shape)\n",
    "        # Combine cls_token and final_causal_proj\n",
    "        #cls_combined = torch.cat((cls_token, final_causal_proj), dim=-1)\n",
    "        \n",
    "        cls_combined = self.fc2(cls_token)\n",
    "        print(\"cls_combined\",cls_combined.shape)\n",
    "        # Flatten token features\n",
    "        #token_features_flat = token_features.view(token_features.size(0), -1)\n",
    "        \n",
    "        # Combine cls_combined with token_features_flat\n",
    "        #combined_features = torch.cat((cls_combined, token_features_flat), dim=-1)\n",
    "        combined_features = torch.cat((final_causal_proj, cls_combined), dim=-1)\n",
    "        print(\"combined_features\",combined_features.shape)\n",
    "        x = self.fc3(combined_features)  # (batch_size, seq_len)\n",
    "        print(\"x\",x.shape)\n",
    "        # Reshape to (batch_size, 1, 30, 30)\n",
    "        x = self.output_reshape(x)\n",
    "        print(\"x\",x.shape)\n",
    "        # Apply convolution to get class logits\n",
    "        logits = self.conv(x)  # (batch_size, num_classes, 30, 30)\n",
    "        print(\"logits\",logits.shape)\n",
    "        # Apply log_softmax to get class probabilities\n",
    "        output = self.log_softmax(logits)  # (batch_size, num_classes, 30, 30)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "cls_token = torch.randn(1, 128)  # Example Cls token\n",
    "token_features = torch.randn(1, 30*30, 128)  # Example token features\n",
    "final_causal = torch.randn(1, 128)  # Example Final Causal\n",
    "\n",
    "head = Head()\n",
    "output = head(cls_token, token_features, final_causal)\n",
    "\n",
    "# Get the class with the highest probability for each pixel\n",
    "predicted_classes = torch.argmax(output, dim=1, keepdim=True)  # (batch_size, 1, 30, 30)\n",
    "\n",
    "print(predicted_classes.shape)  # Expected output: torch.Size([1, 1, 30, 30])\n",
    "# print(predicted_classes)  # Prints the predicted class for each pixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 900, 128])\n",
      "torch.Size([1, 901, 128])\n",
      "torch.Size([1, 901, 128])\n",
      "torch.Size([10, 900, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "torch.Size([10, 900, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "c\n",
      "torch.Size([10, 3, 128])\n",
      "causals torch.Size([10, 1, 128])\n",
      "new_cls_token_expanded torch.Size([1, 1, 128])\n",
      "attn_output torch.Size([1, 1, 128])\n",
      "final_causal_proj torch.Size([1, 128])\n",
      "cls_combined torch.Size([1, 128])\n",
      "combined_features torch.Size([1, 256])\n",
      "x torch.Size([1, 900])\n",
      "x torch.Size([1, 1, 30, 30])\n",
      "logits torch.Size([1, 11, 30, 30])\n",
      "torch.Size([1, 11, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BWNet(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_examples=5):\n",
    "        super(BWNet, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(embed_dim=feature_dim)\n",
    "        self.causal_inference = SelfAttentionWithThreeTokens(feature_dim=feature_dim)\n",
    "        self.combine_module = CombineModule(feature_dim=feature_dim)\n",
    "        self.head = Head()\n",
    "\n",
    "    def forward(self, input_tensor, example_input, example_output):\n",
    "        # 입력 및 예제 텐서를 30x30으로 패딩\n",
    "        \n",
    "        # Feature extraction\n",
    "        cls_feature, _ = self.feature_extractor(input_tensor)\n",
    "        ex_input_cls_feature, _ = self.feature_extractor(example_input)\n",
    "        ex_output_cls_feature, _ = self.feature_extractor(example_output)\n",
    "        \n",
    "        # Causal inference\n",
    "        causals = self.causal_inference(ex_input_cls_feature, ex_output_cls_feature)\n",
    "        \n",
    "        # Combine module\n",
    "        final_causal = self.combine_module(causals)\n",
    "        \n",
    "        # Head\n",
    "        output = self.head(cls_feature, _, final_causal)\n",
    "        \n",
    "        # Padding 제거\n",
    "        # output = self.remove_padding(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    # def remove_padding(self, output):\n",
    "    #     # 패딩 값을 제거하고 원본 크기로 보간\n",
    "    #     mask = output != 0\n",
    "    #     output_cleaned = output[mask].view(output.size(0), 1, -1)\n",
    "    #     output_cleaned = F.interpolate(output_cleaned, size=(30, 30), mode='bilinear', align_corners=True)\n",
    "    #     return output_cleaned\n",
    "\n",
    "# 예시 사용법:\n",
    "input_tensor = torch.randn(1, 1, 30, 30)  # 입력 텐서 예시\n",
    "example_input = torch.randn(10, 1, 30, 30)  # 입력 텐서 예시\n",
    "example_output = torch.randn(10, 1, 30, 30)  # 출력 텐서 예시\n",
    "\n",
    "model = BWNet(feature_dim=128)\n",
    "output = model(input_tensor, example_input, example_output)\n",
    "\n",
    "print(output.shape)  # 최종 출력 크기를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 1, 1800])\n",
      "torch.Size([10, 11, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FEBlock(nn.Module):\n",
    "    def __init__(self, embed_size=1):\n",
    "        super(FEBlock, self).__init__()\n",
    "        # 1x1 ~ 30x30 Convolution layers 생성 (Padding X)\n",
    "        # self.convs = nn.ModuleList([nn.Conv2d(1, embed_size, kernel_size=n, padding=0) for n in range(1, 31)])\n",
    "        # self.fc = nn.ModuleList([nn.Linear((30-n)**(30-n)embed_size, 30*30*embed_size) for n in range(1, 31)])\n",
    "        self.numbers = [1, 2, 3, 5, 7, 9, 11, 13, 15, 25, 30]\n",
    "        self.stages = nn.ModuleList([])\n",
    "        for n in self.numbers:\n",
    "            self.stages.append(nn.Sequential(\n",
    "                nn.Conv2d(1, n, kernel_size=n, padding=0),\n",
    "                nn.Flatten(1),\n",
    "                nn.Linear((30-n+1)**2*n, 30*30*embed_size)\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for stage in self.stages:\n",
    "            features.append(stage(x).unsqueeze(1))  # (batch, 1, 1, n*n*embed_size)\n",
    "            print(features[-1].shape)\n",
    "        return torch.stack(features, dim=1)  # (batch, 30, n*n*embed_size)\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_size=1):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 30*30*embed_size))\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=30*30*embed_size, num_heads=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # print(cls_tokens.shape)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze(2)\n",
    "        # print(x.shape)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (batch, 31, 30*30*embed_size)\n",
    "        x, _ = self.self_attn(x, x, x)\n",
    "        return x[:, 0].unsqueeze(1)  # (batch, 1, 30*30*embed_size)\n",
    "\n",
    "class HeadBlock(nn.Module):\n",
    "    def __init__(self, embed_size=1, num_classes=11):\n",
    "        super(HeadBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(30*30*embed_size, 30*30*embed_size*embed_size)\n",
    "        self.fc2 = nn.Linear(30*30*embed_size*embed_size, 30*30*embed_size)\n",
    "        self.fc3 = nn.Linear(30*30*embed_size, 30*30)\n",
    "        self.conv = nn.Conv2d(1, num_classes, kernel_size=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, 1, 30, 30)  # Reshape to (batch, 1, 30, 30)\n",
    "        x = self.conv(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "class BWNet_MAML(nn.Module):\n",
    "    def __init__(self, embed_size=1):\n",
    "        super(BWNet_MAML, self).__init__()\n",
    "        self.fe_block = FEBlock(embed_size=embed_size)\n",
    "        self.self_attn_block = SelfAttentionBlock(embed_size=embed_size)\n",
    "        self.head_block = HeadBlock(embed_size=embed_size)\n",
    "        \n",
    "\n",
    "    def forward(self, ex_input):\n",
    "        features = self.fe_block(ex_input)  # (batch, 30, 30*30*embed_size)\n",
    "        cls_feature = self.self_attn_block(features)  # (batch, 1, 30*30*embed_size)\n",
    "        out = self.head_block(cls_feature)  # (batch, 1, 30, 30)\n",
    "        #out = self.fc_final(out)  # (batch, 11, 30, 30)\n",
    "        return out\n",
    "\n",
    "input_tensor = torch.randn(1, 1, 30, 30)  # 입력 텐서 예시\n",
    "example_input = torch.randn(10, 1, 30, 30)  # 입력 텐서 예시\n",
    "# example_output = torch.randn(10, 1, 30, 30)  # 출력 텐서 예시\n",
    "\n",
    "model = BWNet_MAML(embed_size=2)\n",
    "output = model(example_input)\n",
    "\n",
    "print(output.shape)  # 최종 출력 크기를 확인\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_titan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
