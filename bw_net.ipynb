{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 900, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "torch.Size([10, 901, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 900, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, embed_dim=128):\n",
    "        super(ConvEmbedding, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten(2)  # Flatten the spatial dimensions (H, W)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        x = self.flatten(x)  # Flatten to (batch_size, embed_dim, H*W)\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, H*W, embed_dim)\n",
    "        return x\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, seq_len=30*30, add_cls_token=True):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.embedding = ConvEmbedding(embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if add_cls_token else None\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len+1 if add_cls_token else seq_len, embed_dim))\n",
    "        self.attention = nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim * 4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.attention, num_layers=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_channels, H, W)\n",
    "        x = self.embedding(x)  # Convert to tokens\n",
    "        print(x.shape)\n",
    "        if self.cls_token is not None:\n",
    "            batch_size = x.size(0)\n",
    "            cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)  # Prepend cls_token\n",
    "        print(x.shape)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # Apply self-attention\n",
    "        print(x.shape)\n",
    "        cls_feature = x[:, 0, :]  # Extract cls token feature\n",
    "        token_features = x[:, 1:, :]  # Extract other token features\n",
    "        \n",
    "        return cls_feature, token_features\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.randn(10, 1, 30, 30)  # Batch size of 1, single channel, 30x30 input\n",
    "model = FeatureExtractor()\n",
    "cls_feature, token_features = model(input_tensor)\n",
    "\n",
    "print(cls_feature.shape)  # Expected output: torch.Size([1, 128])\n",
    "print(token_features.shape)  # Expected output: torch.Size([1, 900, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference - Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_heads=4):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(feature_dim, num_heads)\n",
    "        self.new_cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))  # New Cls token\n",
    "    \n",
    "    def forward(self, example_input_cls, example_output_cls):\n",
    "        # example_input_cls, example_output_cls: Shape (batch_size, embed_dim)\n",
    "        \n",
    "        # Expand new_cls_token to match batch size\n",
    "        batch_size = example_input_cls.size(0)\n",
    "        query = self.new_cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Reshape to (sequence_length, batch_size, embed_dim) as expected by MultiheadAttention\n",
    "        query = query.permute(1, 0, 2)  # (1, batch_size, embed_dim)\n",
    "        key = torch.stack([example_input_cls, example_output_cls], dim=0)  # (2, batch_size, embed_dim)\n",
    "        value = torch.stack([example_input_cls, example_output_cls], dim=0)  # (2, batch_size, embed_dim)\n",
    "        \n",
    "        # Apply cross attention\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        \n",
    "        # Return the output as the new Cls token, shape: (batch_size, embed_dim)\n",
    "        return attn_output.squeeze(0)\n",
    "\n",
    "# Example usage:\n",
    "example_input_cls = torch.randn(1, 128)  # Example input Cls token\n",
    "example_output_cls = torch.randn(1, 128)  # Example output Cls token\n",
    "\n",
    "cross_attention_v1 = CrossAttention()\n",
    "output_cls_v1 = cross_attention_v1(example_input_cls, example_output_cls)\n",
    "\n",
    "print(output_cls_v1.shape)  # Expected output: torch.Size([1, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference - Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "torch.Size([10, 3, 128])\n",
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionWithThreeTokens(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_heads=4):\n",
    "        super(SelfAttentionWithThreeTokens, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(feature_dim, num_heads)\n",
    "        self.new_cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))  # New Cls token\n",
    "    \n",
    "    def forward(self, example_input_cls, example_output_cls):\n",
    "        # example_input_cls, example_output_cls: Shape (batch_size, embed_dim)\n",
    "        \n",
    "        # Expand new_cls_token to match batch size\n",
    "        batch_size = example_input_cls.size(0)\n",
    "        new_cls_token_expanded = self.new_cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Combine all Cls tokens: shape (3, batch_size, embed_dim)\n",
    "        combined_cls_tokens = torch.cat([new_cls_token_expanded, \n",
    "                                         example_input_cls.unsqueeze(1), \n",
    "                                         example_output_cls.unsqueeze(1)], dim=1)\n",
    "        print(\"c\")\n",
    "        print(combined_cls_tokens.shape)\n",
    "        # Apply self-attention\n",
    "        combined_cls_tokens = combined_cls_tokens.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
    "        attn_output, _ = self.multihead_attn(combined_cls_tokens, combined_cls_tokens, combined_cls_tokens)\n",
    "        \n",
    "        # Return the output as the new Cls token, shape: (batch_size, embed_dim)\n",
    "        return attn_output[0]  # The first token corresponds to the new_cls_token\n",
    "\n",
    "# Example usage:\n",
    "example_input_cls = torch.randn(10, 128)  # Example input Cls token\n",
    "example_output_cls = torch.randn(10, 128)  # Example output Cls token\n",
    "\n",
    "self_attention_v2 = SelfAttentionWithThreeTokens()\n",
    "output_cls_v2 = self_attention_v2(example_input_cls, example_output_cls)\n",
    "\n",
    "print(output_cls_v2.shape)  # Expected output: torch.Size([1, 128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output torch.Size([10, 1, 128])\n",
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombineModule(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(CombineModule, self).__init__()\n",
    "        \n",
    "        # Self-Attention Layer for combining causals\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=4)\n",
    "        \n",
    "        # Fully connected layer to produce the final causal representation\n",
    "        self.fc = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "    def forward(self, causals):\n",
    "        # causals: shape (num_causals, batch_size, feature_dim)\n",
    "        causals = causals.unsqueeze(1)  # Add a sequence dimension\n",
    "        attn_output, _ = self.self_attention(causals, causals, causals)\n",
    "        print(\"attn_output\",attn_output.shape)\n",
    "        # Mean pooling over the sequence dimension (num_causals)\n",
    "        combined_causal = attn_output.squeeze(1)  # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        # Pass through a fully connected layer to get the final causal representation\n",
    "        final_causal = self.fc(combined_causal)  # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        return final_causal\n",
    "\n",
    "# Example usage:\n",
    "feature_dim = 128\n",
    "num_causals = 10\n",
    "\n",
    "# Assume causals is the output from the Causal Inference module\n",
    "causals = torch.randn(num_causals, feature_dim)\n",
    "\n",
    "combine_module = CombineModule(feature_dim=feature_dim)\n",
    "final_causal = combine_module(causals)\n",
    "\n",
    "print(final_causal.shape)  # Expected output: torch.Size([batch_size, feature_dim])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 30, 30])\n",
      "tensor([[[[ 5, 10,  0, 10,  5,  5, 10,  5, 10,  5, 10,  5,  5, 10, 10, 10, 10,\n",
      "            5,  5,  5, 10, 10, 10, 10, 10, 10, 10,  5,  5,  5],\n",
      "          [10, 10, 10,  5, 10, 10,  5, 10,  5,  5,  5,  5,  5,  5,  5, 10,  5,\n",
      "           10,  5,  5,  5, 10, 10,  5,  5,  5,  5, 10, 10,  5],\n",
      "          [10,  5,  5, 10,  5, 10,  5,  5, 10, 10, 10, 10, 10, 10, 10,  5,  5,\n",
      "            5, 10,  5,  5,  5, 10,  0,  5, 10, 10,  5, 10,  5],\n",
      "          [10,  5,  0, 10,  5,  5, 10, 10,  5, 10,  5, 10,  5, 10, 10, 10,  5,\n",
      "           10, 10,  5,  5, 10, 10, 10,  5, 10,  5,  5,  5,  5],\n",
      "          [ 5, 10, 10,  5, 10, 10, 10,  5, 10,  5, 10,  5, 10,  5, 10,  5,  5,\n",
      "           10, 10, 10, 10, 10, 10,  5,  5, 10,  5,  5, 10, 10],\n",
      "          [10,  5,  5, 10, 10,  5,  5,  5,  5,  5, 10,  5,  5, 10,  5, 10,  5,\n",
      "           10,  5,  5, 10,  5, 10,  0,  5,  5,  5, 10,  5,  5],\n",
      "          [10,  5, 10, 10, 10,  5,  5,  5,  5, 10, 10,  5, 10, 10,  5,  5,  5,\n",
      "            0, 10,  5,  5, 10,  5,  5, 10,  5, 10, 10, 10, 10],\n",
      "          [10, 10, 10, 10,  5,  5,  5, 10, 10,  5, 10,  5, 10,  5,  5,  5, 10,\n",
      "            5,  0, 10, 10, 10,  5,  5,  5, 10,  5,  5, 10, 10],\n",
      "          [10, 10,  5, 10,  5,  5, 10, 10,  5,  5,  5,  5, 10,  5, 10, 10, 10,\n",
      "            5, 10,  5, 10,  5,  5,  5,  5,  5,  5, 10,  5, 10],\n",
      "          [10, 10, 10, 10, 10, 10,  5,  5,  5, 10,  5, 10, 10,  5, 10, 10,  5,\n",
      "           10, 10,  5, 10,  5, 10,  5,  5, 10,  5,  5,  5,  5],\n",
      "          [ 5, 10,  5,  5, 10, 10, 10, 10,  5, 10,  5, 10,  5,  5, 10, 10, 10,\n",
      "           10, 10,  5, 10,  5, 10,  5, 10, 10, 10, 10, 10,  5],\n",
      "          [10,  5,  5, 10,  5,  5,  5,  5,  5, 10,  5,  5,  5, 10, 10, 10,  5,\n",
      "            5, 10, 10,  5,  5,  0,  5,  5, 10, 10,  5,  5, 10],\n",
      "          [10, 10, 10,  5, 10,  5, 10, 10, 10,  0,  5, 10, 10, 10, 10, 10,  5,\n",
      "           10,  5,  5, 10,  0, 10,  5, 10, 10, 10,  5, 10, 10],\n",
      "          [ 5,  5,  5,  5,  5, 10,  5,  5, 10, 10, 10,  5, 10,  5,  5, 10,  5,\n",
      "           10,  5, 10, 10,  5,  5,  5,  5,  5, 10,  5,  5,  5],\n",
      "          [ 5, 10, 10,  5, 10, 10, 10,  5,  5,  5, 10, 10,  5, 10,  5,  5,  5,\n",
      "           10,  5, 10, 10, 10, 10, 10,  5,  5, 10,  5, 10,  5],\n",
      "          [ 5, 10, 10, 10, 10, 10,  5, 10,  5,  5, 10,  5, 10,  5,  5, 10,  5,\n",
      "            0,  5,  5,  5,  5, 10,  0,  5, 10,  5,  5,  5, 10],\n",
      "          [10, 10,  5, 10, 10, 10, 10,  5,  5,  5,  5,  5,  5, 10,  5, 10,  5,\n",
      "            5, 10, 10, 10, 10, 10, 10,  5,  5, 10, 10,  5, 10],\n",
      "          [ 5, 10, 10,  5,  5, 10, 10, 10,  5, 10, 10, 10,  5,  5, 10, 10, 10,\n",
      "           10,  5, 10,  5,  5, 10,  5,  5, 10, 10, 10, 10, 10],\n",
      "          [ 5, 10, 10, 10, 10, 10,  5, 10,  5, 10, 10, 10, 10,  5,  5, 10,  5,\n",
      "            5, 10, 10, 10, 10,  0, 10, 10,  5,  5,  5,  0, 10],\n",
      "          [10, 10,  5,  5, 10, 10, 10, 10, 10, 10, 10, 10, 10,  5,  5,  5,  5,\n",
      "            5,  5,  5,  0,  5, 10, 10,  5,  5, 10, 10, 10,  5],\n",
      "          [10, 10, 10, 10, 10,  5,  5,  5,  5, 10,  5, 10,  5,  5,  5,  5,  5,\n",
      "           10, 10,  5, 10, 10,  5, 10,  5,  5,  5, 10, 10, 10],\n",
      "          [ 5, 10, 10, 10,  5,  5,  0, 10, 10,  5,  5,  5, 10,  5, 10, 10,  5,\n",
      "            5,  5, 10,  0,  5, 10,  5, 10,  5,  5,  5,  5, 10],\n",
      "          [ 5,  5, 10, 10,  5, 10, 10, 10,  5, 10,  5,  5, 10,  5,  5,  5, 10,\n",
      "           10,  5, 10,  5,  5, 10,  5,  5,  5, 10,  5, 10,  5],\n",
      "          [ 5, 10,  5,  5, 10, 10, 10, 10,  5, 10,  0,  5, 10,  5,  5,  5,  5,\n",
      "            5, 10, 10, 10,  5,  5, 10, 10, 10,  5, 10,  5, 10],\n",
      "          [ 5, 10, 10,  5,  0,  5, 10, 10, 10,  5,  5,  5, 10,  5,  5, 10,  5,\n",
      "           10,  5,  5,  5, 10, 10, 10, 10, 10,  5,  5,  5,  5],\n",
      "          [ 5, 10, 10,  5, 10, 10,  5,  5, 10,  5, 10,  5,  5, 10,  5, 10,  5,\n",
      "           10,  5, 10, 10,  5, 10, 10,  5,  5, 10,  5,  5,  5],\n",
      "          [ 5,  5, 10, 10,  5,  5, 10,  5, 10,  5,  5, 10,  5,  5,  5, 10, 10,\n",
      "            5, 10, 10,  5, 10,  5, 10,  5, 10,  5,  5,  5,  5],\n",
      "          [10,  5,  5, 10,  5,  0,  5,  5, 10, 10,  5, 10,  5,  5,  5,  5,  5,\n",
      "            5, 10, 10,  5, 10, 10,  5, 10,  5,  5, 10, 10, 10],\n",
      "          [ 5, 10,  5, 10,  5,  5, 10, 10, 10, 10, 10, 10, 10, 10,  5,  5, 10,\n",
      "            5,  5, 10,  5, 10,  5, 10, 10, 10,  0, 10,  5,  5],\n",
      "          [ 5, 10,  5,  5,  5, 10, 10, 10, 10, 10, 10,  5,  5,  5,  5,  5, 10,\n",
      "            5,  5,  5, 10,  5, 10,  5,  5,  5,  5, 10, 10, 10]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_dim=128, output_dim=1, seq_len=30*30, num_classes=11):\n",
    "        super(Head, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # FC layers to transform features\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim)  # Project final_causal to match cls_token\n",
    "        self.fc2 = nn.Linear(embed_dim * 2, embed_dim)  # Combine Cls and final_causal\n",
    "        self.fc3 = nn.Linear(embed_dim + (seq_len * embed_dim), seq_len)  # Combine with flattened token features\n",
    "        \n",
    "        # Convolution layer to map to class logits\n",
    "        self.conv = nn.Conv2d(1, num_classes, kernel_size=1)  # 1x1 Convolution for class logits\n",
    "        \n",
    "        # Output reshape (no upsample since the size is already 30x30)\n",
    "        self.output_reshape = nn.Sequential(\n",
    "            nn.Unflatten(1, (1, int(seq_len ** 0.5), int(seq_len ** 0.5)))  # (batch_size, 1, 30, 30)\n",
    "        )\n",
    "        \n",
    "        # LogSoftmax for multi-class classification\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, cls_token, token_features, final_causal):\n",
    "        # Project final_causal to the same dimension as cls_token\n",
    "        final_causal_proj = self.fc1(final_causal)\n",
    "        \n",
    "        # Combine cls_token and final_causal_proj\n",
    "        cls_combined = torch.cat((cls_token, final_causal_proj), dim=-1)\n",
    "        cls_combined = self.fc2(cls_combined)\n",
    "        \n",
    "        # Flatten token features\n",
    "        token_features_flat = token_features.view(token_features.size(0), -1)\n",
    "        \n",
    "        # Combine cls_combined with token_features_flat\n",
    "        combined_features = torch.cat((cls_combined, token_features_flat), dim=-1)\n",
    "        x = self.fc3(combined_features)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Reshape to (batch_size, 1, 30, 30)\n",
    "        x = self.output_reshape(x)\n",
    "        \n",
    "        # Apply convolution to get class logits\n",
    "        logits = self.conv(x)  # (batch_size, num_classes, 30, 30)\n",
    "        \n",
    "        # Apply log_softmax to get class probabilities\n",
    "        output = self.log_softmax(logits)  # (batch_size, num_classes, 30, 30)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "cls_token = torch.randn(1, 128)  # Example Cls token\n",
    "token_features = torch.randn(1, 30*30, 128)  # Example token features\n",
    "final_causal = torch.randn(1, 128)  # Example Final Causal\n",
    "\n",
    "head = Head()\n",
    "output = head(cls_token, token_features, final_causal)\n",
    "\n",
    "# Get the class with the highest probability for each pixel\n",
    "predicted_classes = torch.argmax(output, dim=1, keepdim=True)  # (batch_size, 1, 30, 30)\n",
    "\n",
    "print(predicted_classes.shape)  # Expected output: torch.Size([1, 1, 30, 30])\n",
    "# print(predicted_classes)  # Prints the predicted class for each pixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 900, 128])\n",
      "torch.Size([1, 901, 128])\n",
      "torch.Size([1, 901, 128])\n",
      "torch.Size([4, 900, 128])\n",
      "torch.Size([4, 901, 128])\n",
      "torch.Size([4, 901, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m example_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# Example output tensor of size (1, 1, 20, 20)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m BWNet(feature_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)  \u001b[38;5;66;03m# Set appropriate values for feature_dim and num_examples\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should output torch.Size([1, 1, 30, 30]) after processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 25\u001b[0m, in \u001b[0;36mBWNet.forward\u001b[1;34m(self, input_tensor, example_input, example_output)\u001b[0m\n\u001b[0;32m     22\u001b[0m example_cls_feature, example_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(example_input_padded)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Causal inference\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m causals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_output_padded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Combine module\u001b[39;00m\n\u001b[0;32m     28\u001b[0m final_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_module(causals)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 18\u001b[0m, in \u001b[0;36mSelfAttentionWithThreeTokens.forward\u001b[1;34m(self, example_input_cls, example_output_cls)\u001b[0m\n\u001b[0;32m     15\u001b[0m new_cls_token_expanded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_cls_token\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, embed_dim)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Combine all Cls tokens: shape (3, batch_size, embed_dim)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m combined_cls_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_cls_token_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mexample_input_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mexample_output_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_cls_tokens\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 4"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from bw_net import FeatureExtractor, CausalInference, CombineModule, Head\n",
    "\n",
    "class BWNet(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_examples=5):\n",
    "        super(BWNet, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(embed_dim=feature_dim)\n",
    "        self.causal_inference = SelfAttentionWithThreeTokens(feature_dim=feature_dim)\n",
    "        self.combine_module = CombineModule(feature_dim=feature_dim)\n",
    "        self.head = Head()\n",
    "\n",
    "    def forward(self, input_tensor, example_input, example_output):\n",
    "        # Pad inputs and example tensors to 30x30\n",
    "        input_padded = F.pad(input_tensor, (0, 30 - input_tensor.size(3), 0, 30 - input_tensor.size(2)), mode='constant', value=0)\n",
    "        example_input_padded = F.pad(example_input, (0, 30 - example_input.size(3), 0, 30 - example_input.size(2)), mode='constant', value=0)\n",
    "        example_output_padded = F.pad(example_output, (0, 30 - example_output.size(3), 0, 30 - example_output.size(2)), mode='constant', value=0)\n",
    "        \n",
    "        # Feature extraction\n",
    "        cls_feature, input_features = self.feature_extractor(input_padded)\n",
    "        example_cls_feature, example_features = self.feature_extractor(example_input_padded)\n",
    "        \n",
    "        # Causal inference\n",
    "        causals = self.causal_inference(example_features, example_output_padded)\n",
    "        \n",
    "        # Combine module\n",
    "        final_causal = self.combine_module(causals)\n",
    "        \n",
    "        # Head\n",
    "        output = self.head(cls_feature, input_features, final_causal)\n",
    "        \n",
    "        # Remove padding values for final output\n",
    "        output = self.remove_padding(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def remove_padding(self, output):\n",
    "        # Assumes padding value is 0; modify if necessary\n",
    "        mask = output != 0\n",
    "        output_cleaned = output[mask].view(output.size(0), 1, -1)\n",
    "        output_cleaned = F.interpolate(output_cleaned, size=(30, 30), mode='bilinear', align_corners=True)\n",
    "        return output_cleaned\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.randn(1, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_input = torch.randn(4, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_output = torch.randn(4, 1, 20, 20)  # Example output tensor of size (1, 1, 20, 20)\n",
    "\n",
    "model = BWNet(feature_dim=128)  # Set appropriate values for feature_dim and num_examples\n",
    "output = model(input_tensor, example_input, example_output)\n",
    "\n",
    "print(output.shape)  # Should output torch.Size([1, 1, 30, 30]) after processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_tensor = torch.randn(1, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_input = torch.randn(1, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_output = torch.randn(1, 1, 20, 20)  # Example output tensor of size (1, 1, 20, 20)\n",
    "\n",
    "input_padded = F.pad(input_tensor, (0, 30 - input_tensor.size(3), 0, 30 - input_tensor.size(2)), mode='constant', value=0)\n",
    "example_input_padded = F.pad(example_input, (0, 30 - example_input.size(3), 0, 30 - example_input.size(2)), mode='constant', value=0)\n",
    "example_output_padded = F.pad(example_output, (0, 30 - example_output.size(3), 0, 30 - example_output.size(2)), mode='constant', value=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_titan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
