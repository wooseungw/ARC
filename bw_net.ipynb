{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 900, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, embed_dim=128):\n",
    "        super(ConvEmbedding, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten(2)  # Flatten the spatial dimensions (H, W)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        x = self.flatten(x)  # Flatten to (batch_size, embed_dim, H*W)\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, H*W, embed_dim)\n",
    "        return x\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, seq_len=30*30, add_cls_token=True):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.embedding = ConvEmbedding(embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if add_cls_token else None\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len+1 if add_cls_token else seq_len, embed_dim))\n",
    "        self.attention = nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim * 4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.attention, num_layers=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_channels, H, W)\n",
    "        x = self.embedding(x)  # Convert to tokens\n",
    "        \n",
    "        if self.cls_token is not None:\n",
    "            batch_size = x.size(0)\n",
    "            cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)  # Prepend cls_token\n",
    "        \n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # Apply self-attention\n",
    "        \n",
    "        cls_feature = x[:, 0, :]  # Extract cls token feature\n",
    "        token_features = x[:, 1:, :]  # Extract other token features\n",
    "        \n",
    "        return cls_feature, token_features\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.randn(1, 1, 30, 30)  # Batch size of 1, single channel, 30x30 input\n",
    "model = FeatureExtractor()\n",
    "cls_feature, token_features = model(input_tensor)\n",
    "\n",
    "print(cls_feature.shape)  # Expected output: torch.Size([1, 128])\n",
    "print(token_features.shape)  # Expected output: torch.Size([1, 900, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference - Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_heads=4):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(feature_dim, num_heads)\n",
    "        self.new_cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))  # New Cls token\n",
    "    \n",
    "    def forward(self, example_input_cls, example_output_cls):\n",
    "        # example_input_cls, example_output_cls: Shape (batch_size, embed_dim)\n",
    "        \n",
    "        # Expand new_cls_token to match batch size\n",
    "        batch_size = example_input_cls.size(0)\n",
    "        query = self.new_cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Reshape to (sequence_length, batch_size, embed_dim) as expected by MultiheadAttention\n",
    "        query = query.permute(1, 0, 2)  # (1, batch_size, embed_dim)\n",
    "        key = torch.stack([example_input_cls, example_output_cls], dim=0)  # (2, batch_size, embed_dim)\n",
    "        value = torch.stack([example_input_cls, example_output_cls], dim=0)  # (2, batch_size, embed_dim)\n",
    "        \n",
    "        # Apply cross attention\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        \n",
    "        # Return the output as the new Cls token, shape: (batch_size, embed_dim)\n",
    "        return attn_output.squeeze(0)\n",
    "\n",
    "# Example usage:\n",
    "example_input_cls = torch.randn(1, 128)  # Example input Cls token\n",
    "example_output_cls = torch.randn(1, 128)  # Example output Cls token\n",
    "\n",
    "cross_attention_v1 = CrossAttention()\n",
    "output_cls_v1 = cross_attention_v1(example_input_cls, example_output_cls)\n",
    "\n",
    "print(output_cls_v1.shape)  # Expected output: torch.Size([1, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference - Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionWithThreeTokens(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_heads=4):\n",
    "        super(SelfAttentionWithThreeTokens, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(feature_dim, num_heads)\n",
    "        self.new_cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))  # New Cls token\n",
    "    \n",
    "    def forward(self, example_input_cls, example_output_cls):\n",
    "        # example_input_cls, example_output_cls: Shape (batch_size, embed_dim)\n",
    "        \n",
    "        # Expand new_cls_token to match batch size\n",
    "        batch_size = example_input_cls.size(0)\n",
    "        new_cls_token_expanded = self.new_cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Combine all Cls tokens: shape (3, batch_size, embed_dim)\n",
    "        combined_cls_tokens = torch.cat([new_cls_token_expanded, \n",
    "                                         example_input_cls.unsqueeze(1), \n",
    "                                         example_output_cls.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        combined_cls_tokens = combined_cls_tokens.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
    "        attn_output, _ = self.multihead_attn(combined_cls_tokens, combined_cls_tokens, combined_cls_tokens)\n",
    "        \n",
    "        # Return the output as the new Cls token, shape: (batch_size, embed_dim)\n",
    "        return attn_output[0]  # The first token corresponds to the new_cls_token\n",
    "\n",
    "# Example usage:\n",
    "example_input_cls = torch.randn(1, 128)  # Example input Cls token\n",
    "example_output_cls = torch.randn(1, 128)  # Example output Cls token\n",
    "\n",
    "self_attention_v2 = SelfAttentionWithThreeTokens()\n",
    "output_cls_v2 = self_attention_v2(example_input_cls, example_output_cls)\n",
    "\n",
    "print(output_cls_v2.shape)  # Expected output: torch.Size([1, 128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombineModule(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(CombineModule, self).__init__()\n",
    "        \n",
    "        # Self-Attention Layer for combining causals\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=4)\n",
    "        \n",
    "        # Fully connected layer to produce the final causal representation\n",
    "        self.fc = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "    def forward(self, causals):\n",
    "        # causals: shape (num_causals, batch_size, feature_dim)\n",
    "        \n",
    "        # Apply self-attention to combine the causals\n",
    "        # Permute causals to match expected input shape for MultiheadAttention\n",
    "        causals = causals.permute(1, 0, 2)  # Shape: (batch_size, num_causals, feature_dim)\n",
    "        \n",
    "        attn_output, _ = self.self_attention(causals, causals, causals)\n",
    "        \n",
    "        # Mean pooling over the sequence dimension (num_causals)\n",
    "        combined_causal = attn_output.mean(dim=1)  # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        # Pass through a fully connected layer to get the final causal representation\n",
    "        final_causal = self.fc(combined_causal)  # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        return final_causal\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 1\n",
    "feature_dim = 128\n",
    "num_causals = 5\n",
    "\n",
    "# Assume causals is the output from the Causal Inference module\n",
    "causals = torch.randn(num_causals, batch_size, feature_dim)\n",
    "\n",
    "combine_module = CombineModule(feature_dim=feature_dim)\n",
    "final_causal = combine_module(causals)\n",
    "\n",
    "print(final_causal.shape)  # Expected output: torch.Size([batch_size, feature_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_dim=128, output_dim=1, seq_len=30*30):\n",
    "        super(Head, self).__init__()\n",
    "        # FC layers to transform features\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim)  # Project final_causal to match cls_token\n",
    "        self.fc2 = nn.Linear(embed_dim * 2, embed_dim)  # Combine Cls and final_causal\n",
    "        self.fc3 = nn.Linear(embed_dim + (seq_len * embed_dim), seq_len)  # Combine with flattened token features\n",
    "        self.fc4 = nn.Linear(seq_len, output_dim * seq_len)  # Final output adjustment\n",
    "        \n",
    "        # Output reshape and upsample\n",
    "        self.output_reshape = nn.Sequential(\n",
    "            nn.Unflatten(1, (output_dim, int(seq_len ** 0.5), int(seq_len ** 0.5))),\n",
    "            nn.Upsample(size=(30, 30), mode='bilinear', align_corners=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, cls_token, token_features, final_causal):\n",
    "        # cls_token: shape (batch_size, embed_dim)\n",
    "        # token_features: shape (batch_size, seq_len, embed_dim)\n",
    "        # final_causal: shape (batch_size, embed_dim)\n",
    "        \n",
    "        # Project final_causal to the same dimension as cls_token\n",
    "        final_causal_proj = self.fc1(final_causal)  # (batch_size, embed_dim)\n",
    "        \n",
    "        # Combine cls_token and final_causal_proj\n",
    "        cls_combined = torch.cat((cls_token, final_causal_proj), dim=-1)  # (batch_size, 2 * embed_dim)\n",
    "        cls_combined = self.fc2(cls_combined)  # (batch_size, embed_dim)\n",
    "        \n",
    "        # Flatten token features to (batch_size, seq_len * embed_dim)\n",
    "        token_features_flat = token_features.view(token_features.size(0), -1)  # (batch_size, seq_len * embed_dim)\n",
    "        \n",
    "        # Combine cls_combined with token_features_flat\n",
    "        combined_features = torch.cat((cls_combined, token_features_flat), dim=-1)  # (batch_size, embed_dim + seq_len * embed_dim)\n",
    "        x = self.fc3(combined_features)  # (batch_size, seq_len)\n",
    "        x = self.fc4(x)  # (batch_size, output_dim * seq_len)\n",
    "        \n",
    "        # Reshape and upsample to get (batch_size, 1, 30, 30)\n",
    "        output = self.output_reshape(x)  # Final output shape (batch_size, 1, 30, 30)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "cls_token = torch.randn(1, 128)  # Example Cls token\n",
    "token_features = torch.randn(1, 30*30, 128)  # Example token features\n",
    "final_causal = torch.randn(1, 128)  # Example Final Causal\n",
    "\n",
    "head = Head()\n",
    "output = head(cls_token, token_features, final_causal)\n",
    "\n",
    "print(output.shape)  # Expected output: torch.Size([1, 1, 30, 30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pook0\\AppData\\Local\\Temp\\ipykernel_5716\\1600779546.py:35: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  challenge[f'{i}']['test'] = substitute\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007bbfb7_train_0</td>\n",
       "      <td>[[0, 7, 7], [7, 7, 7], [0, 7, 7]]</td>\n",
       "      <td>[[0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, ...</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(9, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007bbfb7_train_1</td>\n",
       "      <td>[[4, 0, 4], [0, 0, 0], [0, 4, 0]]</td>\n",
       "      <td>[[4, 0, 4, 0, 0, 0, 4, 0, 4], [0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(9, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007bbfb7_train_2</td>\n",
       "      <td>[[0, 0, 0], [0, 0, 2], [2, 0, 2]]</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(9, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>007bbfb7_train_3</td>\n",
       "      <td>[[6, 6, 0], [6, 0, 0], [0, 6, 6]]</td>\n",
       "      <td>[[6, 6, 0, 6, 6, 0, 0, 0, 0], [6, 0, 0, 6, 0, ...</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(9, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007bbfb7_train_4</td>\n",
       "      <td>[[2, 2, 2], [0, 0, 0], [0, 2, 2]]</td>\n",
       "      <td>[[2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(9, 9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                              input  \\\n",
       "0  007bbfb7_train_0  [[0, 7, 7], [7, 7, 7], [0, 7, 7]]   \n",
       "1  007bbfb7_train_1  [[4, 0, 4], [0, 0, 0], [0, 4, 0]]   \n",
       "2  007bbfb7_train_2  [[0, 0, 0], [0, 0, 2], [2, 0, 2]]   \n",
       "3  007bbfb7_train_3  [[6, 6, 0], [6, 0, 0], [0, 6, 6]]   \n",
       "4  007bbfb7_train_4  [[2, 2, 2], [0, 0, 0], [0, 2, 2]]   \n",
       "\n",
       "                                              output input_shape output_shape  \n",
       "0  [[0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, ...      (3, 3)       (9, 9)  \n",
       "1  [[4, 0, 4, 0, 0, 0, 4, 0, 4], [0, 0, 0, 0, 0, ...      (3, 3)       (9, 9)  \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, ...      (3, 3)       (9, 9)  \n",
       "3  [[6, 6, 0, 6, 6, 0, 0, 0, 0], [6, 0, 0, 6, 0, ...      (3, 3)       (9, 9)  \n",
       "4  [[2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, ...      (3, 3)       (9, 9)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "\n",
    "eval_challenge = './kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json'\n",
    "eval_solution = './kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json'\n",
    "\n",
    "\n",
    "def DataMaker(challenge_data, solution_data):\n",
    "    \n",
    "    # Loading the data that contains the \"challenge\"\n",
    "    challenge = pd.read_json(challenge_data)\n",
    "    \n",
    "    # Loading the data that contains the \"Solution\"\n",
    "    with open(solution_data) as json_data:\n",
    "        solution = json.load(json_data) \n",
    "        \n",
    "    # getting alll the id values present in the dataset\n",
    "    all_ids = list(challenge.columns)\n",
    "    \n",
    "    # concatinating along the test the way it is done for the train part\n",
    "    for i in all_ids:\n",
    "        \n",
    "        # Getting the value of each cell for challenge dataset\n",
    "        substitute = challenge[f'{i}']['test'][0]\n",
    "        \n",
    "        # Creating a new \"output\" key value pair\n",
    "        substitute['output'] = solution[f'{i}'][0] \n",
    "        \n",
    "        # Changing the value to \"input : []\" and \"output : []\"\n",
    "        # instead of \"input : []\"\n",
    "        challenge[f'{i}']['test'] = substitute       \n",
    "        \n",
    "        \n",
    "    return challenge\n",
    "\n",
    "def InputOutputDataset(df):\n",
    "    \n",
    "    all_ids = list(df.columns)\n",
    "    new_df = pd.DataFrame(columns= ['id','input','output','input_shape','output_shape'])\n",
    "    for i in all_ids:\n",
    "        size = len(df[i]['train'])\n",
    "        for j in range(size) :\n",
    "            ip = df[i]['train'][j]['input']\n",
    "            op = df[i]['train'][j]['output']\n",
    "            ip_shape = np.array(df[i]['train'][j]['input']).shape\n",
    "            op_shape = np.array(df[i]['train'][j]['output']).shape\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df['id'] = [f'{i}_train_{j}']\n",
    "            temp_df['input'] = [ip]\n",
    "            temp_df['output'] = [op]\n",
    "            temp_df['input_shape'] = [ip_shape]\n",
    "            temp_df['output_shape'] = [op_shape]\n",
    "\n",
    "            new_df = new_df._append(temp_df,ignore_index = True)\n",
    "    \n",
    "        ip = df[i]['test']['input']\n",
    "        op = df[i]['test']['output']\n",
    "        ip_shape = np.array(df[i]['test']['input']).shape\n",
    "        op_shape = np.array(df[i]['test']['output']).shape\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['id'] = [f'{i}_test']\n",
    "        temp_df['input'] = [ip]\n",
    "        temp_df['output'] = [op]\n",
    "        temp_df['input_shape'] = [ip_shape]\n",
    "        temp_df['output_shape'] = [op_shape]\n",
    "        new_df = new_df._append(temp_df,ignore_index = True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "new_train_data = DataMaker(train_challenge,train_solution)\n",
    "new_train_data = InputOutputDataset(new_train_data)\n",
    "\n",
    "new_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1471) must match the size of tensor b (901) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m example_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# Example output tensor of size (1, 1, 20, 20)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m BWNet(feature_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)  \u001b[38;5;66;03m# Set appropriate values for feature_dim and num_examples\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should output torch.Size([1, 1, 30, 30]) after processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mBWNet.forward\u001b[1;34m(self, input_tensor, example_input, example_output)\u001b[0m\n\u001b[0;32m     18\u001b[0m example_output_padded \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(example_output, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m-\u001b[39m example_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m-\u001b[39m example_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Feature extraction\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m cls_feature, input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_padded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m example_cls_feature, example_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(example_input_padded)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Causal inference\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m, in \u001b[0;36mFeatureExtractor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m     cls_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, embed_dim)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Prepend cls_token\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)  \u001b[38;5;66;03m# Apply self-attention\u001b[39;00m\n\u001b[0;32m     38\u001b[0m cls_feature \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# Extract cls token feature\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1471) must match the size of tensor b (901) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from bw_net import FeatureExtractor, CausalInference, CombineModule, Head\n",
    "\n",
    "class BWNet(nn.Module):\n",
    "    def __init__(self, feature_dim=128, num_examples=5):\n",
    "        super(BWNet, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(embed_dim=feature_dim)\n",
    "        self.causal_inference = SelfAttentionWithThreeTokens(feature_dim=feature_dim)\n",
    "        self.combine_module = CombineModule(feature_dim=feature_dim)\n",
    "        self.head = Head()\n",
    "\n",
    "    def forward(self, input_tensor, example_input, example_output):\n",
    "        # Pad inputs and example tensors to 30x30\n",
    "        input_padded = F.pad(input_tensor, (0, 30 - input_tensor.size(3), 0, 30 - input_tensor.size(2)), mode='constant', value=0)\n",
    "        example_input_padded = F.pad(example_input, (0, 30 - example_input.size(3), 0, 30 - example_input.size(2)), mode='constant', value=0)\n",
    "        example_output_padded = F.pad(example_output, (0, 30 - example_output.size(3), 0, 30 - example_output.size(2)), mode='constant', value=0)\n",
    "        \n",
    "        # Feature extraction\n",
    "        cls_feature, input_features = self.feature_extractor(input_padded)\n",
    "        example_cls_feature, example_features = self.feature_extractor(example_input_padded)\n",
    "        \n",
    "        # Causal inference\n",
    "        causals = self.causal_inference(example_features, example_output_padded)\n",
    "        \n",
    "        # Combine module\n",
    "        final_causal = self.combine_module(causals)\n",
    "        \n",
    "        # Head\n",
    "        output = self.head(cls_feature, input_features, final_causal)\n",
    "        \n",
    "        # Remove padding values for final output\n",
    "        output = self.remove_padding(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def remove_padding(self, output):\n",
    "        # Assumes padding value is 0; modify if necessary\n",
    "        mask = output != 0\n",
    "        output_cleaned = output[mask].view(output.size(0), 1, -1)\n",
    "        output_cleaned = F.interpolate(output_cleaned, size=(30, 30), mode='bilinear', align_corners=True)\n",
    "        return output_cleaned\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.randn(1, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_input = torch.randn(1, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_output = torch.randn(1, 1, 20, 20)  # Example output tensor of size (1, 1, 20, 20)\n",
    "\n",
    "model = BWNet(feature_dim=128)  # Set appropriate values for feature_dim and num_examples\n",
    "output = model(input_tensor, example_input, example_output)\n",
    "\n",
    "print(output.shape)  # Should output torch.Size([1, 1, 30, 30]) after processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_tensor = torch.randn(1, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_input = torch.randn(1, 1, 20, 20)  # Example input tensor of size (1, 1, 20, 20)\n",
    "example_output = torch.randn(1, 1, 20, 20)  # Example output tensor of size (1, 1, 20, 20)\n",
    "\n",
    "input_padded = F.pad(input_tensor, (0, 30 - input_tensor.size(3), 0, 30 - input_tensor.size(2)), mode='constant', value=0)\n",
    "example_input_padded = F.pad(example_input, (0, 30 - example_input.size(3), 0, 30 - example_input.size(2)), mode='constant', value=0)\n",
    "example_output_padded = F.pad(example_output, (0, 30 - example_output.size(3), 0, 30 - example_output.size(2)), mode='constant', value=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_titan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
