{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, linewidth=0.5, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 예시:\n",
    "# predicted와 example_output이 [1, 1, 30, 30] 크기의 텐서라고 가정\n",
    "#show_grid_side_by_side(task_input, task_output, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_sw import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from math import sqrt\n",
    "\n",
    "def cast_tuple(val, depth):\n",
    "    return val if isinstance(val, tuple) else (val,) * depth\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class DsConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias=bias),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads,\n",
    "        reduction_ratio\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim * 2, reduction_ratio, stride = reduction_ratio, bias = False)\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = heads), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h = heads, x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class MixFeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        expansion_factor\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DsConv2d(hidden_dim, hidden_dim, 3, padding = 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "## 컨볼루션 임베딩\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dim_in,dim_out, kernel_size, stride, padding):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Conv2d(dim_in, \n",
    "                                   dim_out, \n",
    "                                   kernel_size=kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "'''\n",
    "5. MiT (Mixer Transformer)\n",
    "이미지를 여러 스테이지로 처리합니다. 각 스테이지는 이미지를 패치로 나누고, 패치를 임베딩한 후, 여러 개의 Transformer 레이어를 적용합니다.\n",
    "이 과정은 이미지의 다양한 해상도에서 특징을 추출합니다. \n",
    "'''    \n",
    "class MiT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        channels,\n",
    "        dims,\n",
    "        heads,\n",
    "        ff_expansion,\n",
    "        reduction_ratio,\n",
    "        num_layers,\n",
    "        stage_kernel_stride_pad = ((7, 4, 3),  \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1), \n",
    "                                   (3, 2, 1))\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        dims = (channels, *dims)\n",
    "        dim_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        self.stages = nn.ModuleList([])\n",
    "\n",
    "        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(\n",
    "            dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n",
    "            #여기서 너비와 높이가 같은 정사각형 패치를 사용합니다.\n",
    "            get_overlap_patches = nn.Unfold(kernel, stride = stride, padding = padding)\n",
    "            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n",
    "\n",
    "            layers = nn.ModuleList([])\n",
    "\n",
    "            for _ in range(num_layers):\n",
    "                layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim_out, EfficientSelfAttention(dim = dim_out, heads = heads, reduction_ratio = reduction_ratio)),\n",
    "                    PreNorm(dim_out, MixFeedForward(dim = dim_out, expansion_factor = ff_expansion)),\n",
    "                ]))\n",
    "\n",
    "            self.stages.append(nn.ModuleList([\n",
    "                get_overlap_patches,\n",
    "                overlap_patch_embed,\n",
    "                layers\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        return_layer_outputs = False\n",
    "    ):\n",
    "        h, w = x.shape[-2:]\n",
    "        \n",
    "        \n",
    "        layer_outputs = []\n",
    "        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n",
    "            x = get_overlap_patches(x)\n",
    "            \n",
    "            num_patches = x.shape[-1]\n",
    "            ratio = int(sqrt((h * w) / num_patches))\n",
    "            \n",
    "            x = rearrange(x, 'b c (h w) -> b c h w', h = h // ratio)\n",
    "\n",
    "            x = overlap_embed(x)\n",
    "            for (attn, ff) in layers:\n",
    "                x = attn(x) + x\n",
    "                x = ff(x) + x\n",
    "\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        ret = x if not return_layer_outputs else layer_outputs\n",
    "        return ret\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, input_dim = 256 ,dim=128, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(input_dim , dim, kernel_size=1),  \n",
    "            nn.Conv2d(dim, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "## 디코더\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n",
    "\n",
    "'''\n",
    "6. Segformer\n",
    "MiT를 통해 추출된 여러 스케일의 특징을 결합하고, 최종적으로 세그멘테이션 맵을 생성합니다.\n",
    "각 스테이지의 출력을 디코더 차원으로 매핑하고, 업샘플링하여 동일한 해상도로 만든 후, 이를 결합합니다.\n",
    "결합된 특징 맵을 사용하여 최종 세그멘테이션 맵을 생성합니다.\n",
    "'''\n",
    "class ARC_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dims=(32, 64, 160, 256),\n",
    "        heads=(1, 2, 5, 8),\n",
    "        ff_expansion=(8, 8, 4, 4),\n",
    "        reduction_ratio=(8, 4, 2, 1),\n",
    "        num_layers=2,\n",
    "        channels=1,\n",
    "        num_classes=11,\n",
    "        kernel_stride_paddings = ((1, 1, 0),(3, 2, 1), (3, 2, 1), (3, 2, 1))\n",
    "    ):\n",
    "        super().__init__()\n",
    "        decoder_dim = dims[-1]\n",
    "        \n",
    "        dims, heads, ff_expansion, reduction_ratio, num_layers = map(\n",
    "            partial(cast_tuple, depth=len(kernel_stride_paddings)), (dims, heads, ff_expansion, reduction_ratio, num_layers))\n",
    "\n",
    "        \n",
    "        self.mit = MiT(\n",
    "            channels=channels,\n",
    "            dims=dims,\n",
    "            heads=heads,\n",
    "            ff_expansion=ff_expansion,\n",
    "            reduction_ratio=reduction_ratio,\n",
    "            num_layers=num_layers,\n",
    "            stage_kernel_stride_pad=kernel_stride_paddings\n",
    "        )\n",
    "\n",
    "        self.to_fused = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(dim, decoder_dim, 1),\n",
    "            nn.Upsample(scale_factor=2 ** i)\n",
    "        ) for i, dim in enumerate(dims)])\n",
    "\n",
    "\n",
    "        \n",
    "        self.cbam = CBAM(gate_channels = decoder_dim * len(kernel_stride_paddings) * 2)  \n",
    "        \n",
    "        self.reduce = nn.Conv2d(decoder_dim * len(dims) * 2, decoder_dim * len(dims), 1)        \n",
    "        \n",
    "        self.to_segmentation = Head(decoder_dim*len(dims) * 3, decoder_dim*len(dims) , num_classes)\n",
    "        \n",
    "    def _fusion(self, x):\n",
    "        x = self.mit(x, return_layer_outputs=True)\n",
    "        fused = []\n",
    "        for output, to_fused in zip(x, self.to_fused):\n",
    "            x = to_fused(output)  # Conv2d 적용\n",
    "            # 업샘플링하여 공간 크기를 맞춥니다.\n",
    "            fused.append(x)\n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        return fused    \n",
    "    \n",
    "    def forward(self, x, ex_inputs, ex_outputs):\n",
    "        x = self._fusion(x) # [b,1,H,W] -> [b,dim_0,H,W],...,[b,dim_n, H//(n+1), W//(n+1)] \n",
    "                            # -> [b,dim_n, H, W],...,[b,dim_n, H, W] -> [b, dim_n * 2, H, W]\n",
    "                            # [b, 256, 32, 32]\n",
    "                            \n",
    "        # 예제 입력과 출력을 채널 차원으로 분할\n",
    "        n_examples = ex_inputs.size(1)  # n_examples * channels (여기서는 3)\n",
    "        \n",
    "        # 예제 수만큼 반복하면서 예제 입력과 출력을 처리\n",
    "        fused = []\n",
    "        for i in range(n_examples):\n",
    "            ex_i = ex_inputs[:, i:i+1, :, :]  # [batch_size, 1, H, W]\n",
    "            ex_o = ex_outputs[:, i:i+1, :, :]  # [batch_size, 1, H, W]\n",
    "            \n",
    "            ex_i = self._fusion(ex_i)\n",
    "            ex_o = self._fusion(ex_o)\n",
    "            \n",
    "            ex_f = torch.cat([ex_i, ex_o], dim=1)\n",
    "            ex_f = self.cbam(ex_f)\n",
    "            ex_f = self.reduce(ex_f)\n",
    "            fused.append(ex_f)\n",
    "        \n",
    "        for i, ex_f in enumerate(fused):\n",
    "            ex_f = torch.cat([x, ex_f], dim=1)\n",
    "            ex_f = self.cbam(ex_f)\n",
    "            ex_f = self.reduce(ex_f)\n",
    "            fused[i] = ex_f\n",
    "        \n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        output = self.to_segmentation(fused)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 출력\n",
    "model_args = {\n",
    "    'dims': (64, 128),\n",
    "    'heads': 4,\n",
    "    'ff_expansion': 4,\n",
    "    'reduction_ratio': 2,\n",
    "    'num_layers': 4,\n",
    "    'channels': 1,\n",
    "    'num_classes': 11,\n",
    "    'kernel_stride_paddings': ((1, 1, 0),(3, 2, 1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims, heads, ff_expansion, reduction_ratio, num_layers = map(\n",
    "#             partial(cast_tuple, depth=4), (model_args['dims'], \n",
    "#                                            model_args['heads'], \n",
    "#                                            model_args['ff_expansion'], \n",
    "#                                            model_args['reduction_ratio'], \n",
    "#                                            model_args['num_layers']))\n",
    "# mit = MiT(\n",
    "#         dims=dims,\n",
    "#         heads=heads,\n",
    "#         ff_expansion=ff_expansion,\n",
    "#         reduction_ratio=reduction_ratio,\n",
    "#         num_layers=num_layers,\n",
    "#         channels=model_args['channels'],\n",
    "#         stage_kernel_stride_pad=model_args['kernel_stride_paddings'],\n",
    "        \n",
    "#           )\n",
    "# x = torch.randn(10, 1, 30, 30)\n",
    "\n",
    "# print(\"Input shape:\", x.shape)\n",
    "# x = mit(x,return_layer_outputs = True)\n",
    "\n",
    "# print(\"Output shape:\", x[0].shape)\n",
    "# print(\"Output shape:\", x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = x[0] + x[0]\n",
    "# x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse shape: torch.Size([10, 256, 32, 32])\n",
      "fuse shape: torch.Size([10, 512, 32, 32])\n",
      "fuse shape: torch.Size([10, 512, 32, 32])\n",
      "==================\n",
      "fuse shape: torch.Size([10, 256, 32, 32])\n",
      "fuse shape: torch.Size([10, 512, 32, 32])\n",
      "fuse shape: torch.Size([10, 512, 32, 32])\n",
      "==================\n",
      "fuse shape: torch.Size([10, 256, 32, 32])\n",
      "fuse shape: torch.Size([10, 512, 32, 32])\n",
      "fuse shape: torch.Size([10, 512, 32, 32])\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "fuse = [torch.randn(10, 256, 32, 32), torch.randn(10, 256, 32, 32), torch.randn(10, 256, 32, 32)]\n",
    "\n",
    "x= torch.randn(10, 256, 32, 32)\n",
    "for i , ex in enumerate(fuse) :\n",
    "    print(\"fuse shape:\", ex.shape)\n",
    "    ex = torch.cat([ex, x], dim=1)\n",
    "    print(\"fuse shape:\", ex.shape)\n",
    "    fuse[i] = ex\n",
    "    print(\"fuse shape:\", fuse[i].shape)\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 11, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ARC_Net(**model_args).to(device)\n",
    "# 입력 텐서 생성\n",
    "x = torch.randn(10, 1, 32, 32).to(device)\n",
    "e_i, e_o = torch.randn(10, 3, 32, 32).to(device), torch.randn(10, 3, 32, 32).to(device)\n",
    "print(model(x,e_i,e_o).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
       "========================================================================================================================\n",
       "ARC_Net                                                                [1, 11, 30, 30]           --\n",
       "├─MiT: 1-1                                                             [1, 64, 30, 30]           --\n",
       "│    └─ModuleList: 2-23                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-45                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-46                                           --                        (recursive)\n",
       "├─ModuleList: 1-18                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-2                                                 [1, 128, 30, 30]          --\n",
       "│    │    └─Conv2d: 3-3                                                [1, 128, 30, 30]          8,320\n",
       "│    │    └─Upsample: 3-4                                              [1, 128, 30, 30]          --\n",
       "│    └─Sequential: 2-3                                                 [1, 128, 30, 30]          --\n",
       "│    │    └─Conv2d: 3-5                                                [1, 128, 15, 15]          16,512\n",
       "│    │    └─Upsample: 3-6                                              [1, 128, 30, 30]          --\n",
       "├─MiT: 1-3                                                             [1, 64, 30, 30]           (recursive)\n",
       "│    └─ModuleList: 2-23                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-45                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-46                                           --                        (recursive)\n",
       "├─ModuleList: 1-18                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-5                                                 [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-9                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Upsample: 3-10                                             [1, 128, 30, 30]          --\n",
       "│    └─Sequential: 2-6                                                 [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-11                                               [1, 128, 15, 15]          (recursive)\n",
       "│    │    └─Upsample: 3-12                                             [1, 128, 30, 30]          --\n",
       "├─MiT: 1-5                                                             [1, 64, 30, 30]           (recursive)\n",
       "│    └─ModuleList: 2-23                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-45                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-46                                           --                        (recursive)\n",
       "├─ModuleList: 1-18                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-8                                                 [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-15                                               [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Upsample: 3-16                                             [1, 128, 30, 30]          --\n",
       "│    └─Sequential: 2-9                                                 [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-17                                               [1, 128, 15, 15]          (recursive)\n",
       "│    │    └─Upsample: 3-18                                             [1, 128, 30, 30]          --\n",
       "├─CBAM: 1-7                                                            [1, 512, 30, 30]          --\n",
       "│    └─ChannelGate: 2-10                                               [1, 512, 30, 30]          --\n",
       "│    │    └─Sequential: 3-19                                           [1, 512]                  33,312\n",
       "│    │    └─Sequential: 3-20                                           [1, 512]                  (recursive)\n",
       "│    └─SpatialGate: 2-11                                               [1, 512, 30, 30]          --\n",
       "│    │    └─ChannelPool: 3-21                                          [1, 2, 30, 30]            --\n",
       "│    │    └─BasicConv: 3-22                                            [1, 1, 30, 30]            100\n",
       "├─Conv2d: 1-8                                                          [1, 256, 30, 30]          131,328\n",
       "├─MiT: 1-9                                                             [1, 64, 30, 30]           (recursive)\n",
       "│    └─ModuleList: 2-23                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-45                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-46                                           --                        (recursive)\n",
       "├─ModuleList: 1-18                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-13                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-25                                               [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Upsample: 3-26                                             [1, 128, 30, 30]          --\n",
       "│    └─Sequential: 2-14                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-27                                               [1, 128, 15, 15]          (recursive)\n",
       "│    │    └─Upsample: 3-28                                             [1, 128, 30, 30]          --\n",
       "├─MiT: 1-11                                                            [1, 64, 30, 30]           (recursive)\n",
       "│    └─ModuleList: 2-23                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-45                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-46                                           --                        (recursive)\n",
       "├─ModuleList: 1-18                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-16                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-31                                               [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Upsample: 3-32                                             [1, 128, 30, 30]          --\n",
       "│    └─Sequential: 2-17                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-33                                               [1, 128, 15, 15]          (recursive)\n",
       "│    │    └─Upsample: 3-34                                             [1, 128, 30, 30]          --\n",
       "├─CBAM: 1-13                                                           [1, 512, 30, 30]          (recursive)\n",
       "│    └─ChannelGate: 2-18                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─Sequential: 3-35                                           [1, 512]                  (recursive)\n",
       "│    │    └─Sequential: 3-36                                           [1, 512]                  (recursive)\n",
       "│    └─SpatialGate: 2-19                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─ChannelPool: 3-37                                          [1, 2, 30, 30]            --\n",
       "│    │    └─BasicConv: 3-38                                            [1, 1, 30, 30]            (recursive)\n",
       "├─Conv2d: 1-14                                                         [1, 256, 30, 30]          (recursive)\n",
       "├─MiT: 1-15                                                            [1, 64, 30, 30]           (recursive)\n",
       "│    └─ModuleList: 2-23                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-45                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-46                                           --                        (recursive)\n",
       "├─ModuleList: 1-18                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-21                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-41                                               [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Upsample: 3-42                                             [1, 128, 30, 30]          --\n",
       "│    └─Sequential: 2-22                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-43                                               [1, 128, 15, 15]          (recursive)\n",
       "│    │    └─Upsample: 3-44                                             [1, 128, 30, 30]          --\n",
       "├─MiT: 1-17                                                            [1, 64, 30, 30]           (recursive)\n",
       "│    └─ModuleList: 2-23                                                --                        (recursive)\n",
       "│    │    └─ModuleList: 3-45                                           --                        (recursive)\n",
       "│    │    └─ModuleList: 3-46                                           --                        (recursive)\n",
       "├─ModuleList: 1-18                                                     --                        (recursive)\n",
       "│    └─Sequential: 2-24                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-47                                               [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Upsample: 3-48                                             [1, 128, 30, 30]          --\n",
       "│    └─Sequential: 2-25                                                [1, 128, 30, 30]          (recursive)\n",
       "│    │    └─Conv2d: 3-49                                               [1, 128, 15, 15]          (recursive)\n",
       "│    │    └─Upsample: 3-50                                             [1, 128, 30, 30]          --\n",
       "├─CBAM: 1-19                                                           [1, 512, 30, 30]          (recursive)\n",
       "│    └─ChannelGate: 2-26                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─Sequential: 3-51                                           [1, 512]                  (recursive)\n",
       "│    │    └─Sequential: 3-52                                           [1, 512]                  (recursive)\n",
       "│    └─SpatialGate: 2-27                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─ChannelPool: 3-53                                          [1, 2, 30, 30]            --\n",
       "│    │    └─BasicConv: 3-54                                            [1, 1, 30, 30]            (recursive)\n",
       "├─Conv2d: 1-20                                                         [1, 256, 30, 30]          (recursive)\n",
       "├─CBAM: 1-21                                                           [1, 512, 30, 30]          (recursive)\n",
       "│    └─ChannelGate: 2-28                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─Sequential: 3-55                                           [1, 512]                  (recursive)\n",
       "│    │    └─Sequential: 3-56                                           [1, 512]                  (recursive)\n",
       "│    └─SpatialGate: 2-29                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─ChannelPool: 3-57                                          [1, 2, 30, 30]            --\n",
       "│    │    └─BasicConv: 3-58                                            [1, 1, 30, 30]            (recursive)\n",
       "├─Conv2d: 1-22                                                         [1, 256, 30, 30]          (recursive)\n",
       "├─CBAM: 1-23                                                           [1, 512, 30, 30]          (recursive)\n",
       "│    └─ChannelGate: 2-30                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─Sequential: 3-59                                           [1, 512]                  (recursive)\n",
       "│    │    └─Sequential: 3-60                                           [1, 512]                  (recursive)\n",
       "│    └─SpatialGate: 2-31                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─ChannelPool: 3-61                                          [1, 2, 30, 30]            --\n",
       "│    │    └─BasicConv: 3-62                                            [1, 1, 30, 30]            (recursive)\n",
       "├─Conv2d: 1-24                                                         [1, 256, 30, 30]          (recursive)\n",
       "├─CBAM: 1-25                                                           [1, 512, 30, 30]          (recursive)\n",
       "│    └─ChannelGate: 2-32                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─Sequential: 3-63                                           [1, 512]                  (recursive)\n",
       "│    │    └─Sequential: 3-64                                           [1, 512]                  (recursive)\n",
       "│    └─SpatialGate: 2-33                                               [1, 512, 30, 30]          (recursive)\n",
       "│    │    └─ChannelPool: 3-65                                          [1, 2, 30, 30]            --\n",
       "│    │    └─BasicConv: 3-66                                            [1, 1, 30, 30]            (recursive)\n",
       "├─Conv2d: 1-26                                                         [1, 256, 30, 30]          (recursive)\n",
       "├─Head: 1-27                                                           [1, 11, 30, 30]           --\n",
       "│    └─Sequential: 2-34                                                [1, 11, 30, 30]           --\n",
       "│    │    └─Conv2d: 3-67                                               [1, 256, 30, 30]          196,864\n",
       "│    │    └─Conv2d: 3-68                                               [1, 11, 30, 30]           2,827\n",
       "========================================================================================================================\n",
       "Total params: 3,289,231\n",
       "Trainable params: 3,289,231\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.96\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 364.29\n",
       "Params size (MB): 13.16\n",
       "Estimated Total Size (MB): 377.48\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "outer_model = ARC_Net(**model_args).to(device)\n",
    "\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(outer_model, input_size=((1, 1, 30, 30), (1, 3, 30, 30), (1, 3, 30, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bw_net_maml import BWNet_MAML\n",
    "\n",
    "# model = BWNet_MAML(embed_size=1).to(device)\n",
    "\n",
    "# # 입력 텐서 생성\n",
    "# x = torch.randn(1, 1, 30, 30).to(device)\n",
    "\n",
    "# # FLOPs 및 파라미터 수 계산\n",
    "# try:\n",
    "#     flops, params = profile(model, inputs=(x,))\n",
    "#     flops, params = clever_format([flops, params], \"%.3f\")\n",
    "#     print(f\"FLOPs: {flops}\")\n",
    "#     print(f\"파라미터 수: {params}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during profiling: {e}\")\n",
    "#     print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#weight = torch.ones(11).to('cuda')\n",
    "#weight[0] = 0.0005  # 0은 무시\n",
    "#print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 32, 32]) torch.Size([10, 1, 32, 32]) torch.Size([10, 3, 32, 32]) torch.Size([10, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "class ARC_Dataset(Dataset):\n",
    "    def __init__(self, challenges, solution, task_data_num=1, example_data_num=3, max_combinations=10):\n",
    "        challenges = load_json(challenges)\n",
    "        solution = load_json(solution)\n",
    "        self.data = []\n",
    "        self.task_data_num = task_data_num\n",
    "        self.example_data_num = example_data_num\n",
    "        self.max_combinations = max_combinations\n",
    "\n",
    "        for key, value in challenges.items():\n",
    "            for i in range(len(value['test'])):\n",
    "                task_input = value['test'][i]['input']\n",
    "                task_output = solution[key][i]\n",
    "                example_list = value['train'].copy()\n",
    "\n",
    "                n_examples = len(example_list)  # 원래 예제의 개수\n",
    "\n",
    "                # 예제 수가 example_data_num보다 적으면 현재 작업의 예제를 복제하여 채움\n",
    "                if n_examples < self.example_data_num:\n",
    "                    while len(example_list) < self.example_data_num:\n",
    "                        example_list.append(random.choice(example_list))\n",
    "\n",
    "                    # 조합은 예제 리스트 자체로 설정\n",
    "                    ex_combinations = [tuple(example_list)]\n",
    "                else:\n",
    "                    # 예제의 조합 생성\n",
    "                    ex_combinations = list(combinations(example_list, self.example_data_num))\n",
    "\n",
    "                    # 조합의 수를 제한\n",
    "                    if len(ex_combinations) > self.max_combinations:\n",
    "                        ex_combinations = random.sample(ex_combinations, self.max_combinations)\n",
    "\n",
    "                for ex_combo in ex_combinations:\n",
    "                    ex_input = [ex['input'] for ex in ex_combo]\n",
    "                    ex_output = [ex['output'] for ex in ex_combo]\n",
    "\n",
    "                    # 데이터에 추가하면서 원래 예제의 개수도 포함\n",
    "                    self.data.append({\n",
    "                        'id': key,\n",
    "                        'input': task_input,\n",
    "                        'output': task_output,\n",
    "                        'ex_input': ex_input,\n",
    "                        'ex_output': ex_output,\n",
    "                        'num_original_examples': n_examples  # 원래 예제 개수 추가\n",
    "                    })\n",
    "\n",
    "        # 리스트를 데이터프레임으로 변환\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def pad_to_32x32(self, tensor):\n",
    "        if tensor.dim() == 2:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        c, h, w = tensor.shape\n",
    "        pad_h = max(0, 32 - h)\n",
    "        pad_w = max(0, 32 - w)\n",
    "        \n",
    "        # 좌우 및 상하 패딩을 반반씩 나눠서 적용\n",
    "        padding = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n",
    "        tensor = F.pad(tensor, padding, mode='constant', value=0)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def pad_to_30x30(self, tensor):\n",
    "        if tensor.dim() == 2:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        c, h, w = tensor.shape\n",
    "        pad_h = max(0, 30 - h)\n",
    "        pad_w = max(0, 30 - w)\n",
    "        \n",
    "        # 좌우 및 상하 패딩을 반반씩 나눠서 적용\n",
    "        padding = (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n",
    "        tensor = F.pad(tensor, padding, mode='constant', value=0)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    def mapping_input(self, tensor):\n",
    "        mapping = {\n",
    "            1: random.randint(1, 10),\n",
    "            2: random.randint(11, 20),\n",
    "            3: random.randint(21, 30),\n",
    "            4: random.randint(31, 40),\n",
    "            5: random.randint(41, 50),\n",
    "            6: random.randint(51, 60),\n",
    "            7: random.randint(61, 70),\n",
    "            8: random.randint(71, 80),\n",
    "            9: random.randint(81, 90),\n",
    "            10: random.randint(91, 100)\n",
    "        }\n",
    "        temp_tensor = tensor.clone()\n",
    "        for k in mapping:\n",
    "            temp_tensor[temp_tensor == k] = -k  # 임시로 기존 값에 음수를 취해 중복을 피함\n",
    "\n",
    "        # 최종 매핑 적용\n",
    "        for k, v in mapping.items():\n",
    "            temp_tensor[temp_tensor == -k] = v\n",
    "        return temp_tensor\n",
    "    \n",
    "    def noise_input(self, tensor):\n",
    "        mapping = {\n",
    "            1: 1+ np.random.normal(0, 1),\n",
    "            2: 2+ np.random.normal(0, 1),\n",
    "            3: 3+ np.random.normal(0, 1),\n",
    "            4: 4+ np.random.normal(0, 1),\n",
    "            5: 5+ np.random.normal(0, 1),\n",
    "            6: 6+ np.random.normal(0, 1),\n",
    "            7: 7+ np.random.normal(0, 1),\n",
    "            8: 8+ np.random.normal(0, 1),\n",
    "            9: 9+ np.random.normal(0, 1),\n",
    "            10: 10+ np.random.normal(0, 1)\n",
    "        }\n",
    "        temp_tensor = tensor.clone()\n",
    "        for k in mapping:\n",
    "            temp_tensor[temp_tensor == k] = -k  # 임시로 기존 값에 음수를 취해 중복을 피함\n",
    "\n",
    "        # 최종 매핑 적용\n",
    "        for k, v in mapping.items():\n",
    "            temp_tensor[temp_tensor == -k] = v\n",
    "        return temp_tensor\n",
    "    \n",
    "    def augment_example_output(self, tensor):\n",
    "        # 출력 데이터 증강 (아직 구현 필요)\n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task = self.df.iloc[idx]\n",
    "        \n",
    "        # task_input과 task_output 변환 및 패딩 추가\n",
    "        task_input = self.pad_to_32x32((torch.tensor(task['input'], dtype=torch.float32) + 1))\n",
    "        task_output = self.pad_to_32x32((torch.tensor(task['output'], dtype=torch.float32) + 1))\n",
    "        \n",
    "        # 입력 채널 차원 추가\n",
    "        if task_input.dim() == 2:\n",
    "            task_input = task_input.unsqueeze(0)  # [1, H, W]\n",
    "        if task_output.dim() == 2:\n",
    "            task_output = task_output.unsqueeze(0)  # [1, H, W]\n",
    "        \n",
    "        # 예제 입력과 출력 변환 및 패딩 추가\n",
    "        example_input = []\n",
    "        example_output = []\n",
    "        for ex_in, ex_out in zip(task['ex_input'], task['ex_output']):\n",
    "            ex_in_tensor = self.pad_to_32x32(torch.tensor(ex_in, dtype=torch.float32) + 1)\n",
    "            ex_out_tensor = self.pad_to_32x32(torch.tensor(ex_out, dtype=torch.float32) + 1)\n",
    "            \n",
    "            # 입력 채널 차원 추가\n",
    "            if ex_in_tensor.dim() == 2:\n",
    "                ex_in_tensor = ex_in_tensor.unsqueeze(0)  # [1, H, W]\n",
    "            if ex_out_tensor.dim() == 2:\n",
    "                ex_out_tensor = ex_out_tensor.unsqueeze(0)  # [1, H, W]\n",
    "            \n",
    "            example_input.append(ex_in_tensor)\n",
    "            example_output.append(ex_out_tensor)\n",
    "        \n",
    "        # 예제 입력과 출력을 채널 차원으로 결합\n",
    "        ex_inputs = torch.cat(example_input, dim=0)  # [n_examples * channels, H, W]\n",
    "        ex_outputs = torch.cat(example_output, dim=0)  # [n_examples * channels, H, W]\n",
    "        \n",
    "        return task_input, task_output, ex_inputs, ex_outputs\n",
    "\n",
    "# 사용 예제\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "ti, to, ei, eo = next(iter(train_loader))\n",
    "print(ti.shape, to.shape, ei.shape, eo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  78%|███████▊  | 40/51 [00:20<00:05,  2.03it/s, loss=1.2430]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "train_args = {\n",
    "    'challenges': train_challenge,\n",
    "    'solution': train_solution,\n",
    "    'num_classes': 11,\n",
    "    'batch_size': 20,\n",
    "    'epochs': 1000,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    # print(y_pred.shape, y.shape)\n",
    "    weight = torch.ones(train_args['num_classes']).to(y.device)\n",
    "    weight[0] = 0.05\n",
    "    # weight[1] = 0.5\n",
    "    ce = F.cross_entropy(y_pred, y, weight=weight)\n",
    "    return ce\n",
    "\n",
    "def calculate_accuracy(predictions, targets, mask=None):\n",
    "    pred_classes = predictions.argmax(dim=1)  # [batch_size, H, W]\n",
    "    targets = targets.squeeze(1)  # [batch_size, H, W]\n",
    "\n",
    "    total_samples = targets.size(0)\n",
    "    correct_samples = pred_classes.eq(targets).all(dim=(1, 2)).sum().item()\n",
    "\n",
    "    if mask is not None:\n",
    "        correct_pixels = ((pred_classes == targets) & mask).sum().item()\n",
    "        total_pixels = mask.sum().item()\n",
    "    else:\n",
    "        correct_pixels = pred_classes.eq(targets).sum().item()\n",
    "        total_pixels = targets.numel()\n",
    "\n",
    "    # 디버깅을 위한 출력\n",
    "    # print(f\"correct_samples: {correct_samples}, total_samples: {total_samples}\")\n",
    "    # print(f\"correct_pixels: {correct_pixels}, total_pixels: {total_pixels}\")\n",
    "\n",
    "    return correct_samples, total_samples, correct_pixels, total_pixels\n",
    "\n",
    "def visualize_predictions(inputs, targets, predictions, condition):\n",
    "    if condition:\n",
    "        # 입력 이미지와 예측 결과를 CPU로 이동\n",
    "        inputs = inputs.cpu()\n",
    "        targets = targets.cpu()\n",
    "        predictions = predictions.cpu()\n",
    "\n",
    "        # 첫 번째 배치의 첫 번째 이미지를 시각화\n",
    "        input_image = inputs[0]            # shape: (C, H, W)\n",
    "        target_image = targets[0]          # shape: (C, H, W) 또는 (H, W)\n",
    "        prediction_image = predictions.argmax(dim=1)[0]  # shape: (H, W)\n",
    "\n",
    "        # 입력 이미지 형태 조정\n",
    "        if input_image.shape[0] == 1:\n",
    "            # 채널 차원이 1인 경우, 채널 차원 제거\n",
    "            input_image = input_image.squeeze(0)  # shape: (H, W)\n",
    "        else:\n",
    "            # 채널 차원이 3인 경우 (RGB 이미지)\n",
    "            input_image = input_image.permute(1, 2, 0)  # shape: (H, W, C)\n",
    "\n",
    "        # 타겟 이미지 형태 조정\n",
    "        if target_image.dim() == 3 and target_image.shape[0] == 1:\n",
    "            target_image = target_image.squeeze(0)  # shape: (H, W)\n",
    "\n",
    "        # 시각화를 위한 플롯 생성\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "        # 입력 이미지 표시\n",
    "        axes[0].imshow(input_image, cmap='gray')\n",
    "        axes[0].set_title('Input Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # 타겟 이미지 표시\n",
    "        axes[1].imshow(target_image, cmap='gray')\n",
    "        axes[1].set_title('Ground Truth')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # 예측 결과 표시\n",
    "        axes[2].imshow(prediction_image, cmap='gray')\n",
    "        axes[2].set_title('Prediction')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_args['batch_size'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(eval_challenge, eval_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=train_args['batch_size'], shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "model = ARC_Net(**model_args).to(device)\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.AdamW(model.parameters(), lr=train_args['learning_rate'])\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct_samples = 0\n",
    "    total_samples = 0\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{train_args[\"epochs\"]}', leave=False)\n",
    "\n",
    "    for batch_idx, (task_inputs, task_outputs, ex_inputs, ex_outputs) in progress_bar:\n",
    "        task_inputs = task_inputs.to(device, non_blocking=True)\n",
    "        task_outputs = task_outputs.to(device, non_blocking=True)\n",
    "        ex_inputs = ex_inputs.to(device, non_blocking=True)\n",
    "        ex_outputs = ex_outputs.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(task_inputs, ex_inputs, ex_outputs)\n",
    "\n",
    "        # 손실 함수 계산\n",
    "        loss = criterion(output, task_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(output, task_outputs)\n",
    "        total_correct_samples += correct_samples\n",
    "        total_samples += batch_total_samples\n",
    "        total_correct_pixels += correct_pixels\n",
    "        total_pixels += batch_total_pixels\n",
    "\n",
    "        # 특정 조건에서 시각화\n",
    "        if batch_idx == 0:\n",
    "            visualize_predictions(task_inputs, task_outputs, output, condition=True)\n",
    "        \n",
    "        # 프로그레스 바 업데이트\n",
    "        progress_bar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    \n",
    "    # 에포크가 끝난 후\n",
    "    avg_sample_accuracy = 100. * total_correct_samples / total_samples\n",
    "    avg_pixel_accuracy = 100. * total_correct_pixels / total_pixels\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f'\\nEpoch [{epoch+1}/{train_args[\"epochs\"]}] Complete')\n",
    "    print(f'Average Loss: {avg_loss:.6f}')\n",
    "    print(f'Training Sample Accuracy: {avg_sample_accuracy:.2f}%')\n",
    "    print(f'Training Pixel Accuracy: {avg_pixel_accuracy:.2f}%\\n')\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct_samples = 0\n",
    "    total_samples = 0\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(eval_loader), total=len(eval_loader), desc='Testing', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (task_inputs, task_outputs, ex_inputs, ex_outputs) in progress_bar:\n",
    "            task_inputs = task_inputs.to(device, non_blocking=True)\n",
    "            task_outputs = task_outputs.to(device, non_blocking=True)\n",
    "            ex_inputs = ex_inputs.to(device, non_blocking=True)\n",
    "            ex_outputs = ex_outputs.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(task_inputs, ex_inputs, ex_outputs)\n",
    "\n",
    "            # 손실 함수 계산\n",
    "            loss = criterion(output, task_outputs)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            correct_samples, batch_total_samples, correct_pixels, batch_total_pixels = calculate_accuracy(output, task_outputs)\n",
    "            total_correct_samples += correct_samples\n",
    "            total_samples += batch_total_samples\n",
    "            total_correct_pixels += correct_pixels\n",
    "            total_pixels += batch_total_pixels\n",
    "\n",
    "            # 특정 조건에서 시각화\n",
    "            if batch_idx == 0:\n",
    "                visualize_predictions(task_inputs, task_outputs, output, condition=True)\n",
    "            # 프로그레스 바 업데이트\n",
    "            progress_bar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    \n",
    "    avg_sample_accuracy = 100. * total_correct_samples / total_samples\n",
    "    avg_pixel_accuracy = 100. * total_correct_pixels / total_pixels\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "\n",
    "    print(f'Test Average Loss: {avg_loss:.6f}')\n",
    "    print(f'Test Sample Accuracy: {avg_sample_accuracy:.2f}%')\n",
    "    print(f'Test Pixel Accuracy: {avg_pixel_accuracy:.2f}%\\n')\n",
    "\n",
    "\n",
    "# 학습 실행\n",
    "for epoch in range(train_args['epochs']):  \n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
