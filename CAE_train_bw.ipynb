{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4, 1, 30, 30] at entry 0 and [6, 1, 30, 30] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 324\u001b[0m\n\u001b[0;32m    321\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    322\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest model saved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 324\u001b[0m train_and_validate(model, train_loader, eval_loader, criterion, optimizer, device, epochs\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 271\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m    269\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    270\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    272\u001b[0m     input_tensor, output_tensor, example_input, example_output \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    274\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4, 1, 30, 30] at entry 0 and [6, 1, 30, 30] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader_pairs import ARC_Dataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import random\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "def fix_random_seeds(seed_value=42):\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_random_seeds(77)\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device  \n",
    "print(f'Using {device} device')\n",
    "\n",
    "\n",
    "# 총 클래스의 수\n",
    "num_classes = 11\n",
    "\n",
    "# 0번과 1번 클래스에 부여할 가중치 (10% 이하로 설정)\n",
    "weight_0 = 0.04\n",
    "weight_1 = 0.05\n",
    "\n",
    "# 나머지 9개 클래스에 공평하게 가중치 부여\n",
    "remaining_weight = 1.0 - (weight_0 + weight_1)\n",
    "weight_other = remaining_weight / (num_classes - 2)\n",
    "\n",
    "# 가중치 리스트 생성\n",
    "class_weights = [weight_0, weight_1] + [weight_other] * (num_classes - 2)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "def criterion(y_pred, y):\n",
    "    y = y.long().squeeze(1)\n",
    "    ce = F.cross_entropy(y_pred, y, weight=class_weights_tensor)\n",
    "    # ce = F.cross_entropy(y_pred, y)\n",
    "    return ce\n",
    "\n",
    "# CBAM 모듈 정의\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(channels, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, C: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bnorm1 = nn.BatchNorm2d(C)\n",
    "        self.bnorm2 = nn.BatchNorm2d(C)\n",
    "        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.conv1(self.relu(self.bnorm1(x)))\n",
    "        r = self.dropout(r)\n",
    "        r = self.conv2(self.relu(self.bnorm2(r)))\n",
    "        return r + x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, mode: str, C_in: int, C_out: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(C_out)\n",
    "        if mode == \"down\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"up\":\n",
    "            self.conv = nn.ConvTranspose2d(C_in, C_out, kernel_size=4, stride=2, padding=0)\n",
    "        elif mode == \"same\":\n",
    "            self.conv = nn.Conv2d(C_in, C_out, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            raise ValueError(\"Wrong ConvBlock mode.\")\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv(z)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = ConvBlock(\"same\", 11,  channels[0], dropout)\n",
    "        self.res12 = ResBlock(channels[0], dropout)\n",
    "        self.conv2 = ConvBlock(\"same\", channels[0], channels[1], dropout)\n",
    "        self.res23 = ResBlock(channels[1], dropout)\n",
    "        self.conv3 = ConvBlock(\"same\", channels[1], channels[2], dropout)\n",
    "    \n",
    "    def one_hot_encode(self, z, num_classes=11):\n",
    "        \"\"\"\n",
    "        One-hot encode the input tensor and adjust the shape.\n",
    "        \"\"\"\n",
    "        return F.one_hot(z.squeeze(1).long(), num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_one_hot = self.one_hot_encode(z)\n",
    "        residuals = [0] * 3\n",
    "        x = self.conv1(z_one_hot)\n",
    "        x = self.res12(x)\n",
    "        residuals[0] = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.res23(x)\n",
    "        residuals[1] = x\n",
    "        x = self.conv3(x)\n",
    "        residuals[2] = x\n",
    "        return x, residuals\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv3 = ConvBlock(\"same\", channels[-1] * 2, channels[-2], dropout)\n",
    "        self.res32 = ResBlock(channels[-2], dropout)\n",
    "        self.conv2 = ConvBlock(\"same\", channels[-2] * 2, channels[-3], dropout)\n",
    "        self.res21 = ResBlock(channels[-3], dropout)\n",
    "        self.conv1 = ConvBlock(\"same\", channels[-3] * 2, channels[-3], dropout)\n",
    "        self.conv0 = nn.Conv2d(channels[-3], 11, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, residuals):\n",
    "        x = torch.cat([x, residuals[2]], dim=1)\n",
    "        x = self.conv3(x)\n",
    "        x = self.res32(x)\n",
    "        x = torch.cat([x, residuals[1]], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.res21(x)\n",
    "        x = torch.cat([x, residuals[0]], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv0(x)\n",
    "        return x\n",
    "\n",
    "class CBAMAEmodel(nn.Module):\n",
    "    def __init__(self, channels=[256, 512, 512], embed_dim=512, dropout=0.1):\n",
    "        super(CBAMAEmodel, self).__init__()\n",
    "        self.preprocess_and_embed = Encoder(channels, dropout)\n",
    "        self.decoder = Decoder(channels, dropout)\n",
    "        self.cbam1 = CBAM(channels=embed_dim*2)\n",
    "        self.cbam2 = CBAM(channels=embed_dim*2)\n",
    "        self.reduce_channels = nn.Conv2d(embed_dim*2, embed_dim*1, kernel_size=1)\n",
    "        self.reduce_channels2 = nn.Conv2d(embed_dim*2, embed_dim*1, kernel_size=1)\n",
    "\n",
    "    def forward(self, edu_input, edu_output, task_input):\n",
    "        embedded_edu_input, _ = self.preprocess_and_embed(edu_input)\n",
    "        embedded_edu_output, _ = self.preprocess_and_embed(edu_output)\n",
    "        embedded_task_input, residuals = self.preprocess_and_embed(task_input)\n",
    "        combined_edu = torch.cat([embedded_edu_input, embedded_edu_output], dim=1)\n",
    "        attended_edu = self.cbam1(combined_edu)\n",
    "        attended_edu = self.reduce_channels(attended_edu)\n",
    "        task_edu_cat = torch.cat([attended_edu, embedded_task_input], dim=1)\n",
    "        task_causal_mix = self.cbam2(task_edu_cat)\n",
    "        task_causal_mix = self.reduce_channels2(task_causal_mix)\n",
    "        final_output = self.decoder(task_causal_mix, residuals)\n",
    "        return final_output\n",
    "\n",
    "colors = ['#000000','#1E93FF','#F93C31','#4FCC30','#FFDC00',\n",
    "          '#999999','#E53AA3','#FF851B','#87D8F1','#921231','#555555']\n",
    "colormap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "def show_grid_side_by_side(*grids):\n",
    "    num_grids = len(grids)\n",
    "    fig, axes = plt.subplots(1, num_grids, figsize=(num_grids * 2.8, 2.8))\n",
    "\n",
    "    if num_grids == 1:\n",
    "        axes = [axes]  # 리스트로 변환하여 일관성 유지\n",
    "    \n",
    "    for ax, grid in zip(axes, grids):\n",
    "        if grid.ndim == 4:\n",
    "            grid = grid.squeeze()  # [1, 1, 30, 30] -> [30, 30]로 변환\n",
    "        elif grid.ndim == 3:\n",
    "            grid = grid[0]  # [1, 30, 30] -> [30, 30]로 변환\n",
    "            \n",
    "        ax.pcolormesh(grid, edgecolors=colors[-1], linewidth=0.5, cmap=colormap, vmin=0, vmax=10)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "train_challenge = './kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solution = \"./kaggle/input/arc-prize-2024/arc-agi_training_solutions.json\"\n",
    "eval_challenge = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
    "eval_solution = \"./kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json\"\n",
    "\n",
    "kwargs = {\n",
    "    'epochs': 100,\n",
    "    'task_numbers': 16, #equal to the number of tasks\n",
    "    'task_data_num': 1,\n",
    "    'example_data_num': 5, #equal to inner model batch size\n",
    "    'inner_lr': 0.001,\n",
    "    'outer_lr': 0.001,\n",
    "    'embed_size': 1,\n",
    "    \n",
    "}\n",
    "\n",
    "train_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "train_loader = DataLoader(train_dataset, batch_size=kwargs['task_numbers'], shuffle=True)\n",
    "\n",
    "eval_dataset = ARC_Dataset(train_challenge, train_solution)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=kwargs['task_numbers'], shuffle=False)\n",
    "\n",
    "model = CBAMAEmodel(channels=[256, 512, 512], embed_dim=512)\n",
    "optimizer= optim.AdamW(model.parameters(),lr=kwargs['outer_lr'])\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_samples = 0\n",
    "        for data in tqdm(train_loader, desc='Training'):\n",
    "            input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(example_input, example_output, input_tensor)\n",
    "            loss = criterion(output, output_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item() * input_tensor.size(0)  # 각 배치의 손실을 샘플 수로 가중\n",
    "            train_samples += input_tensor.size(0)\n",
    "            \n",
    "            del input_tensor, output_tensor, example_input, example_output\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_samples\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(val_loader, desc='Validation'):\n",
    "                input_tensor, output_tensor, example_input, example_output = [d.to(device) for d in data]\n",
    "                \n",
    "                output = model(example_input, example_output, input_tensor)\n",
    "                loss = criterion(output, output_tensor)\n",
    "                \n",
    "                total_val_loss += loss.item() * input_tensor.size(0)\n",
    "                total += input_tensor.size(0)\n",
    "                \n",
    "                # Assuming classification task\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct += (predicted == output_tensor).sum().item()\n",
    "                \n",
    "                del input_tensor, output_tensor, example_input, example_output\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / total\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Best model saved')\n",
    "\n",
    "train_and_validate(model, train_loader, eval_loader, criterion, optimizer, device, epochs=kwargs['epochs'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
